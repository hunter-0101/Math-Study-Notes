> Given a matrix $A\in M_n(\mathbb R)$, if $\|A-I\|_F<1$ the F-norm ([[Matrix]]), then $A$ is non-singular.

**Discussion**: this is essentially the result in [[Banach Algebra]] that $\|t\|<1$ implies $e-t$ invertible, and all different kinds of solutions below is basically a more complicated way of showing this.
## Proof via the fixed point theorem
This proof is mainly motivated by the fact that the problem itself is an exercise of the section introducing the Banach fixed point theorem ([[Metric Space]]).
Obviously by [[Solving Linear System]] we have the equivalence $$\det A\neq0\quad\iff\quad Ax=b\text{ has a unique solution}$$Now notice that $$Ax=b\quad\iff(I+(A-I))x=b\quad\iff\quad x=b-(A-I)x$$if we define an operator by $$Tx=b-(A-I)x$$then we can easily verify that $\|Tx-Ty\|_2\le\|T\|_F\|x-y\|_2$ ([[Matrix]]), which means that it's a contraction map, hence the fixed point theorem can be applied.
## Proof via linearity & optimization
This proof is more natural, and is constructed via a simple [[Toy Model]] experience. However I have to admit that after proceeding to the second equality I have no idea how to calculate the minimal value in the general case, and turned to Gemini for help.
Assume otherwise that $\det A=0$, then WLOG we may assume that there exists $\beta_j,1\le j<n$ s.t. $$a_n=\sum_{j=1}^{n-1}\beta_ja_j$$where $a_j$ is the $j$-th column of $A$, hence the original expression can be written as $$\begin{align}\|A-I\|_F&=\sum_{1\le i,j\le n}(a_{ij}-\delta_{ij})^2\\&=\sum_{j=1}^{n-1}\sum_{i=1}^n(a_{ij}-\delta_{ij})^2+\sum_{i=1}^n\left(\sum_{j=1}^{n-1}\beta_ja_{ij}-\delta_{ij}\right)^2\end{align}$$Below we show that $\|A-I\|_F\ge1$. By changing order of summation we could define $$E_i=\sum_{j=1}^{n-1}(a_{ij}-\delta_{ji})^2+\left(\sum_{j-1}^{n-1}\beta_ja_{ij}-\delta_{in}\right)^2$$we fix all $\beta_j$, and try to find the minimal values of $E_i$. Obviously if we can show that $E=\sum E_i\ge1$, then the proof is immediately completed. By [[Differential Calculus on Euclidean Space]], $$\frac{\partial E_i}{\partial a_{ik}}=2(a_{ik}-\delta_{ik})+2\left(\sum_{j=1}^{n-1}\beta_ja_{ij}-\delta_{in}\right)\cdot\beta_k=0$$if we define $$C_i=\sum_{j=1}^{n-1}\beta_ja_{ij}-\delta_{in}\quad\Longrightarrow\quad\frac{\partial E_i}{\partial a_{ik}}=2(a_{ik}-\delta_{ik}+\beta_kC_i)$$Now substituting all $C_i$ back to $a_{ik}$ we find $$C_i+\delta_{in}=\sum_{j-1}^{n-1}\beta_j(\delta_{ij}-\beta_jC_i)=\sum_{j=1}^{n-1}\beta_j\delta_{ij}-C_i\sum_{j=1}^{n-1}\beta_j^2$$If we define $S=\sum_{j=1}^{n-1}\beta_j^2$, then the above expression is further turned into $$C_i+\delta_{in}=\sum_{j=1}^{n-1}\beta_j\delta_{ij}-C_iS\quad\Longrightarrow\quad C_i(1+S)=\sum_{j=1}^{n-1}\beta_j\delta_{ij}-\delta_{in}$$Considering $1\le i<n$ and $i=n$ separately, we solve the values for $C_i$ as $$C_i=\frac{\beta_i}{1+S},\quad\forall 1\le i<n\quad\text{and}\quad C_n=-\frac{1}{1+S}$$Substitute these values back to the original expression, we have $$E_i^\min=\sum_{j=1}^{n-1}(-\beta_jC_i)^2+C_i^2=C_i^2(1+S)=\begin{cases}\dfrac{\beta_i^2}{1+S},&1\le i<n\\\dfrac{1}{1+S},&i=n\end{cases}$$and finally we have $$E^\min=\sum_{i=1}^nE_i^\min=1$$hence we completes the proof.
### Proof via optimization w.r.t $\beta$
Notice that the main target is actually finding $$\min_\beta E_\beta\quad\text{where}\quad E_\beta(\beta_1,\cdots,\beta_{n-1})=\sum_{i=1}^n\left(\sum_{j=1}^{n-1}\beta_ja_{ij}-\delta_{in}\right)^2$$if we define $$\beta=\begin{bmatrix}\beta_1\\\beta_2\\\vdots\\\beta_{n-1}\end{bmatrix},\quad M=\begin{bmatrix}a_{11}&a_{12}&\cdots&a_{1,n-1}\\a_{21}&a_{22}&\cdots&a_{2,n-1}\\\vdots&\vdots&\ddots&\vdots\\a_{n1}&a_{n2}&\cdots&a_{n,n-1}\end{bmatrix},\quad b=e_n=\begin{bmatrix}0\\0\\\vdots\\1\end{bmatrix}$$the the above is essentially a linear least square problem ([[Singular Value Decomposition]]) $$\min_\beta E_\beta=\min_\beta\|M\beta-b\|_2^2$$by related theory the optimal coefficient is $$\beta^*=(M^TM)^{-1}(M^Tb)$$In this way we successfully eliminate all $\beta_j$ form our expression, and if suffices to minimize $E$ w.r.t $a_{ij}$. However I'm suspecting that it's too complicated, if not impossible, to achieve this goal analytically using the above expression.
## Another proof via linearity
This proof follows is motivated by the last one, when I'm trying to simplify it using proper vector expressions.
WLOG assume that there exists $x=(x_1,\cdots,x_n)\neq0,\|x\|_2=1$ s.t. $Ax=0$, then since $$x=Ix-Ax=(I-A)x=\sum_{j=1}^n x_j (e_j - a_j)$$Now, we take the squared norm of both sides: $$1=\|x\|_2^2 = \left\|\sum_{j=1}^n x_j (e_j - a_j)\right\|_2^2$$However, applying the Cauchy-Schwarz inequality ([[Lp Space]]) we have $$\left\|\sum_{j=1}^nx_j(e_j-a_j)\right\|_2^2=\sum_{i=1}^n\left(\sum_{j=1}^n x_j(\delta_{ij}-a_{ij})\right)^2\le\sum_{i=1}^n\left(\|x\|_2\sum_{j=1}^n(a_{ij}-\delta_{ij})^2\right)=\|A-I\|_F$$and this is immediately a contradiction.
## Proof via spectral analysis
Denote $B=A-I$. If $A\notin\gl_n(\mathbb R)$ there exists non-zero $x$ with $Ax=0$, which means $$(I+B)x=0\quad\iff\quad Bx=-x$$hence $-1\in\sigma(B)$, and this implies that $$\|B\|_F\ge\|B\|_2\ge1$$Now the contradiction is immediately derived. 