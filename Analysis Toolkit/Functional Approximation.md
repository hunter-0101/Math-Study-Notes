This technique is mostly used for improving the property of target functions, e.g., integrability, continuity, smoothness, etc.
## Smoothing
- **Density argument**: [[Lp Space]].
- **Integral smoothing**: the main idea behind this is that integration usually makes a function more smooth. For a function $f(x)$, if integrable, we could define $$f_\alpha(x)=\frac{1}{\alpha}\int_x^{x+\alpha}f(t)dt$$then the resulting $f_\alpha(x)$ is smooth, thus differentiable, leaving us more space and tools. The smoothness is guaranteed by the AC of integral. [[Lebesgue Integral]]
	- The construction preserves monotony and sign, and **converges uniformly** to $f(x)$. [[Function on Euclidean Space]]
	- The integral can be appleid multiple times, with the coefficient modified to $\frac{1}{\alpha^k}$ accordingly. This depends on how "smooth" you want your function to be. Applying this $k$ times yields a function in $C^k$.
- **Convolution smoothing**: by [[Convolution]] we know that most of the time convolution turned out to be well-behaved (also indicated by the integral smoothing). Thus we may use it to construct some stronger conditions.
	- **Example**: let $\lambda(E)>0$, show that $E-E$ contains a neighborhood of zero. To utilize continuity of convolution we consider the indicator function $f=\mathbb 1_E,g=\mathbb 1_{-E}$. Notice $h=f*g$ is continuous at $0$, and $h(0)=\lambda(E)>0$, thus we find a small neighborhood of $0$ with positive $h$, and the rest is easy.
	- **Approximation to the identity**: see [[Convolution]] for detail. The key idea is that, since convolution "blurs" the function (**uniformly**), then it makes sense to select a family of convolution kernel to gradually increase accuracy and recover the original value.
## Polynomial approximation
- **Weierstrass approximation theorem**: for any real-valued continuous function $f\in C[a,b]$ and $\varepsilon>0$, there exists a polynomial $p$ such that $$|f(x)-p(x)|<\varepsilon,\quad\forall x\in[a,b]$$**Original proof by Weierstrass**: it's known from [[Convolution]] that for uniformly continuous $f:\mathbb R\to\mathbb R$ the following function $$S_hf(x)=\frac{1}{h\sqrt\pi}\int_\mathbb Rf(u)e^{-\left(\frac{u-x}{h}\right)^2}du$$converges uniformly to $f$ as $h\to0^+$. Now for arbitrary continuous $f$ on $[a,b]$ we extends it to a uniformly continuous function on $\mathbb R$ that vanishes outside $[a-1,b+1]$ (which is easy), and use a proper translation such that $\text{supp}(f)=[-R,R]$ and $|f(x)|\le M$. For any $\varepsilon>0$ we know that there exist $h_0>0$ with $\|S_{h_0}f-f\|_\infty<\frac{\varepsilon}{2}$. On the other hand, by analyticity of $e^{-v^2}$ $$\left|\frac{1}{h_0\sqrt\pi}e^{-\left(\frac{u-x}{h_0}\right)^2}-\frac{1}{h_0\sqrt\pi}\sum_{k=0}^N\frac{(-1)^k}{k!}\left(\frac{u-x}{h_0}\right)^{2k}\right|<\frac{\varepsilon}{4RM}$$for some $N$. Integrate both sides on $[-R,R]$ and denote the polynomial as $P_N(x)$, then we get $\|S_{h_0}f(x)-P(x)\|_\infty<\frac{\varepsilon}{2}$.Combining it with the previous theorem completes the proof.
	- **Relation with [[Fourier Series]]**: for periodic $f\in\tilde C([a,b])$ (i.e., $f(a)=f(b)$) if we extends the above definition to $S_hf(z),z\in\mathbb C$ which is entire, and set $F_h(z)=S_hf(\ln n/i)$, then by periodicity $F_h(z)$ is single-valued and thus analytic, and we have its Laurent expansion $$F_h(z)=\sum_{n\in\mathbb N}c_{n,h}z^n$$now fix $|z|=1$ with $z=e^{ix}$, then we get the Fourier series. In other words, we've proved the absolute and uniform convergence of Fourier series. More generally, **==density of algebraic and trigonometric polynomials are equivalent==**.
		- $\Rightarrow$: already proved above.
		- $\Leftarrow$: since every trigonometric function $\sin(nx),\cos(nx)(n\in\mathbb N)$ is entire, their Taylor series approximate them uniformly and absolutely, thus density of trigonometric polynomial naturally give rise to density of algebraic polynomial.
	- **Proof by Lebesgue**: a continuous function can be uniformly approximated by polygonal line, which can be represented as a linear function plus sum of functions of the form $a|x-b|$. It suffice to show that $|x|$ can be uniformly approximated by polynomials, which is relatively easy using generalized binomial theorem (Maclaurin series of $(1+x)^r$).
	- **Application**: We may use this theorem to deduce that if $f$ defined on compact interval $I$ satisfies $\int_Ix^nf(x)=0,\forall n\in\mathbb N$, then $f\equiv0$ on $I$. The condition can be weakened to $f\in L^2([a,b])$ on an open interval.
- **Bernstein polynomial**: define the **Bernstein basis polynomials** of degree $n$ as$$b_{k,n}(x)=\binom{n}{k}x^k(1-x)^{n-k}$$the Bernstein basis polynomials of a specific degree form a basis for the linear space of polynomials of degree at most $n$. For a real-valued continuous function $f$ we have  $$B_n(f)(x)=\sum_{k=0}^nf\left(\frac{k}{n}\right)b_{k,n}(x)\to f,\quad n\to\infty\text{ uniformly}$$**Proof**: notice that $$|B_n(f)(x)-f(x)|=\left|\sum_{k=0}^n\left(f\left(\frac{k}{n}\right)-f(x)\right)b_{k,n}(x)\right|\le\sum_{k=0}^n|f(k/n)-f(x)|b_{k,n}(x)$$for any $\varepsilon>0$ take a corresponding $\delta$ s.t. $f(x_1)-f(x_2)<\varepsilon,\forall|x_1-x_2|<\delta$, and separate the summation into two parts $\sum_1:|k/n-x|<\delta$ and $\sum_2|k/n-x|\ge\delta$. Since $\sum_kb_{k,n}=1$ we have $\sum_1\le\varepsilon$, and by Chebyshev's inequality ([[Central Limit Theorem]]) for $n$ large enough we have $$\sum_2\le2M\sum_k\delta^{-2}(x-k/n)^2b_{k,n}(x)=2M\delta\frac{x(1-x)}{n}<\frac{1}{2}M\delta^{-2}n^{-1}$$where $M$ is the upper bound for $|f|$. The above estimate is independent of $x$, hence the convergence is uniform.
	- **Intuition**: consider the RV $K\sim B(n,x)$, then by LLN ([[Central Limit Theorem]]) we have $$\lim_{n\to\infty}P\left(\left|\frac{K}{n}-x\right|>\delta\right)=0,\quad\forall \delta>0$$The relation is uniform in $x$ by Chebyshev's inequality, following uniform boundedness of its variance $\var(K/n)=\frac{x(1-x)}{n}\le\frac{1}{4n}$. By uniform continuity we have $$\lim_{n\to\infty}P\left(\left|f\left(\frac{K}{n}\right)-f(x)\right|>\varepsilon\right)=0\quad\Longrightarrow\quad\lim E\left(\left|f\left(\frac{K}{n}\right)-f(x)\right|\right)=0$$Now by triangle inequality we have $$\lim_{n\to\infty}\left|Ef\left(\frac{K}{n}\right)-Ef(x)\right|\le\lim_{n\to\infty}E\left(\left|f\left(\frac{K}{n}\right)-f(x)\right|\right)=0$$indicating that $Ef(K/n)$ is a uniform approximation of $f$. Notice that $$Ef\left(\frac{K}{n}\right)=\sum_{K=0}^nf\left(\frac{K}{n}\right)p(K)=\sum_{K=0}^nf\left(\frac{K}{n}\right)b_{K,n}(x)=B_n(f)(x)$$hence we find a polynomial that serves as a uniform approximation.
		- **Generalization**: there are two essential facts used in the construction: the variance of $B(n,x)$ is uniformly bounded (precisely $\var(K/n)=o(1)$), and $f(x)$ can be well approximated by $Ef(K/n)$ as $n\to\infty$. It makes sense to pick other kinds of distribution satisfying these two conditions. We can even generalize to infinite interval, which gives rise to the discussion of moment ([[Random Variable]]).
	- **Relation with approximation to the identity**: instead of a standard convolution, we're taking the binomial distribution $B(n,x)$ ([[Distribution Function Gallery]]) as one-point convolution kernel, and improve accuracy by $n\to\infty$. It turns out that the collection of Bernstein basis $\set{\set{b_{k,n}}:n\in\mathbb N}$ can be seen as a generalized family of good kernels.
	- **Relation with STFT**: in STFT ([[FT on L1]]) we're localizing a function using time-frequency dual frame, while in Bernstein polynomial we're using probability-trial dual frame. The probability $x$ adjusts the bias of "window function", while $n$ changes the **resolution**. In short, both are **decomposing an object into localized representation**.
- **Necessity of increasing degree**: we have $$\overline{\mathbb P_n}\neq C([a,b]),\quad\forall n\in\mathbb N$$in the topology induced by $L^\infty$ norm. [[Lp Space]]
  **Proof**: it suffices to show the incapability for approximation of $\mathbb P_n$ on a set of points. Take $x_i\in[a,b],1\le i\le n+2$, and consider two vectors $v_1,v_2\in\mathbb R^{n+2}$ given by $$v_1=(1,-1,1\cdots),\quad v_2=(-1,1,-1,\cdots)$$If $\mathbb P_n$ is dense in $C([a,b])$ in the aforementioned topology then $$P_{n}^x=\set{(p(x_1),p(x_2),\cdots,p(x_{n+2})):p\in P_n}$$is of course dense in $\mathbb R^{n+2}$ in the same norm. Pick $\varepsilon<1$ and corresponding $$\|p_1^x-v_1\|_\infty<\varepsilon,\quad\|p_2^x-v_2\|_\infty<\varepsilon\quad\Longrightarrow\quad p_1(x_i)-p_2(x_i)\begin{cases}>0,&2\not\mid i\\<0,&2\mid i\end{cases}$$Hence $\deg(p_1-p_2)\le n$ while having at least $n+1$ roots, which implies $p_1=p_2$, contradiction.
- **Squeeze theorem**: refer to [[Series#Asymptotic analysis of series]].
- **Multiplying a factor**: this is often used for ensure integrability, e.g., [[FT on L1]]. It's mostly used in pair with Fubini's theorem, DCT, etc. guaranteed by integrability.