In analysis we're frequently faced with estimation problems that requires a upper/lower bound of an expression.
## [[Convexity]]
Convexity is very ubiquitous in different fields of mathematics. For estimation, its definition naturally yields a tangent-line inequality $$f(x)\ge f(x_0)+J_f(x_0)(x-x_0)$$In such case a very complicated expression can be turned into a linear term, making the overall estimation much easier.
- **[[Lp Space]]**: almost all the inequalities in real analysis, in particular those concerning functions, are derived directly or indirectly from convexity. As discussed, it's fair to say that convexity is the only tool available and universally applicable in this field.
## Estimation by splitting
When direct estimation is not feasible (e.g., too coarse), one useful direction of refinement is to separate the sequence into different class, each exhibiting a specific pattern that is easy to estimate. 
- **Two-part splitting**: the results on different integral kernels in [[Convolution]] are typical examples. The [[Frequency Splitting]] technique provide some deeper discussion.
- **Dyadic decomposition**: a very common one of separation is $$[0,\infty]=[0,1]+\bigcup_{k=0}^\infty[2^k,2^{k+1}]$$The key idea is that, when the terms to be estimated decays in a rate that is similar to $1/n$, or exhibits monotonicity, and we wanna get a lower bound for its sum, then we may consider to apply dyadic decomposition to its index, and estimate each interval uniformly. 
	- **Application**: one form of estimation in good kernels ([[Convolution]]), convex series ([[Series]]), etc..
- **Uniform splitting**: when estimating improper integrals ([[Riemann Integral]]), say $\int_0^\infty\frac{1}{1+x^3\sin^2 x}$ we need to separate the interval into $[n\pi,(n+1)\pi]$ and estimate each term.
	- **Sub-interval discussion**: this form of separation usually involve further refinement in each sub-interval, since the terms in each interval usually exhibits a bell-shaped curve.
- **Value-based splitting**: the above techniques are all index-based, and we can also consider value-based partition when there is **no monotonicity or other explicit patterns**.
	- **Application for series summation**: given $a_n>0,\sum a_n<\infty$, show that $\sum a_n^{\frac{n}{n+1}}<\infty$.
	  **Proof**: notice that $$a_n^{\frac{n}{n+1}}<Ca_n\quad\iff\quad a_n>\frac{1}{C^{n+1}}$$Hence if we let $I_C=\set{n\in\mathbb N:a_n>1/C^{n+1}}$, then $\sum_{n\in I_C}a_n$ can be bounded linearly, while the rest terms decays faster than geometrically. 
	- **Examples in real analysis**: value-based partition is very often used in real analysis, such as the usual simple function definition, density of special functions in [[Lp Space]], and Markov's inequality ([[Central Limit Theorem]]).
## Loosen-tighten technique
The **loosen-tighten technique** consists of a two-stage process: 
1. **Loosen**: create an inequality or bound that depends on a free parameter. At this stage we trade tightness for flexibility.
2. **Tighten**: we then **optimize** the bound by selecting the value of the parameter that makes the expression minimal (or optimal in some other sense).
The trick is to introduce a degree of freedom that balances **two competing terms**—typically one term that increases and one that decreases as the parameter changes. 
- **Example**: in the proof of Minkowski's inequality ([[Lp Space]]) a new parameter is introduced to create an upper bound by Jensen's inequality, and then an optimal coefficient is chosen; similar happens in the proof of Nash inequality above, and the sharp form of Young's convolution inequality ([[Lp Space]]). 
- **When to use**: this technique is usually useful when there is some competing terms in an expression (say, mass accumulation and decay). It's structurally motivated by convexity, frequency localization, interpolation, etc..
## Amplification
When some kind of symmetry can be observed in an expression we may consider to obtain some estimation via transforming the object in a well-chosen manner. This works particularly well when two sides of an inequality have different symmetries. For more see [the note by Tao](https://terrytao.wordpress.com/2007/09/05/amplification-arbitrage-and-the-tensor-power-trick/)
- **Example**: when proofing Cauchy-Schwarz inequality $|\langle u,v\rangle|\le\|u\|\cdot\|v\|$ we may consider to weaken the inequality to $|\langle u,v\rangle|\le\frac{\lambda^2}{2}\|u\|^2+\frac{1}{2\lambda^2}\|v\|^2$ given arbitrary $\lambda$, and then absorb $\lambda$ into the norms to get a weaker expression. Note that this works because introducing $\lambda$ does not effect LHS, and we already know that $|\langle u,v\rangle|\le\|u\|^2+\|v\|^2$ is indeed true.
- **Relation with loosen-tighten**: amplification focuses more on arbitraging from an un-matching pattern on both sides of an inequality to strengthen it (**implicit or symmetry-driven**), while loosen-tighten is more about manually introducing flexibility (**explicit minimization**). Fundamentally, they both derive from a **variational mindset**—use structure + parameter to extract tight information.
## Asymptotic estimation for improper integral
To find an **improper integral** $\int_D f(x)dx$ where $D$ is unbounded, the main idea to evaluate the integral using asymptotic estimation is to **compress the integral onto a finite integral, and compensate the loss using increasing accuracy in finite interval**. This idea mainly applied to those involving $e^x,\ln x$, and can be extended to finite interval with similar estimation.
- **Gaussian integral**: we try to evaluate the integral $\int_0^\infty e^{-x^2}dx$. Notice that $$1-x^2\le e^{-x^2}\le\frac{1}{1+x^2}$$Thus for any $n\in\mathbb Z_+$ we have $$\int_0^1(1-x^2)^ndx\le\int_0^1e^{-nx^2}dx\le\int_0^1\frac{dx}{(1+x^2)^n}$$for three integrals we consider the substitutions $x=\sin\theta,x=y/\sqrt n,x=\tan\theta$ respectively $$\int_0^{\pi/2}(\cos\theta)^{2n+1}d\theta\le\frac{1}{\sqrt n}\int_0^{\sqrt n}e^{-y^2}dy\le\int_0^{\pi/4}(\cos\theta)^{2n-2}d\theta<\int_0^{\pi/2}(\cos\theta)^{2n-2}d\theta$$Thus it suffice to estimate $I_k=\int_0^{\pi/2}(\cos\theta)^kd\theta$. Notice that $I_k=\frac{k-1}{k}I_{k-2}$ using change of variable. Then by $I_0=\pi/2,I_1=1$ we have $I_{2n}I_{2n+1}=\frac{1}{2n+1}\frac{\pi}{2}$. It's now obvious that $\sqrt nI_n\to\frac{\pi}{2}$, which is exactly the result.
- **Calculate $I=\int_0^1\frac{x-1}{\ln x}$**: to obtain its value we consider the following estimation$$n(x^{\frac{1}{n}}-1)\le\ln x\le n(1-x^{-\frac{1}{n}}),\quad\forall x\in \mathbb R_+,n\in\mathbb Z_+$$Now we have the following inequality $$\frac{1}{n}\int_0^1\frac{x-1}{x^{\frac{1}{n}}-1}dx\ge\int_0^1\frac{x-1}{\ln x}\ge\frac{1}{n}\int_0^1\frac{x-1}{1-x^{-\frac{1}{n}}}dx$$Denote two integrals as $I_1(n),I_2(n)$. Notice that$$\begin{align}I_1(n)&=\frac{1}{n}+\frac{1}{n+1}+\cdots+\frac{1}{2n-1}\to\ln2\\ I_2(n)&=\frac{1}{n+1}+\frac{1}{n+2}+\cdots+\frac{1}{2n}\to\ln2\end{align}$$Thus we know that $I=\ln2$. Other results related to such integral can be found in [[Special Function Gallery]].