In general, a **quadratic form** over [[Field]] $F$ is an expression $$f(x)=\sum_{1\le i,j\le n}a_{ij}x_ix_j=x^TAx$$where $x=(x_1,\cdots,x^n)$ and $A=[a_{ij}]_{n\times n}\in M_n(F)$ ([[Matrix]]). For convenience, we'll denote $S^n(F)$ the set of all symmetric matrices over the [[Field]] $F$.
- **Proper & affine quadratic form**: sometimes the above expression is referred to as a proper quadratic form, and its translations, which can be expressed as $$f(x)=\sum_{1\le i,j\le n}a_{ij}x_ix_j+\sum_{i=1}^nb_ix_i=x^TAx+b^Tx$$is called an **affine quadratic form**. We'll mostly use quadratic form to refer to the proper ones as the canonical choice due to uniqueness of center.
	- **Uniqueness of center**: an affine quadratic form with non-vanishing eigenvalues is symmetric w.r.t $0$ iff it's proper.
	  **Proof**: taking the origin as symmetric center is equivalent to the identity $$f(x)=f(-x)\quad\iff\quad x^TAx+b^Tx=(-x)^TA(-x)-b^Tx$$notice that $x^TAx=(-x)^TA(-x)$, the above identity is further equivalent to $$b^Tx=-b^Tx\quad\iff\quad b=0$$which completes the proof. 
	- **Discussion on vanishing eigenvalues**: notice that when $0\in\sigma(A)$ it might happen that an quadratic form does not admit a symmetric center (like elliptic paraboloid below). In these cases we usually do not differentiate the proper and affine variant of the form. 
- **Definite-ness**: symmetric $A$ is said to be positive (semi-)definite if $$A\succ(\succeq)0\quad\iff\quad x^TAx>(\ge)0,\quad\forall x\in\mathbb R^n$$The set of all positive semi-definite matrices is denoted $S^n_+$, and the set of all positive definite matrices is denoted $S_{++}^n$. Similarly we can define negative (semi-)definite matrices, which will be denoted $S_{-}^n,S_{--}^n$,  
	- **Definition via eigenvalue**: $A$ is positive (semi-)definite iff all eigenvalues of $A$ are positive (non-negative). This follows directly from [[Eigendecomposition]].
	- **Definition via minors**: $A$ is positive definite iff all its leading principle minors are positive.
	  **Proof**: $\Rightarrow$ follows from restricting $\mathbb R^n$ to $\mathbb R^k$ and use the first equivalence; for $\Leftarrow$, denoting $A_{I,J}$ the $(I,J)$-sub-matrix, then we prove the result via induction on $n$: the case for $n=1$ is trivial, and assuming validity for $n-1$, for $A\in M_n(\mathbb R)$ we notice that the assumption $\det A_{[n-1],[n-1]}>0$ implies linearly independence of row vectors, hence $(a_{n1},a_{n2},\cdots,a_{n,n-1})$ is the linear combination of row vectors of $A_{[n-1],[n-1]}$, and by eliminating these entries we can invoke the result on block matrix ([[Determinant]]) to conclude that the last eigenvalue for $A$ is positive, while the rest $n-1$ eigenvalues are also positive by assumption and the first equivalence, hence the proof is complete.
## Quadratic surface
By a **quadratic surface** we mean any surface determined by the equation $$f(x)=x^TAx+b^Tx=1$$We also separate the degenerated variant $$x^TAx+b^Tx=0$$Both types of quadratic surface are common in applications. 
- **Shape dependence**: the shape of the quadratic surface is determined by the coefficient of highest-order term in its canonical expression. 
  **Proof**: since $A$ is symmetric, by [[Eigendecomposition]] it's unitarily diagonalizable, i.e., we can always find $P\in\su(n,F)$ ([[Matrix Group]]) s.t. $$f(Px)=x^T\Lambda x+b^TPx\where\Lambda=\diag(\lambda_1,\cdots,\lambda_n)$$By proper translation we can always cancels the $x_i$ term out (unless $0\in\sigma(A)$, whose corresponding coefficient is invariant under translation), yielding an equation in which all $x_i$ are separated, and contains only one term of highest order, i.e., terms with eigenvalues as their coefficients (except for $x_i$ terms). This, together with the scaling-invariance discussed below, completes the proof.
	- **Scaling-invariance for quadratic form with $0\in\sigma(A)$**: for coordinate with $0$ as its corresponding eigenvalue, the ratio between coefficient with vanishing and non-vanishing eigenvalues can always be canceled out by uniform scaling $x\mapsto cx$, hence they're of the same shape up to uniform scaling. 

---

## Quadratic surfaces in $\mathbb R^3$
For quadratic forms in $\mathbb R^3$, based on the eigenvalues we can categorize them into 6 classes:
![[Pasted image 20241008085112.png]]
Below we'll go through each of them, illustrating some of the useful properties. We'll only look at the canonical form, which can always be obtained via proper linear transform of coordinate, as discussed in [[Eigendecomposition]].
- **Ellipsoid**: the canonical form of an ellipsoid is $$\frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2}=1$$which has three positive eigenvalues. 
	- **Cross sections**: any cross section of an ellipsoid is an ellipses (or circles, which is degenerated). Geometrically, this is because a ellipsoid can be obtained from a ball from pure stretching; algebraic proof can be found below.
- **Single-Sheet Hyperboloid**: the canonical form of a single-sheet hyperboloid is $$\frac{x^2}{a^2} + \frac{y^2}{b^2} - \frac{z^2}{c^2} = 1$$which has two positive eigenvalues and one negative eigenvalue.
	- **Doubly ruled surface**: since the equation can be written as $$\frac{x^2}{a^2}-\frac{z^2}{c^2}=1-\frac{y^2}{b^2}\quad\iff\quad\frac{\frac{x}{a}-\frac{z}{c}}{1-\frac{y}{b}}=\frac{1+\frac{y}{b}}{\frac{x}{a}+\frac{z}{c}}$$by assigning an explicit value to the ratio we get two planes $$\begin{cases}\mu\left(\dfrac{x}{a}+\dfrac{z}{c}\right)+\nu\left(1+\dfrac{y}{b}\right)=0\\\mu\left(1-\dfrac{y}{b}\right)+\nu\left(\dfrac{x}{a}-\dfrac{z}{c}\right)=0\end{cases}$$This determines **one family of lines** within the surface of the hyperboloid. By writing the ratio in another form yields another family of skew lines that lies in it.
		- **Why use two parameters**: one of $\mu,\nu$ might equal to zero, which shows that only specifying a ratio parameter is not sufficient, unless we allow it to take the value $\infty$, which is not preferred in the geometry setting.
	- **Characterization via skew lines**: pick three arbitrary skew lines $l_1,l_2,l_3$, and consider $$\Gamma=\set{l\subset\mathbb R^3:l\text{ is a line that intersects with each }l_i,1\le i\le 3}$$We have that $\Gamma$ is a single-sheet hyperboloid.
	  **Geometric proof**: we first proof that such $l$ exists. Consider $$\Sigma_x=\set{l:l\text{ passes }x\text{ and intersects with }l_2},\quad\forall x\in l_1$$then $\Sigma_x$ is a plane, and if we denote $\Sigma_0$ the plane that contains $l_2$ and is perpendicular to the common normal of $l_1,l_2$, then it's easy to verify that $$\bigcup_{x\in l_1}\Sigma_x=\mathbb R^3\backslash\Sigma_0\quad\Longrightarrow\quad\bigcup_{x\in l_1}\Sigma_x\cap l_3\neq\varnothing$$this implication follows due to the skewness assumption, since otherwise $l_3\in\Sigma_0$, which must either intersects with or be parallel to $l_2$.
		- **Skew lines**: we say that two space lines are skew if they're not parallel to each other, and do not intersect.
	- Any three skew lines in a space uniquely determines a single-sheet hyperboloid that contains all of them. An algebraic proof is provided below, and further geometric intuitions can be found at the end of this note.
	  1. Algebraically, a hyperboloid corresponds to a quadratic equation $$\Gamma:(r-r_0)^TQ(r-r_0)=1$$where $r_0$ is the center of the hyperboloid, and $Q$ is the parameter matrix. 
	  2. Assume that the three skew lines are represented as $l_i:r_i=P_i+td_i$ with a point and directional vector. Put $r_i$ into the equation of the hyperboloid yields$$(P_i-r_0+td_i)^TQ(P_i-r_0+td_i)=1$$Since these three lines are contained in $\Gamma$, we have $(P_i-r_0)^TQ(P_i-r_0)=1$, subtracting it from the above equation yields $$td_i^TQd_i+2d_i^TQ(P_i-r_0)=0$$
	  3. Since the above equation holds for any $t\in R$, we have $$\begin{cases}d_i^TQd_i=0&(a)\\d_i^TQ(P_i-r_0)=0&(b)\end{cases}$$Since $Q$ is symmetric, from $(a)$ we get $3$ linear homogeneous equations with $6$ variables. Thus the solution can be expressed as $Q=\alpha Q_1+\beta Q_2+\gamma Q_3$.
	  4. From $(b)$ we know that $d_i^TQP_i=d_i^TQr_0$. As $l_i$ are skew, $V=[d_1,d_2,d_3]$ spans $R^3$. Therefore we can express $Qr_0$ as $Qr_0=Vu$. Let $\Omega=[\alpha,\beta,\gamma]^T$, then we have $$[d_i^TQ_1P_i,d_i^TQ_2P_i,d_i^TQ_3P_i]^T\Omega=d_i^tVu,\quad i=1,2,3$$which can be summarized as $F\Omega=Gu$ with $F_{ij}=d_i^TQ_jP_i,G=V^TV$.
	  5. The above matrix-vector equation yields $u=G^{-1}F\Omega$, hence $Qr_0=VG^{-1}F\Omega$. Put it back gets $$Qr_0=VG^{-1}F\Omega\quad(c)$$
	  6. Back to $(b)$, consider $(b.1)-(b.2),(b.1)-(b.3)$ and put in $(c)$, we get two homogeneous linear equations with variable $\Omega=[\alpha,\beta,\gamma]^T$. Thus its solution can be represented as $\Omega=\Omega_0t$ with constant $\Omega_0=[\alpha_0,\beta_0,\gamma_0]$ and $t$ yet to be found. Put everything back to $(c)$ gets $r_0=Q_)^{-1}VG^{-1}F\Omega_0$.
	  7. Finally, since $P_i\in\Gamma$ we can obtain $$t=\frac{1}{(P_i-r_0)^TQ(P_i-r_0)}$$
- **Double-Sheet Hyperboloid**: the canonical form of a double-sheet hyperboloid is  $$\frac{x^2}{a^2} - \frac{y^2}{b^2}-\frac{z^2}{c^2} = 1$$which has one positive eigenvalue and two negative eigenvalues.
	- Separated sheets with hyperbolic cross-sections.
- **Elliptic Paraboloid**: the canonical form of an elliptic paraboloid is $$\frac{x^2}{a^2} + \frac{y^2}{b^2} = 2z$$which has two positive eigenvalues and a zero eigenvalue.
	- Parabolas along \(z\)-axis, ellipses in horizontal planes.
- **Hyperbolic Paraboloid**: the canonical form of a hyperbolic paraboloid is $$\frac{x^2}{a^2} - \frac{y^2}{b^2} = 2z$$It's saddle-shaped, hyperbolas and parabolas in cross-sections.
	- Hyperbolic paraboloid is doubly ruled. **Each family of lines are parallel to a fixed plane**.
### Other properties of quadratic surfaces
- **Cross section**: a cross section of a quadratic surface is a quadratic form.
  **Proof**: by rotation/translation we can assume WLOG that the plane used to generate cross section is $z=0$. By substituting it into a quadratic form we always get $$ax^2+bxy+cy^2+dx+ey+f=0$$which is a quadratic curve. 

---
> [!info] Geometric interpretation
> Geometrically, it can be shown that such axis exists and is unique. One way is to directly consider the axis for two skew lines, and generalize to three.
> > [!info] Relate two skew lines with rotation
> > 1. **Any two skew lines are related by infinitely many possible pure rotation**
> > 	2. For two skew lines $l_1,l_2$, pick a suitable $d>0$ and consider the cylinders with $l_i$ as axis and $d$ as radius. Their intersection form a space curve $\Gamma_d$, and any $P\in\Gamma_d$ is of the same distance $d$ to $l_1,l_2$.
> > 	3. Consider the tangent line of $\Gamma_d$ at $P$, denoted with $l_{d,P}$, then $l_{d,P}$ is of the same distance to $l_1,l_2$. It can be verified that $l_{d,P}$ that is of the same angle with two lines is the desired axis of rotation. Obviously there are infinitely many axis. [[Curve]]
> > 4. **Any three skew lines share a unique axis of rotation relating each other**
> 
> Another approach is to consider the **==representatives==** - two random points on lines.
> > [!info] Locate the lines with two points
> > As two points fully determines a lines in space, it makes sense to construct a rotation axis that only relates three pair of points.
> > 5. **Rotation axis relating two pairs of points**
> > 	6. Pick any direction $d$ that is not perpendicular or parallel to $l_1,l_2$, and arbitrarily pick two planes $\pi_1,\pi_2$ with $d$ as normal vector. Let $P_{ij}=\pi_i\cap l_j$, then we only have to find a rotation circle on each $\pi_i$, such that their center coincide after being projected to the plane. Obviously this is guaranteed to exist due to skewness. (**Conclusion**: for any two skew lines and any random direction, it is possible to pick a axis of rotation along that direction that relates two lines.)
> > 7. **Rotation axis relating three pairs of points**
> > 	8. To generalize the above idea, we need to find a proper direction such that the two circles defined by three intersecting points on each plane share the same center up to projection. It can be shown that we only have to **find a direction of the same angle to three skew lines**. (Why? **==Idea of derivative==**!)
> > 	9. Obviously for any three skew lines, their direction vector spans $R_3$, thus there are $3$ lines whose angle with three skew lines are the same. Within these $3$ lines, only one $l$ qualifies as the desired axis of rotation. (Why? Consider the ==**clockwise**== of $l_i$ w.r.t. $l$! Clockwise is preserved under rotation.)