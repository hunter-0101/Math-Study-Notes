In general, a **quadratic form** over [[Field]] $F$ is an expression $$f(x)=\sum_{1\le i,j\le n}a_{ij}x_ix_j=x^TAx$$where $x=(x_1,\cdots,x^n)$ and $A=[a_{ij}]_{n\times n}\in M_n(F)$ ([[Matrix]]). For convenience, we'll denote $S^n(F)$ the set of all symmetric matrices over the [[Field]] $F$.
- **Proper & affine quadratic form**: sometimes the above expression is referred to as a proper quadratic form, and its translations, which can be expgemiressed as $$f(x)=\sum_{1\le i,j\le n}a_{ij}x_ix_j+\sum_{i=1}^nb_ix_i=x^TAx+b^Tx$$is called an **affine quadratic form**. We'll mostly use quadratic form to refer to the proper ones as the canonical choice due to uniqueness of center.
	- **Uniqueness of center**: an affine quadratic form with non-vanishing eigenvalues is symmetric w.r.t $0$ iff it's proper.
	  **Proof**: taking the origin as symmetric center is equivalent to the identity $$f(x)=f(-x)\quad\iff\quad x^TAx+b^Tx=(-x)^TA(-x)-b^Tx$$notice that $x^TAx=(-x)^TA(-x)$, the above identity is further equivalent to $$b^Tx=-b^Tx\quad\iff\quad b=0$$which completes the proof. 
	- **Discussion on vanishing eigenvalues**: notice that when $0\in\sigma(A)$ it might happen that an quadratic form does not admit a symmetric center (like elliptic paraboloid below). In these cases we usually do not differentiate the proper and affine variant of the form. 
- **Definite-ness**: symmetric $A$ is said to be positive (semi-)definite if $$A\succ(\succeq)0\quad\iff\quad x^TAx>(\ge)0,\quad\forall x\in\mathbb R^n$$The set of all positive semi-definite matrices is denoted $S^n_+$, and the set of all positive definite matrices is denoted $S_{++}^n$. Similarly we can define negative (semi-)definite matrices, which will be denoted $S_{-}^n,S_{--}^n$,  
	- **Definition via eigenvalue**: $A$ is positive (semi-)definite iff all eigenvalues of $A$ are positive (non-negative). This follows directly from [[Eigendecomposition]].
	- **Definition via minors**: $A$ is positive definite iff all its leading principle minors are positive.
	  **Proof**: $\Rightarrow$ follows from restricting $\mathbb R^n$ to $\mathbb R^k$ and use the first equivalence; for $\Leftarrow$, denoting $A_{I,J}$ the $(I,J)$-sub-matrix, then we prove the result via induction on $n$: the case for $n=1$ is trivial, and assuming validity for $n-1$, for $A\in M_n(\mathbb R)$ we notice that the assumption $\det A_{[n-1],[n-1]}>0$ implies linearly independence of row vectors, hence $(a_{n1},a_{n2},\cdots,a_{n,n-1})$ is the linear combination of row vectors of $A_{[n-1],[n-1]}$, and by eliminating these entries we can invoke the result on block matrix ([[Determinant]]) to conclude that the last eigenvalue for $A$ is positive, while the rest $n-1$ eigenvalues are also positive by assumption and the first equivalence, hence the proof is complete.
- **Arithmetic property**
	- **Square root of $A\in S_+^n(\mathbb R)$**: by [[Eigendecomposition]] we can write $$A=P\Lambda P^{-1}=(P\Lambda^{1/2}P^{-1})(P\Lambda^{1/2}P^{-1})$$which completes the factorization. This decomposition is unique, which can be easily verified. 
	- **Sylvester's law of inertia**: in general, define the **positive/negative/null index of inertia** of $A\in S^n(\mathbb R)$ as $$n_+=\#\set{\lambda\in\sigma(A):\lambda>0},\quad n_-=\#\set{\lambda\in\sigma(A):\lambda<0},\quad n_0=n-n_+-n_-$$then these three indexes are invariant under [[Matrix]] congruence. 
	  **Proof**: by [[Eigendecomposition]] it suffices to consider $\Lambda=\diag(\lambda_i:1\le i\le n)$, which can be further transformed into its **canonical form** $$P^T\Lambda P=\diag\left(\frac{\lambda_i}{|\lambda_i|}:1\le i\le n\right)\where P=\diag\left(\frac{1}{\sqrt{|\lambda_i|}}:1\le i\le n\right)$$To see that indexes are preserved, just notice that since $P^TP$ is symmetric, there exists some $Q\in O_n$ ([[Group Gallery]]) s.t. $$QP^TPQ^{-1}=\Lambda_P\imply QP^T=\Lambda_PQP^{-1}$$hence for any other normalized diagonal matrix $\Lambda'=P^T\Lambda P$, we have $$QP^T\Lambda PQ^{-1}=Q\Lambda'Q^{-1}=\Lambda'\imply \Lambda_PQP^{-1}\Lambda PQ^{-1}=\Lambda'$$Here LHS contains a similarity with model matrix $QP^{-1}$ which, by [[Matrix]], preserves eigenvalues, while $\Lambda_P$ contains only positive entries by discussion in [[Singular Value Decomposition]], hence $\Lambda'$ has the same indexes as $\Lambda$. 
		- **Signature of a quadratic form**: we define the signature of $A$ as the tuple $$(n_+,n_-,n_0)$$Note that sometimes we also use $\Delta n=n_+-n_-$ instead since this value alone is sufficient for determining all three indexes. 
## Bilinear form
For $V$ a (finite-dimensional) [[Vector Space]] over [[Field]] $F$, a **bilinear form** is a map $$B:V\times V\to F$$that is linear in both variables. Some basic concepts and further generalizations can be found in [[Exterior Algebra]], in which we talk about $k$-linear forms ($k$-tensors).
- **Gram matrix**: given a basis $V=\span(v_i:1\le i\le n)$ a bilinear form $B$ can be characterized by its Gram [[Matrix]] representation $$B(x,y)=x^TG_By\where G_B=(B(v_i,v_j))_{n\times n}$$This follows directly from a straightforward calculation based on bilinearity. 
	- **Symmetry & Gram matrix**: obviously by definition we have $$G_B\in S^n(F)\quad\iff\quad B(u,v)=B(u,v),\quad\forall u,v\in V$$This is useful in some cases. 
	- **Change of basis formula**: given a change of basis $E'=EP$ ([[Vector Space]]) we have $$B(u,v)=x^TG_By=(P^Tx')^TG_B(P^Ty)=x'^T(PG_BP^T)y'$$while implies that the Gram matrix under the new basis is $$G_B'=PG_BP^T$$This is quite similar to that of linear operators.
	- **Intuition for Sylvester's law of inertia**: matrix congruence arises as the effect on $G_B$ by change of basis, hence invariance of the signature $(n_+, n_-, n_0)$ is essentially a reflection of the fact that behavior of $B$ itself is independent on the representation on $V$, which is quite intuitive, since after diagonalization congruence acts on eigenvalues by multiplying by a positive constant: $$B(\lambda v,\lambda v)=|\lambda|^2B(v,v)$$This is exactly the same as how we explained eigenvalues being invariant under matrix similarity via change of basis in [[Matrix]]. 
- **Associated quadratic form**: obviously any symmetric bilinear form $B$ can be naturally associated with a quadratic form $Q(u)=B(u,u)$.
	- **Definiteness**: definiteness of $B$ is defined via that of associated $Q$. 
	- **Polarization identity**: given quadratic form $Q$ its associated bilinear form $B$ is given by $$B(u,v)=\frac{1}{2}(Q(u+v)-Q(u)-Q(v))$$which is valid provided that $\char F\neq2$.
	  **Proof**: by definition we have $$Q(u+v)=B(u+v,u+v)=B(u,u)+2B(u,v)+B(v,v)$$This simplifies to the desired equality by identifying $B(u,u)=Q(u),B(v,v)=Q(v)$. 
- **$B$-orthogonal basis**: a basis $(v_i)_{i=1}^n$ of $V$ is said to be $B$-orthonormal if $$B(v_i,v_j)=\delta_i^j,\quad\forall1\le i,j\le n$$this is a direct generalization to the orthonormal basis defined on inner product space ([[Hilbert Space]]). 
	- **Orthonormalization via Gram matrix**: given $B$ positive definite, any basis $V=(v_i)_{i=1}^n$ can be $B$-orthonormalized by $$E=VG_B^{-1/2}$$It's $B$-diagonal since by the change of basis formula the new Gram matrix is $$(B(e_i,e_j))_{n\times n}=G_B'=G_B^{-1/2}G_BG_B^{-1/2}=I$$Note that form general symmetric bilinear forms we're also able to find a basis that turn $G_B$ into its canonical form, although concepts like orthogonality makes no sense for it. 