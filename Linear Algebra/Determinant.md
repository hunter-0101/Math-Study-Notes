Given [[Field]] $F$ The determinant of a square [[Matrix]] is a function $\det(A):M_n(F)\to F$, explicitly expressed as the following **Leibniz formula**:$$\det(A) = \sum_{\tau\in S_n}\sgn(\tau) \prod_{i = 1}^n a_{i\tau(i)} = \sum_{\sigma \in S_n} \sgn(\sigma) \prod_{i = 1}^n a_{\sigma(i)i}$$where $\sgn$ is the sign function of the permutation.
**Theorem**: there exists exactly one function $F:M_n(K)\to K$ which is **alternating** (its value equals zero when two components are the same), **multilinear** with respect to columns, and such that $F(I)=1$.
**Existence** is guaranteed by the Leibniz formula of determinant $\det(A)$.
**Uniqueness**: Let $F$ be such a function, and let $A = (a_i^j)_{i = 1, \dots, n}^{j = 1, \dots , n}$ be an $n \times n$ matrix. Call $A^j$ the $j$-th column of $A$, i.e. $A_j = (a_i^j)_{i = 1, \dots , n}$, so that $A = \left(A^1, \dots, A^n\right)$. Now one writes each of the $A^j$'s in terms of the $E^k$:$$A^j = \sum_{k = 1}^n a_k^j E^k$$As $F$ is multilinear, one has
$$\begin{align}F(A)& = F\left(\sum_{k_1 = 1}^n a_{k_1}^1 E^{k_1}, \dots, \sum_{k_n = 1}^n a_{k_n}^n E^{k_n}\right) = \sum_{k_1, \dots, k_n = 1}^n \left(\prod_{i = 1}^n a_{k_i}^i\right) F\left(E^{k_1}, \dots, E^{k_n}\right)\end{align}$$
From alternation it follows that any term with repeated indices is zero. The sum can therefore be restricted to tuples with non-repeating indices, i.e. permutations:
$$F(A) = \sum_{\sigma \in S_n} \left(\prod_{i = 1}^n a_{\sigma(i)}^i\right) F(E^{\sigma(1)}, \dots , E^{\sigma(n)})$$
Because F is alternating, the columns <math>E</math> can be swapped until it becomes the identity.  The $\mathrm{sgn}(\sigma)$ is defined to count the number of swaps necessary and account for the resulting sign change. One finally gets:$$\begin{align}F(A)& = \sum_{\sigma \in S_n} \mathrm{sgn}(\sigma) \left(\prod_{i = 1}^n a_{\sigma(i)}^i\right) F(I)\\& = \sum_{\sigma \in S_n}\mathrm{sgn}(\sigma) \prod_{i = 1}^n a_{\sigma(i)}^i\end{align}$$as $F(I)$ is required to be equal to $1$. Therefore no function besides the function defined by the Leibniz Formula can be a multilinear alternating function with $F\left(I\right)=1$. 
- **Intuition**: the defining properties of determinant actually makes it a **volume form** of the parallelotope spanned by the vectors as the variables. If treated as a linear transform, then it describes the multiplier applied on the volume of a hypercube.
- **Arithmetic property**
	- **Multi-linearity and alternating property**: these are just definitions.
	- **Invertibility criterion**: $A\in\gl_n(F)\iff\det A\neq0$, since they can be connected via full rank property of [[Matrix]].
	- **Multiplicativity**:  $\det(AB)=\det A\det B$.
	  **Proof**: by writing $A=(A^1,\cdots,A^n)$ the column vectors we have $$\begin{align}\det(AB)&=\det\left(\left(A^1,\cdots,A^n\right)B\right)\\&=\det\left(\sum_{i=1}^nb_{i1}A^i,\cdots,\sum_{i=1}^n b_{in}A^i\right)&\text{by matrix multiplication}\\&=\det A\cdot\sum_{\sigma\in S_n}\sgn(\sigma)\prod_{i=1}^nb_{i\sigma(i)}&\text{by alternating and multilinearity}\\&=\det A\det B&\text{by axiomatic definition of det}\end{align}$$Intuitively, using the volume form intuition above this property is basically saying that change in volume form is multiplicative, which is quite straightforward. 
		- **Inverse matrix**: as a corollary we have $\det(A^{-1})=(\det A)^{-1}$.
	- **Laplace expansion**: denote $M_{ij}$ the minor at $(i,j)$, then $$\det A=\sum_{j=1}^n(-1)^{i+j}a_{ij}M_{ij}$$this is a direct consequence of the Leibniz formula, and is useful for reducing the order of the matrix when calculating determinant. Note that this generalizes to multi-column or multi-row expansion. For a fixed $H\in S=\set{I\subset[n]:|I|=k}$ we have $$\det A=\sum_{L\in S}\varepsilon^{H,L}b_{H,L}c_{H,L}\quad\text{where}\quad\varepsilon^{H,L}=(-1)^{\sum_{h\in H}h+\sum_{l\in L}l}$$here $b_{H,L},c_{H,L}$ are the square minor of $A$ where $b_{H,L}$ is obtained by deleting from $A$ rows and columns with indices in $H,L$ respectively, while $c_{H,L}$ is obtained by taking complementary indices.
- **Purely algebraic definition**: see [[Characteristic Polynomial]].
- **Differential at identity**: notice that $T_I\gl(n,\mathbb R)\cong\mathbb R^{n\times n}$, we have $$\det_\nolimits{*,I}:\mathbb R^{n\times n}\to\mathbb R,\quad\det_\nolimits{*,I}(X)=\tr X$$This can be shown by using the curve $c(t)=e^{tX}$. [[Differential Calculus on Smooth Manifold]]
## Determinant of block matrix
Below we consider some properties of $$\det A\quad\text{where}\quad A=[A_{ij}],\quad A_{ij}\in\ F^{m_i\times m_j}$$We're mainly relying on the multi-linearity and alternating properties to perform transformation on block matrices, and for obvious reasons they preserves determinant.
**Theorem**: given [[Field]] $F$ (or commutative ring) and a commutative sub-[[Ring]] $R\subset F^{n\times n}$ we have $$\det_FM=\det_F\det_RM,\quad\forall M\in R^{m\times m}$$**Proof**: notice that we have $$\begin{pmatrix}A&b\\c&d\end{pmatrix}\begin{pmatrix}dI_{m-1}&0\\-c&1\end{pmatrix}=\begin{pmatrix}dA-bc&b\\0&d\end{pmatrix}$$Denote the first matrix as $M$, then by applying $\det_R$ to each side we get $$\left(\det_RM\right)d^{m-1}=\left(\det_R(dA-bc)\right)d$$Again we apply $\det_F$ to each side, then we get $$\det_F\left(\det_RM\right)\left(\det_F d\right)^{m-1}=\det_F\left(\det_R(dA-bc)\right)\cdot\det_Fd$$Meanwhile, applying $\det_F$ directly on both sides of the original identity we get $$\det_RM\cdot\left(\det_F d\right)^{m-1}=\det_F(dA-bc)\cdot\det_Fd$$Now we use induction on $m$ to prove the result. The case for $m=1$ is trivial, and by induction assumption applied on $m-1$ we compare two identities above to get $$\left(\det_FM-\det_F\det_RM\right)\left(\det_Fd\right)^{m-1}=0$$If we work in $F[x]$ and replace $d$ with $d_x=d+xI_n$ and revise other symbols accordingly, then each term become a polynomial in $x$, however we know that $\det_Fd$ is a monic polynomial of degree $n$, hence it's not a zero divisor, and we must have the first term being zero, i.e., $$\det_M=\det_F\det_RM$$which completes the proof.
- **Determinant of $2\times 2$ block matrix**: we have the result that $$\begin{vmatrix}A_n&B_{n\times m}\\-C_{m\times n}&I_m\end{vmatrix}=|A_n||I_m+C_{m\times n}A_n^{-1}B_{n\times m}|=|A_n+B_{n\times m}C_{m\times n}|$$For cases where $I$ is replaced by $D$ another square matrix, we have $$\begin{vmatrix}A&B\\C&D\end{vmatrix}=\begin{cases}|A|\cdot|D-CA^{-1}B|,&\text{if }|A|\neq0\\|D|\cdot|A-BD^{-1}C|,&\text{if }|D|\neq0\end{cases}$$Of course both are valid if both $A,D$ are invertible.
	- **Corollary**: $|A+uv^T|=|A|(1+v^TA^{-1}u)=|A|+v^TA^*u$.
	- **Sylvester's determinant theorem**: based on this formula, notice that $$\begin{vmatrix}I_m&B\\A&I_n\end{vmatrix}=|I_m-AB|=|I_n-BA|,\quad\forall A\in F^{n\times m},N\in F^{m\times n}$$Hence the $\chi_{AB},\chi_{BA}$ only vary by a coefficient $\lambda^{n-m}$. [[Characteristic Polynomial]]
- **Schur formula**: for square matrices $A,B,C,D$, $$\begin{vmatrix}A&B\\C&D\end{vmatrix}=\begin{cases}|AD-ACA^{-1}B|,& \text{if }|A|\neq0\\|AD-BD^{-1}CD|,&\text{if }|D|\neq0\\|AD-CB|,&\text{if }AC=CA\\|AD-BC|,&\text{if } CD=DC\end{cases}$$this is sometimes useful in special calculations.
## Useful formulas
- $$|A(t_1,t_2,\dots,t_n)|=\begin{vmatrix}a_{11}+t_1 & a_{12}+t_2 & \dots & a_{1n}+t_n \\ a_{12}+t_1 & a_{22}+t_2 & \dots & a_{2n}+t_n \\ \vdots & \vdots & & \vdots \\ a_{n1}+t_1 & a_{n2}+t_2 & \dots & a_{nn}+t_n\end{vmatrix}=|A|+\sum_{j=1}^nt_j\sum_{i=1}^nA_{ij}$$
- Compute determinant using matrix decomposition. for $n+1\times n+1$matrix$A(i,j)=(a_{i-1}+b_{j-1})^n$, we have$$A=\begin{pmatrix}1 & C_n^1a_0 & C_n^2a_0^2 & \dots & C_n^n \\ 1 & C_n^1a_1 & C_n^2a_1^2 & \dots & C_n^na_1^n \\ \vdots & \vdots & \vdots & & \vdots \\1 & C_n^1a_n & C_n^2a_n^2 & \dots & C_n^na_n^n\end{pmatrix}\begin{pmatrix}b_0^n & b_1^n & \dots & b_n^n \\ b_0^{n-1} & b_1^{n-1} & \dots & b_n^{n-1}\\ \vdots & \vdots & & \vdots \\ 1 & 1 & \dots & 1\end{pmatrix}$$thus$$|A|=C_n^1C_n^2\dots C_n^n\Pi_{0\le i<j\le n}(a_j-a_i)(b_i-b_j)$$
- For linear transform of vectors, linear independence remains when transform matrix $|A|\neq0$. With Vandermonde determinant we can construct infinitely many basis.
- Cauchy Determinant: for $a_i,b_j\in C,a_i+b_j\neq0$,$$D_n=D_n(a_1,\dots,a_n;b_1,\dots,b_n)=|\frac{1}{a_i+b_j}|=\frac{\prod_{1\le i<j\le n}(a_i-a_j)(b_i-b_j)}{\prod_{1\le i,j\le n}(a_i+b_j)}$$
- $A_n=(a_{ij})_n$, then$$D_{n+1}=\begin{vmatrix} &&&x_1\\&A&&\vdots\\&&&x_n\\y_1&\dots&y_n&z\end{vmatrix}=|A_n|z-\sum_{1\le i,j\le n}A_{ij}x_iy_j$$
### Determinant of Toeplitz matrix [[Matrix]]
- For a real n-size **skew-symmetric matrix** $A$, i.e.  $A=-A^T$, we have $|A|=0$ if n odd and $|A|=pf(A)^2$ if n even
	- For odd n, the polynomial $|A-xI|$ must have a root $\lambda$, for the corresponding eigenvector $e$, we have $\lambda=\lambda<e,e>=\dots=-\lambda<e,e>=-\lambda$, thus $\lambda=0\Rightarrow|A|=0$. This can also be obtained with the observation that $|A|=|-A^T|=(-1)^n|A|=-|A|$.
	- For even n, now we denote the size as $2n$, then$$pf(A)=\frac{1}{2^nn!}\sum_\sigma \sgn(\sigma)\prod_{i=1}^na_{\sigma(2i-1),\sigma(2i)}$$Proof given by [this pdf](http://eik.bme.hu/~palyi/topins2-2016spring/pfaffian.pdf). The fact that $|A|$ can be written as a square can be explained (unrigorously) with two observations:
		- Any skew-symmetric matrix is similar to a tridiagonal matrix.
		- The determinant of tridiagonal matrix with zero main diagonal has a recurrence of $D_n=-a^2D_{n-2}$.
## Vandermonde determinant
The **generalized Vandermonde matrix** given $x=(x_1,\cdots,x_n),a=(a_1,\cdots,a_m)$ is define as $$G_{mn}(x,a)=[x_j^{a_i}]_{mn}$$The common Vandermonde matrix is $V_n=G_{nn}(x,(0,1,\cdots,n-1))$. 
- **Vandermonde determinant**: we have that $$\det V_n=\prod_{1\le i<j\le n}(x_j-x_i)$$**Proof**: consider the [[Vector Space]] isomorphism $$\phi:\mathbb P_{n-1}\to F^n,\quad p(x)\mapsto(p(x_1),\cdots,p(x_n))$$Obviously $V_n$ is exactly the matrix for $\phi$ under the canonical basis $x_k,0\le k<n$ for $\mathbb P_{n-1}$, while by Newton interpolation formula ([[Polynomial Interpolation]]) the set $\prod_{i=0}^k(x-x_i),0\le k<n$ is also a basis, and the change of basis matrix is an upper triangular matrix with $1$'s along the diagonal, hence has determinant $1$. Now notice that matrix of $\phi$ in the new basis is $$\begin{bmatrix}1&0&0&\cdots&0\\1&x_1-x_0&0&\cdots&0\\1&x_2-x_0&(x_2-x_0)(x_2-x_1)&\cdots&0\\\vdots&\vdots&\vdots&\ddots&\vdots\\1&x_n-x_0&(x_n-x_0)(x_n-x_1)&\cdots&(x_n-x_0)(x_n-x_1)\cdots(x_n-x_{n-1})\end{bmatrix}$$which yields the desired formula by change of basis formula. [[Vector Space]]
	- **Proof via induction**: since elementary row/column transformation does not change determinant we can easily prove the formula using induction on $n$.
	- **Proof via factorization**: by Leibniz formula we have $$\det V_n\in \mathbb Z[x_1,\cdots,x_n],\quad\deg(\det V_n)=\frac{n(n+1)}{2}$$Notice that $\det V_n=0$ when $x_i=x_j$, hence by factor theorem ([[Polynomial]]) we have $(x_j-x_i)\mid\det V_n$. By arbitrariness we further deduce $$\det V_n=Q\prod_{1\le i<j\le n}(x_j-x_i)\quad\text{where}\quad Q\in F[x_1,\cdots,x_n]$$Now since the product in RHS has degree $\frac{n(n+1)}{2}$ we have $Q\in F$, and by the diagonal entries we have $Q=1$, completing the proof.
- The coefficients of the unique polynomial $c_0+c_1x+\cdots+c_{n-1}x^{n-1}$ that passes through $n$ points $(x_i,y_i)\in C^2$ with distinct $x_i$ are $$[c_0,c_1,\cdots,c_{n-1}]=[y_1,y_2,\cdots,y_n]G_n^{-1}(x,I_n)$$where $I_n=[0,1,\cdots,n-1]$
- Assume that $A=(a_{ij})_n,f_i(x)=a_{i1}+a_{i2}x+\dots+a_{in}x^{n-1}$. By additive properties of determinant, for $x_i\in C$,$$\begin{vmatrix}f_1(x_1)&f_1(x_2)&\dots&f_1(x_n)\\f_2(x_1)&f_2(x_2)&\dots&f_2(x_n)\\\vdots&\vdots&&\vdots\\f_n(x_1)&f_n(x_2)&\dots&f_n(x_n)\end{vmatrix}=|A|\det V_n(x_1,\dots,x_n)$$