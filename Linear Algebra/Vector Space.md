A set $V$ is called a **vector (linear) space** over [[Field]] $F$ if it's equipped with two binary functions (addition $+:V\times V\to V$ and scaler multiplication $\cdot:F\times V\to V$) s.t.:
1. **Group structure**: $(V,+)$ is an abelian [[Group]].
2. **Distributivity**: $(a+b)v=av+bv,a(u+v)=au+av$.
3. **Compatibility**: $a(bv)=(ab)v$.
4. **Scaler multiplication identity**: $1v=v$.
$V$ is also called a $K$-vector field. Notice that a vector space is naturally a [[Module]]
- **Direct sum decomposition**: we write:$$V=\bigoplus V_k$$if each $v\in V$ can be uniquely represented as $v=\sum v_k,v_k\in V_k$.
- **Hamel (Algebraic) basis**: a Hamel basis is a linearly independent set of [[Module]] generators. We sometimes use **Hamel-span** to emphasize that only finite combination of generators is allowed. A Hamel basis always exist for a vector space.
  **Proof**: denote $X$ the collection of linearly independent subsets of $V$. Obviously $X$ is not empty and is partially ordered ([[Order]]) by inclusion $\subset$. For any $Y\subset X$ s.t. $(Y,\subset)$ is totally ordered we pick $L_Y$ the union of all elements in $Y$, then $L_Y\in X$ is an upper bound for $Y$. Now by Zorn's lemma ([[Set Theory]]) we can pick $L_\max$ a maximal element of $X$, which is a basis for $V$ by maximality.
	- **Discussion**: the fact that $v\in V$ cannot be expressed as a $K$-linear combination of $L_\max$ is equivalent to $v$ being linearly independent of $L_\max$ replies on division ring structure of $K$, since we're using the division operation that $$rv+\sum r_iv_i=0\quad\iff\quad v=-r^{-1}\sum r_iv_i$$This is actually a characterization of division ring, as discussed in [[Module]].
	- **Dimension**: given a basis $E=\set{e_i}$ of $V$ we define $\dim_F V=|E|$ if $|E|<\infty$, and say that $V$ is infinite dimensional if such finite basis does not exists. 
	  **Well-define-ness**: when two basis contains different number of elements then the larger one must be linearly dependent, via discussion on rank of [[Matrix]].
	- If $\mathbb K\subset\mathbb C$ is a field and $E$ is an infinite dimensional Banach space over $K$, then every Hamel basis for $E$ has at least cardinality $\mathfrak c=|\mathbb C|$.
	  **Proof**: denote the basis as $\set{v_j}_{j\in\mathbb N}$, by definition of Hamel basis we have $$E=\bigcup_{n\in\mathbb N}\operatorname{span}(v_j)_{j=1}^n$$This means $E$ is a countable union of proper subspaces of finite dimension, but every finite dimensional proper subspace of a normed space is nowhere dense, thus $E$ is a first category, contradicting the Baire category theorem.
- **Isomorphism**: an isomorphism $\phi$ between two vector spaces $V,W$ is a bijective linear transform, which is define below.
## Linear operator
For vector spaces $V, W$ over [[Field]] $F$, a **linear operator (transform)**  is a mapping $T:V \to W$ that preserves addition and scalar multiplication. In this note we assume $\dim V=n,\dim W=m$ with $m,n<\infty$. More general discussion can be found in [[Hilbert Space]] and [[Operator on Hilbert Space]].
- **[[Matrix]] of linear operator**: given basis $\set{v_j}_{1\le j\le n},\set{w_i}_{1\le i\le m}$ for $V,W$ define the matrix of $T$ under the basis as the unique $A\in F^{m\times n}$ satisfying $$T(v_1,\cdots,v_n)=(T(v_1),\cdots T(v_n))=(w_1,\cdots,w_m)A$$Intuitively it's saying that $A_{ij}$ is the $w_i$ component in $T(v_j)$. The induced coordinate transform $A:F^n\to F^m$ is given by $x\mapsto y=Ax$ since we have that $$T\left(\sum_{j=1}^mx_jv_j\right)=\sum_{j=1}^nx_jT(v_j)=\sum_{j=1}^nx_j\sum_{i=1}^mA_{ij}w_i=\sum_{i=1}^mw_i\sum_{j=1}^nx_jA_{ij}=\sum_{i=1}^ny_iw_i\quad\Longrightarrow\quad y=Ax$$The usual formulas for $T\in\mathcal L(V)$ is just a special cases.
	- **Change of basis**: given $T\in\mathcal L(V,W)$ and two pair of basis $$(v_j')=(v_j)P,\quad (w_i')=(w_i)Q$$its matrix $A,B$ under old and new basis is associated, by coordinate formula, via $$T(v_j')=T((v_j)P)=T(v_j)P=(w_i)AP=(w_i')Q^{-1}AP\quad\Longrightarrow\quad B=Q^{-1}AP$$where the extraction of $P$ follows from linearity of $T$. 
- **Kernel & image**: we define $$\ker T=\set{v\in V:Tv=0},\quad\im T=\set{w\in W:Tv=w\text{ for some }v\in V}$$they're defined exactly similar to the level set and image of functions. 
	- Obviously $\ker T^k\subset\ker T^{k+1}$. More precisely we have $$\dim\left(\frac{\ker T^k}{\ker T^{k-1}}\right)\ge\dim\left(\frac{\ker T^{k+1}}{\ker T^k}\right),\quad\forall k\in\mathbb N$$since $T$ maps the right to the left in an injective fashion. As a corollary when it happens that $\ker T^m=\ker T^{m+1}$ then the kernel just stop expanding.
	- $T\in\mathcal L(V)$ is injective with $\im T$ closed iff it's bounded below, i.e., $\|Tx\|\ge c\|x\|,\forall x\in V$ for some $c>0$.
### Minimal invariant subspace decomposition
Following the notations and results in [[Jordan Normal Form]], for $T\in\mathcal L(V)$ with $\dim V<\infty$ we have the direct sum invariant subspace decomposition $$V=\bigoplus_{i=1}^rV_i=\bigoplus_{i=1}^r\bigoplus_{j=1}^{g_i}V_i^j\where V_i=\ker(\lambda_iI-T)^{a_i},\quad\lambda_i\in\sigma(T)$$Here $V_i$ are **generalized eigenspaces** spanned by generalized eigenvectors, and $V_i^j$ are **minimal invariant subspaces** spanned by one Jordan chain. and is sometimes called the primary decomposition theorem. 
- **Minimality of subspace**: this decomposition is minimal that any $V_i^j$ can be generated by one single vector. In particular, pick the generator of the Jordan chain $v_{i,j}^{m_{i,j}}$, then $$V_i^j=\span(v_{i,j}^k:1\le k\le m_{i,j})\where\begin{cases}(\lambda_iI-T)v_{i,j}^1=0\\(\lambda_iI-T)v_{i,j}^k=v_{i,j}^{k-1},&2\le k\le m_{i,j}\end{cases}$$that is, each $V_i^j$ is a cyclic subspace spanned by one (cyclic) Jordan chain. 
- **Operator decomposition**: on top of the invariant subspace decomposition, we can further decompose $T$ itself into operator components that act independently on them: $$T=\bigoplus_{i=1}^rT_i=\bigoplus_{i=1}^r\bigoplus_{j=1}^{g_i} T_i^j\where T_i^j=T|_{V_i^j},\quad T_i^j(V_i^j)\subset V_i^j$$The decomposition is also the most refined, although most of the time a decomposition into $T_i$ would suffices for the problem.
	- **Characteristic polynomial decomposition**: accordingly, their [[Characteristic Polynomial]] and minimal polynomials also factors into their irreducible components ([[Polynomial over a Field]]) following the same pattern, which can be proved easily using the Jordan normal form. 
	- **Nilpotent component**: by construction of the decomposition we have $$N_i^{a_i}=0\where N_i=\lambda_iI_i-T_i$$Sometimes writing $T_i=\lambda_iI_i-N_i$ could simplify the analysis. 
	- **Discussion**: this is the essential reason that invariant subspace decomposition is useful - it allows us to decompose the operator $T$ itself into smaller, simpler components to be handled independently on their own invariant subspace. 
- **Projection operator**: based on the decomposition above we can define $$\pi_i:V\to V_i$$as the projection onto each $V_i$. 

---
## Other info
- **Tensor product**: given two $K$-vector spaces $V,W$ where $K$ is a [[Field]], define their tensor product as an abelian [[Group]] together with $\otimes:V\times W\to V\otimes W$ satisfying $$\begin{align}(v_1+v_2)\otimes w&=v_1\otimes w+v_2\otimes w\\v\otimes(w_1+w_2)&=v\otimes w_1+v\otimes w_2\\s(v\otimes w)&=(sv)\otimes w=v\otimes(sw)\end{align}$$for all $v_i\in V,w_i\in W,s\in K$.
- **Topological vector space**: a vector space $V$ with a topology ([[Topological Space]]) is called a TVS if all its vector space operations are continuous. 
	- **Example**: [[Normed Vector Space]] is a TVS with metric topology. Continuity of operations follows directly from the axioms of norms. 