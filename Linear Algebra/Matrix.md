The set of all matrices over a [[Field]] $K$ is denoted $K^{m\times n}$. Set of square matrices is denoted $M_n(\mathbb R)$ or $M_n(\mathbb C)$. Further contents on things like $\gl,\sl$ could be found in [[Group Gallery]]. Note that matrices can actually be defined over arbitrary [[Ring]] with most of the concepts below well-defined, although absence of commutativity may bring some extra issues to be dealt with. 
- **Linear operator nature**: as discussed in [[Vector Space]], a matrix is the representation of a linear [[Operator on Hilbert Space]] given some basis: $$K^{m\times n}\cong\mathcal L(K^n,K^m)\quad\text{given}\quad K^n=\span(e_i),K^m=\span(f_j)$$Any pair of a linear operator and a basis determines a unique matrix, and conversely any matrix uniquely represents a linear operator in the pre-defined canonical basis.
- **Multiplication**: given $A_{m\times n},B_{n\times p}$ we define $C_{m\times p}=AB$ by $$c_{ij}=\sum_{k=1}^na_{ik}b_{kj},\quad\forall 1\le i\le m,1\le j\le p$$This is essentially a realization of linear operator ([[Vector Space]]) composition given sets of compatible basis. 
	- **Triple multiplication**: given matrices $A,B,C$ making $ABC$ well-defined we have $$ABC=\begin{bmatrix}a_1^T\\a_2^T\\\vdots\\ a_m^T\end{bmatrix}B\begin{bmatrix}c_1&c_2&\cdots&c_n\end{bmatrix}=[a_i^TBc_j]_{m\times n}$$where $a_i,c_j$ are row/column vectors. 
	- **Product between basis for $M_n(K)$**: obviously we have that $$M_n(K)=\span_K(E_{ij}:1\le i,j\le j)\quad\text{where}\quad E_{ij}=(\delta_i^s\delta_j^t)_{n\times n}$$their product is given by $$E_{ij}E_{st}=\delta_j^sE_{it}$$This is actually quite obvious: denote $e_i$ the canonical basis for $K^n$, then $$E_{ij}E_{st}=e_ie_j^Te_se_t^T=e_i(e_j^Te_s)e_t^T=\delta_j^s E_{it}$$This relation is sometimes useful.
- **Adjoint (conjugate transpose) matrix**: for $A\in M_n(\mathbb C)$ define $$A^*=\overline A^T=\overline{A^T}$$It's obvious that when $A\in M_n(\mathbb C)$ we have $A^*=A^T$. A useful identity is that $$\mathbb R^n=\im T\oplus\ker T^*=\im T^*\oplus\ker T$$as discussed in [[Operator on Hilbert Space]], since in finite-dimensional space any subspace is closed.
	- **Discussion on definition**: in [[Operator on Hilbert Space]] an adjoint is defined via the equality condition $$\braket{Ax,y}=\braket{x,A^*y},\quad\forall x,y\in\mathbb C^n$$As it turns out by direct computation, this is realized exactly by conjugate transpose.
- **Inverse matrix**: $B\in M^n(K)$ is called the inverse of $A\in M_n(K)$ if $AB=BA=I_n$. The set of invertible matrices is denoted $\gl(n,K)$. By [[Group]] since $I$ is a two-sided identity we know that a left inverse is immediately a right inverse, and vise versa.
	- **Inverse under operations**: we have $$(A^T)^{-1}=(A^{-1})^T,\quad\overline A^{-1}=\overline{A^{-1}}\quad\Longrightarrow\quad (A^*)^{-1}=(A^{-1})^*$$Which can be checked directly.
- **Matrix similarity**: $A,B,\in M_n(\mathbb C)$ are said to be similar if there exists $P\in\gl(n,\mathbb C)$ s.t. $B=P^{-1}AP$. By change of basis formula ([[Vector Space]]) $A\sim B$ iff they're the representations of the same linear operator under two different basis.
	- **Matrix congruence**: $A,B\in M_n(F)$ are congruent if there exists $P\in\gl(n,F)$ s.t. $$P^TAP=B$$this is obviously an equivalence relation. Matrix congruence arises as the effect of change of basis on the Gram matrix attached to a bilinear form, or [[Quadratic Form]] on a finite-dimensional vector space. 
- **Rank**: for $A\in F^{m\times n}$, its rank is defined as taking it as an [[Operator on Hilbert Space]], i.e., $$\rank A=\dim(\im A),\quad\forall A\in\mathcal L(F^n,F^m)$$Equivalently, we can define $\rank A$ as the maximal number of linearly independent column (row) vectors.
  **Proof of equivalence**: obviously $\dim(\im A)$ is the maximal number of linearly independent column vectors, and it suffices to show equivalence between definition via column and row vectors. Denoting the row vectors as $a_1,\cdots,a_m$, then an elementary row operation realizes $$(a_1,\cdots,a_m)\longmapsto(a_1,\cdots,a_i+\lambda a_j,\cdots,a_m)$$notice that two sets of vectors are equivalent, hence has the same rank. On the other hand, denoting the column vectors as $b_1,\cdots,b_n$, then an elementary row operation preserves the set of solutions ([[Solving Linear System]]), hence rank as well. Now by property of [[Field]] we can use elementary row/column operation to reduce  to the form $$A\sim\begin{bmatrix}I&0\\0&0\end{bmatrix}$$whose column rank equals row rank, hence so is $A$.  
	- **Rank-nullity theorem**: for any $A\in M_n(F)$ we have $$\rank A+\dim\ker A=n$$this follows naturally from the direct sum decomposition of $F^n$ discussed above. 
	- **Rank of operations of matrices**: by operator definition it's obvious that $$\rank(A+B)\le\rank A+\rank B$$for multiplication we have $$\rank(AB)\le\min(\rank A,\rank B)$$which is also obvious by investigating the dimensionality of image under composition.
	- **Sylvester rank inequality**: for any $A,B\in M_n(F)$ we have $$\rank(AB)\ge\rank A+\rank B-n$$**Proof**: the desired inequality is, by rank-nullity theorem, equivalent to $$\dim(\ker A)+\dim(\ker B)\ge\dim(\ker(AB))$$Consider the restriction $B|_{\ker(AB)}:\ker (AB)\to F^n$, we have that $$\ker B|_{\ker(AB)}=\ker B,\quad\im B_{\ker(AB)}\subset\ker A$$Hence invoking rank-nullity theorem again we have $$\begin{align}\dim(\ker(AB))&=\dim(\ker B|_{\ker(AB)})+\rank B|_{\ker(AB)}\\&\le\dim(\ker B)+\dim(\ker A)\end{align}$$which completes the proof. 
		- **Equality condition**: by the estimation above we have that equality holds iff $$\im B|_{\ker(AB)}=\ker A\quad\iff\quad\ker A\subset\im B$$**Proof of equivalence**: $(\Rightarrow)$ is trivial, and for $(\Leftarrow)$, take any $y\in\ker A$, then by assumption there exists $x$ s.t. $Bx=y$, hence $$ABx=A(Bx)=Ay=0\imply x\in\ker (AB)\imply y\in\im B|_{\ker(AB)}$$Arbitrariness of $y$ implies that $\ker A\subset\im B|_{\ker(AB)}$, while the inverted inclusion is also trivial, hence completing the proof. 
		- **Corollary**: by Sylvester's rank inequality we can further deduce that for $AB=0$ we have $\rank A+\rank B\le n$.
- **Invariance property**: by [[Vector Space]] similar matrices are essentially the same linear transform represented under different coordinates, hence several properties are invariant under matrix similarity: trace, [[Determinant]], eigenvalues/eigenvectors ([[Eigendecomposition]]), [[Characteristic Polynomial]], minimal polynomial, rank, etc.
- **Cyclic vector**: $v\in F^n$ is called a cyclic vector for $A\in\mathcal L(F^n)$ if $$F^n=\span(v,Av,\cdots,A^{n-1}v)$$we have that $A$ has a cyclic vector iff $\chi_A=\mu_A$.
  **Proof**: for $\Rightarrow$, denote $v$ a cyclic vector for $A$, then the matrix of $A$ under the cyclic basis is $$A=\begin{bmatrix}0&0&\cdots&\cdots&0&*\\1&0&\cdots&\cdots&0&*\\0&1&0&\cdots&0&*\\\vdots&\vdots&\ddots&\ddots&\vdots&\vdots\\0&\cdots&\cdots&1&0&*&\end{bmatrix}$$hence we can easily verify that $\lambda I-A$ has rank $n-1$, which means for each eigenvalue there is only one eigenvector, which means $\chi_A=\mu_A$ by [[Characteristic Polynomial]]. For $\Leftarrow$ we construct $v$ via generator of Jordan chain ([[Jordan Normal Form]]): $$v=\sum_{i=1}^kv_i\where(\lambda_iI-A)^{a_i-1}v_i\neq0,\quad\forall 1\le i\le k$$If we assume that there exists a list of coefficients $\alpha_j,0\le j\le n-1$ s.t. $$0=\sum_{j=0}^{n-1}\alpha_jA^jv=\sum_{j=0}^{n-1}\alpha_j\sum_{i=1}^kA^jv_i=\sum_{i=1}^k\sum_{j=0}^{n-1}\alpha_jA^jv_i$$then by linear independence of generalized eigenvectors of different eigenvalues, we have $$\left(\sum_{j=0}^{n-1}\alpha_jA^j\right)v_i=0,\quad\forall 1\le i\le k$$On the other hand, denote $A_i=A|_{V_i}$ where $V_i$ the invariant subspace generated by $v_i$, then by [[Module]] and the construction of $v_i$ we know that $$\ann_{F[A_i]}(v_i)=\left((\lambda_iI-A)^{a_i}\right)\quad\Longrightarrow\quad(\lambda_iI-A)^{a_i}\ \left|\ \sum_{k=0}^{n-1}\alpha_jA^j\right.,\quad\forall1\le i\le k$$now since $(\lambda_i-x)^{a_i}$ are pairwise coprime ([[Polynomial Ring]]) we can further deduce that $$\prod_{i=1}^k(\lambda_iI-A)^{a_i}\ \left|\ \sum_{k=0}^{n-1}\alpha_jA^j\right.$$which immediately produces a contradiction by looking at the degree.
	- **Matrix as polynomial in another**: if $A$ has a cyclic vector $v$, then for any $B$ that commutes with $A$, i.e., $AB=BA$ we have the implication $$Bv=\sum_{i=0}^{n-1}a_iA^iv\quad\Longrightarrow\quad B=\sum_{i=1}^{n-1}a_iA^i$$we can verify this directly for any $w\in F^n$. This provide an explicit method for expressing $B$ as the polynomial in $A$.
## Trace
Given $A\in M_n(\mathbb C)$ we define $\tr(A)=\sum a_{ii}$. Given an orthonormal basis $(e_i)$ we have $$\tr A=\sum_{i=1}^n\braket{e_i,Ae_i}=\sum_{i=1}^n e_i^TAe_i$$This serves as more general definition (as in [[Operator on Hilbert Space]]) since trace is invariant under matrix similarity (discussed above & proved below). 
- **Characterization properties**: the following three properties $$\begin{align}\tr(A+B)&=\tr A+\tr B\\\tr(cA)&=c\tr A\\\tr(AB)&=\tr(BA)\end{align}$$characterize $\tr$ up to a scaler multiple. The key observation for this is that, there is only one degree of freedom under these conditions: consider the basis $E_{ij}$ defined above, then $$\tr(E_{ii})=\tr(E_{ij}E_{ji})=\tr(E_{ji}E_{ij})=\tr(E_{jj}),\quad\forall i\neq j$$hence $E_{ii}=C,\quad\forall1\le i\le n$, while $$\tr(E_{ij})=\tr(E_{ii}E_{ij})=\tr(E_{ij}E_{ii})=\tr(0)=0,\quad\forall i\neq j$$The above discussion means specifying $C$ immediately determines $\tr A,\forall A\in M_n(K)$.
	- **Cyclic property**: from $\tr(AB)=\tr(BA)$ we further deduce that $$\tr(A_1A_2\cdots A_n)=\tr(A_2A_3\cdots A_nA_1)$$and this can be applied recursively to yield an equality chain.
- **Eigenvalue definition**: we have $\tr(AB)=\tr(BA)$, and applying this to $B=A^{-1}X$ we get $$\tr(A^{-1}XA)=\tr X,\quad\forall A\in\gl(b,\mathbb C)$$Hence we can define trace via [[Eigendecomposition]]: $$\tr A=\sum_{\lambda\in\sigma(A)}\lambda$$This can also be verified by Schur decomposition ([[Matrix Decomposition]]).
## Special types of matrix
- **Hermitian matrix**: $A=A^*$. 
	- **Eigenvalues**: for Hermitian $A$ we have $\lambda\in\mathbb R$. 
	- The Hermitian matrix can be diagonalized by a unitary matrix.
- **Orthogonal matrices**: $A\in M_n(\mathbb R)$ is orthogonal if $AA^T=A^TA=I$.
	- **Eigenvalue**: orthogonal matrix is always diagonalizable, with $|\lambda|=1$. The reasoning is basically the same as that of unitary matrix.
- **Unitary matrix**: $A\in M_n(\mathbb C)$ is unitary if $AA^*=A^*A=I\iff A^*=A^{-1}$. 
	- **Diagonalization**: unitary $A$ is normal (define below) by definition, hence is **unitarily similar** to a diagonal matrix, i.e., there exist $U$ unitary s.t. $A=U\Lambda U^*$. This follows from the fact that unitary matrix is inherently normal.
	- **Eigenvalue**: for $A$ unitary we have $|\lambda|=1$, since for eigenvector $v$ we have $$|\lambda|^2\braket{v,v}=\lambda\overline\lambda \braket{v,v}=\braket{\lambda v,\lambda v}=\braket{Av,Av}=\braket{v,A^*Av}=\braket{v,v}$$which immediately yield the result. 
- **Normal matrix**: $A\in M_n(\mathbb C)$ is normal if $AA^*=A^*A$. By [[Singular Value Decomposition]] or Schur decomposition ([[Matrix Decomposition]]) a matrix is normal iff it's unitarily similar to a diagonal matrix.
- A **circulant matrix** is defined by its first row, and each subsequent row is obtained by shifting the previous row one position to the right, wrapping the elements around to the first column if necessary.
	- **Eigenvalues**: Set $F(i,j)=\frac{1}{\sqrt{n}}\omega^{(i-1)(j-1)},\omega=e^{\frac{2\pi i}{n}}$ as the DFT matrix, then $C=FDF^{-1}$ where $D$ is the diagonalized matrix of $C$. [[Fourier Series]]. More explicitly, $\lambda(k)=c_0+c_1\omega^k+\dots+c_{n-1}\omega^{(n-1)k}$ for $k=0,1,\dots,n-1$.
- **Block matrix**: given $A\in K^{m\times n}$, a partition of $A$ is of the form $$A=\begin{bmatrix}A_{11}&A_{12}&\cdots&A_{1q}\\A_{21}&A_{22}&\cdots&A_{2q}\\\vdots&\vdots&\ddots&\vdots\\A_{p1}&A_{p2}&\cdots&A_{pq}\end{bmatrix}\quad\text{with}\quad A_{ij}\in K^{m_i\times n_j},m=\sum_{i=1}^pm_i,n=\sum_{j=1}^qn_j$$Addition and multiplication can be performed for two block matrices in the usual way when they're partitioned in a proper manner. In particular, for two matrices that are partitioned in the same manner, this happens only when the partition is symmetric in size.
## Matrix norm
See [[Vector Space]] for general discussion of normed space. See [[Hilbert Space]] for more general discussion of inner product space.
A vector norm should satisfies: positive-valued, definite, absolutely homogeneous, and triangle inequality. The only difference that distinguishes matrices is multiplication, thus it would be good to have sub-multiplication property: $\|AB\|\le\|A\|\cdot\|B\|$. Every norm can be rescaled to be sub-multiplicative.
- **Operator norm**: given vector norms $\|\cdot\|_\alpha,\|\cdot\|_\beta$ on $K^n,K^m$, any matrix $A_{mn}$ has the norm $$\|A\|_{\alpha,\beta}=\sup\{\|Ax\|_\beta:x\in K^n,\|x\|_\alpha=1\}=\sup\Set{\frac{\|Ax\|_\beta}{\|x\|_\alpha}:x\in K^n,x\neq0}$$When $\alpha=\beta=p$, the norm is denoted $\|A\|_p$.
	- **Column norm**: $\|A\|_1=\max_j\sum_{i=1}^m|a_{ij}|$.
	- **Row norm**: $\|A\|_\infty=\max_i\sum_{j=1}^n|a_{ij}|$.
	- **Spectral norm**: $\|A\|_2=\sup\{x^*Ay:x\in K^m,y\in K^n,\|x\|=\|y\|=1\}$ by Cauchy Schwarz inequality.
		- **Square matrix**: notice that $AA^*$ is self-adjoint, we have $\|AA^*\|_2=\rho(AA^*)$ via diagonalization ([[Spectral Theorem]]). Since $\|A\|_2=\sqrt{\|AA^*\|_2}$ ([[Operator on Hilbert Space]]), $$\|A\|_2=\sqrt{\rho(AA^*)}=\sqrt{\rho(A^*A)}$$where we used the fact that $\sigma(AA^*)=\sigma(A^*A)$ except for possible zeros. [[Singular Value Decomposition]]
			- **Corollary**: for $A=A^*$ we have $\|A\|_2=\rho(A)$.
	- **Sub-multiplication property**: $\|AB\|\le\|A\|\cdot\|B\|$ for any operator norm by its definition.
	- **Relation with spectral radius**: as long as $\|A\|_*<1$ for one $\|\cdot\|_*$ we'll have $\rho(A)<1$ by linearity of norm ([[Operator on Hilbert Space]]). Intuitively, norm controls the ensure unit ball, while [[Eigendecomposition]] only controls a small set of vectors.
- **Hilbert-Schmidt norm**: define the norm of $A$ as $$\|A\|_{HS}^2=\sum_{i\in I}\|Ae_i\|^2$$where $\set{e_i}_{i\in I}$ is a orthonormal basis.
	- **Well-defined-ness**: consider two orthonormal basis $\set{e_i},\set{f_j}$, we have $$\sum_i\|Ae_i\|^2=\sum_{i,j}\left|\braket{Ae_i,f_j}\right|^2=\sum_{i,j}\left|\braket{e_i,A^*f_j}\right|^2=\sum_j\|A^*f_j\|^2$$hence by taking $\set{e_i}=\set{f_j}$ we get $\|A\|_{HS}=\|A^*\|_{HS}$, and replace $A$ with $A^*$ we get $\sum_i\|Ae_i\|^2=\sum_j\|Af_j\|^2$, which yields independence.
	- **Relation with Frobenius norm**: by picking $e_i$ as the standard orthonormal basis we can verify that the HS-norm actually coincide with F-norm. However, a crucial distinction between these two definitions is that, F-norm is defined only for matrices, and are basis-dependent, while HS-norm can be defined for any [[Operator on Hilbert Space]] on [[Hilbert Space]] and is independent of basis.
- **Entry-wise matrix norms**: we could also define $$\|A\|_{p,p}=\|\operatorname{vec}(A)\|_p=\Big(\sum_i\sum_j|a_{ij}|^p\Big)^{\frac{1}{p}}$$This kind of norm just reshape the matrix into vector and apply the vector norm, and the general form is usually denoted as $L_{p,q}$ norm. In this case $$\|A\|_{p,q}=\Bigg(\sum_j\Big(\sum_i|a_{ij}|^p\Big)^{\frac{q}{p}}\Bigg)^\frac{1}{q}$$Note that most of the norms are not so commonly used in different fields of math.
	- **Frobenius norm**: we define $$\|A\|_F=\sqrt{\sum_{1\le i,j\le n}|a_{ij}|^2}=\sqrt{\tr(AA^*)}=\sum_{i=1}^n\sigma_i^2$$here $\sigma_i$ are singular values ([[Singular Value Decomposition]]) of $A$. By Cauchy-Schwarz inequality ([[Lp Space]]) we have $\|A\|_F\le\|A\|_2$, with equality achieved when $\rank A=1$.
## Matrix exponential
A partial motivation for matrix exponential is that, to calculate the differential of functions on $\gl(n,\mathbb R)$ we need a sequence of non-singular matrices, and since $\gl(n,\mathbb R)$ is only dense in $M_n(\mathbb R)$ matrix exponential is a natural choice. [[Differential Calculus on Smooth Manifold]]
The exponential of a matrix $A$ is defined as $$e^A=\sum_{k=0}^\infty\frac{A^k}{k!}=\lim_{k\to \infty}\left(I+\frac{A}{k}\right)^k$$The limit exists since $\Set{\sum_{k=0}^n\frac{\|A\|^k}{k!}}$ is a Cauchy sequence, where $\|\cdot\|$ is the Euclidean norm ([[Vector Space]]). [[Banach Space]]
- **Normed algebra structure**: $M_n(\mathbb R)$ is a normed algebra under the above norm. [[Field]]
- **Basic calculation properties**
	- If $XY=YX$, then $e^{X+Y}=e^Xe^Y$, by distributivity of normed vector space ([[Field]]).
	- $\frac{d}{dt}e^{tX}=Xe^{tX}$, which follows from similar proof as above.
	- If $|T|\neq0$ then $\exp(T^{-1}AT)=T^{-1}\exp(A)T$.
	- $\det(e^A)=e^{\tr(A)}$. First prove the result for upper triangular matrix, then make use of the Schur decomposition. [[Matrix Decomposition]]
- **Computation**: from Cayley-Hamilton ([[Characteristic Polynomial]]) we know that any $A^m$ with $m>n$ can be reduced to a polynomial of $A$ of degree less than $n$, thus we can write $$e^A=a_{n-1}A^{n-1}+\cdots+a_1A+a_0I$$Assume that $A$ has eigenvalues $\lambda_i(1\le i\le n)$, then we have $e^{\lambda_i}=\sum_{k=0}^{n-1}a_k\lambda_i^k$. For eigenvalues with $a_i>1$, we also have $e^{\lambda_i}=\frac{d^s}{d\lambda^s}\sum_{k=0}^{n-1}a_k\lambda^k|_{\lambda=\lambda_i}(1\le s\le a_i-1)$. Combining all the equations, would determine the value of $a_k$.
## Moore-Penrose inverse
Moore-Penrose inverse of $A\in\mathbb K^{m\times n}$, Also called the **pseudoinverse**, is a matrix $A^\dagger\in\mathbb K^{n\times m}$ s.t.
1. **Weak inverse**: $AA^\dagger A=A, A^\dagger AA^\dagger=A^\dagger$.
2. **Hermitian condition**: $(AA^\dagger)^*=AA^\dagger,(A^\dagger A)^*=A^\dagger A$.
Given SVD $A=U\Sigma V^*$, then $A^\dagger=V\Sigma^\dagger U^*$ where $\Sigma^\dagger$ is obtained by replacing $\sigma\neq0$ with $1/\sigma$.