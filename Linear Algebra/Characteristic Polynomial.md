For a [[Matrix]] $A$ with eigenvalues $\lambda_k$ its **characteristic polynomial** is defined as $$\chi_A(x)=\det(xI-A)=\prod_{k}(x-\lambda_k)$$For linear operator $T$ ([[Vector Space]]) we define instead $\chi_T(x)=\prod(x-\lambda_k)$, which is well-defined by invariance of [[Eigendecomposition]] under matrix similarity. Here we're assuming that the underlying field to be $\mathbb C$. Validity of the above splitting expression depends on whether the field is algebraically closed.
- **Interpretation**: by the volume form interpretation ([[Other Algebra Tools]]) $\chi_A$ detects $x$ when $xI-A$ collapses into a singular transform, which happens exactly when an eigenvector exists. It's essentially a universal obstruction to invertibility for the operator family $\set{xI-A:x\in\mathbb C}$. Its invariance under similarity origins from the simple fact that **volume distortion is intrinsic**, not coordinate-dependent.
- **Coefficients**: let $A\in\gl(n,F)$, then the coefficients are given by $$\chi_A(x)=\sum_{i=0}^n(-1)^{n-i}\tr\left(\bigwedge\nolimits^{n-i}A\right)x^i$$via wedge power ([[Exterior Algebra]]), with trace defined for [[Operator on Hilbert Space]].
  **Proof**: it's obvious from definition of exterior product that $$\sigma\left(\bigwedge\nolimits^{k}A\right)=\set{\lambda_{i_1}\cdots\lambda_{i_k}:(i_1,\cdots,i_k)\subset[n]}$$hence by a simple [[Polynomial]] expansion we get the result.
- **Cayley-Hamilton theorem**: $\chi_A(A)=0$. This is a direct result of the [[Jordan Normal Form]]. 
	- **Geometric proof**: it suffices to show $\chi_T(T)v=0,\forall v\in V$. For this, notice that $$W=\span(v,Tv,T^2v,\cdots)\subset V\quad\Longrightarrow\quad\exists k\in\mathbb N,a_0,\cdots,a_k\in\mathbb C\text{ s.t. }f(T)(v)=\sum_{i=0}^ka_iT^iv=0$$here $f$ is the polynomial of least possible degree, which satisfies $\dim W=k$. Hence $f$ is exactly the characteristic polynomial of $T|_W$, and by definition we have $\chi_{T|_W}\mid \chi_T$. This implies $\chi_T(T)v=0$, and arbitrariness of $v$ completes the proof.
	- For functions of $A$, we have $\chi_{f(A)}(x)=\prod_i(x-f(\lambda_i))$.
- **Properties**
	- For two square matrices $A,B$, we always have $\chi_{AB}=\chi_{BA}$. This can be proven by assuming $A=P\begin{pmatrix}I&0\\0&0\end{pmatrix}Q$ and transform $B$ into block matrix using the same transform matrix. The expression of $AB$ and $BA$ directly yields the result.
## Minimal polynomial
The **minimal polynomial** $\mu_A$ is the polynomial of least degree s.t. $\mu_A(A)=0$. 
- **Expression of $\mu_A$**: adopting notations in [[Jordan Normal Form]] we have $$\mu_A(x)=\prod_k(x-\lambda_k)^{d_k}\quad\text{where}\quad d_k=\max_im_{k,i}$$**Proof**: it suffices to consider the Jordan form $A\cong J=\diag(J_{k,i})$ via similarity. For each $J_{k,i}$ of size $m_{k,i}\times m_{k,i}$ to annihilate its $1$'s on the super-diagonal we have $$\mu_{k}(J_{k,i})=(J_{k,i}-\lambda_kI_{m_{k,i}})^{d_k}=0\quad\Longrightarrow\quad d_k\ge m_{k,i}$$Hence to ensure that $\mu_A=\prod\mu_k=\prod(x-\lambda_k)^{d_k}$ annihilates of all Jordan blocks we need $$d_k=\max_im_{k,i}\quad\Longrightarrow\quad\deg\mu_A=\sum_k\max_i m_{k,i}$$This completes the proof.
	- **Relation with characteristic polynomial**: $\mu_A(x)\mid \chi_A(x)$ by [[Polynomial]], and they equals iff each eigenvalue corresponds to a single Jordan chain. 
	  **Proof**: by the above expression we have that $$\mu_A=\chi_A\quad\iff\quad\sum_k\max_im_{k,i}=\deg\mu_A=\sum_ka_k\quad\iff\quad a_k=m_{k,i},\forall k,i$$Here we used the fact that $a_k=\sum_im_{k,i}$. [[Matrix]]
	- **Relation with diagonalizability**: by the expression above we also know that $A$ is diagonalizable iff all roots of $\mu_A$ are simple.
## Exterior algebra perspective on Cayley-Hamilton theorem
For a free [[Module]] $M$ over commutative [[Ring]] $R$ with $\rank M=r$, similar to [[Exterior Algebra]] we can define $\bigwedge M$, and by the theory of [[Polynomial Ring]] consider $$D(t)=\sum_{n\in\mathbb N}D_n t^n\in\End_R\left(\bigwedge M\right)[[t]],\quad D(t):\bigwedge M\to\bigwedge M[[t]]$$We say that $D(t)$ is invertible if there exists $\overline D(t)$ s.t. $D(t)\overline D(t)=\overline D(t)D(t)=1_{\bigwedge M}$, which, by [[Euclidean Domain]], is further equivalent to $D_0\in\aut(\bigwedge M)$.
- **Hasse-Schmidt derivation**: $D(t)\in\End_R(\bigwedge M)[[t]]$ is referred to as a HS-derivation if any one of the following two equivalent conditions holds: 
  1. $D(t)(\alpha\wedge\beta)=D(t)\alpha\wedge D(t)\beta,\forall\alpha,\beta\in\bigwedge M$.
  2. $D_n(\alpha\wedge\beta)=\sum_{i=0}^nD_i\alpha\wedge D_{n-i}\beta,\forall\alpha,\beta\in\bigwedge M, n\in\mathbb N$.
  **Proof**: for $\Rightarrow$ it suffices to expand two sides and compare the coefficient for $t^n$; for $\Rightarrow$ we only need to traverse backwards.
	- **Group of HS-derivations**: the collection of HS-derivations on $\bigwedge M$ is denoted $\text{HS}(\bigwedge M)$. This set is closed under multiplication and inversion (verified directly from definition), hence forms a [[Group]].
	- **Integration by parts**: denote $\overline D(t)$ the inverse of $D(t)\in\text{HS}(\bigwedge M)$, then $$D(t)\alpha\wedge\beta=D(t)\alpha\wedge D(t)\overline D(t)\beta=D(t)(\alpha\wedge\overline D(t)\beta)$$
For more, refer to *Remarks on the Cayley-Hamilton theorem* by L. Gatto. & I. Scherbak.
## Purely algebraic definition
Let $F$ be a [[Field]] and $A$ be a $F$-algebra with $\dim_FA=m<\infty$, and $a_i$ an $F$-basis of $A$. Denote $R=F[X_1,\cdots,X_m]$ the [[Polynomial Ring]] of for commuting indeterminate $X_i$, and let $K=\Frac(R)$ be the field of fraction of $R$ ([[Field]]). Now take a generic element $\gamma=\sum a_iX_i\in A\otimes_FK$, then $$\span_K(1,\gamma,\gamma^2,\cdots)\subset A\otimes K$$while by [[Vector Space]] we have $$\dim_K(A\otimes_FK)=\dim_FA<\infty$$Hence there exists a monic polynomial of least degree $\mu_{\gamma/K}\in K[X]$ s.t. $\mu_{\gamma/K}(\gamma)=0$, which we refer to as the **minimal polynomial** of $\gamma$ over $K$. Further given $a=\sum\alpha_ia_i\in A$ where $\alpha_i\in F$, the substitution ([[Polynomial Ring]]) $X_i\mapsto\alpha_i$ defines a map $R[X]\to F[X]$. The image of $\mu_{\gamma/K}$ under the map, denoted $\chi_{a,A/F}$, is called the **characteristic polynomial** of $a$.
- **Field of coefficients**: we actually have $\mu_{\gamma/K}\in R[X]$.
  **Proof**: consider the $R$-submodule ([[Module]]) $A_j$ of $A\otimes R$ generated by $\set{1,\gamma,\cdots,\gamma^j}$, then we have an ascending chain $A_1\subset A_2\subset\cdots$. Since $R$ is Noetherian by Hilbert's basis theorem ([[Polynomial Ring]]) while $A\otimes R$ is a finitely generated $R$-module, the chain eventually stabilize, i.e., there exists $j\in\mathbb N$ s.t. $\gamma^{j+1}\in A_j$, hence $\gamma$ satisfies a monic polynomial $f\in R[X]$. Since $\mu_{\gamma/K}\mid f$ we have, by Gauss's lemma, that $\mu_{\gamma/K}\in R[X]$.
- **Invariance of characteristic polynomial**: it's immediate by the above construction that $$\deg\chi_{a,A/F}\le\dim_FA$$and the degree is the same across all $a\in A$. Moreover, we have that $\chi_{a,A/F}$ only depends on $a,A$, and $F$, and not on the the choice of basis.
  **Proof**: given another basis $b_i$ we write $b_i=\sum_j g_{ij}a_j$ with $\varepsilon=\sum b_iX_i$, and define $$f:R\to R,\quad f(X_j)=\sum_iX_ig_{ij}$$Now for $a=\sum\alpha_j a_j=\sum\beta_ib_i$ we have the diagram
  <p align="center"><img align="center" src="https://i.upmath.me/svg/%0A%5Cbegin%7Btikzcd%7D%0A%7BR%5BX%5D%7D%20%5Carrow%5Br%2C%20%22X_j%5Cmapsto%5Calpha_j%22%5D%20%5Carrow%5Bd%2C%20%22f%22'%5D%20%26%20%7BF%5BX%5D%7D%20%5Carrow%5Bd%2C%20equal%5D%20%5C%5C%0A%7BR%5BX%5D%7D%20%5Carrow%5Br%2C%20%22X_i%5Cmapsto%5Cbeta_i%22'%5D%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%26%20%7BF%5BX%5D%7D%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%0A%5Cend%7Btikzcd%7D%0A" /></p>
  Since $\alpha_j=\sum_i\beta_ig_{ij}$ we know that the diagram commutes. Meanwhile, since $f$ extends to an automorphism of $A\otimes R$ in an obvious way given by $$f(\gamma)=\sum_jf(X_j)a_j=\sum_j\left(\sum_iX_ig_{ij}\right)a_j=\sum_iX_i\left(\sum_j g_{ij}a_j\right)=\varepsilon$$hence $f(\mu_{\gamma/K})=\mu_{\varepsilon/K}$. The image in $F[X]$ is $\chi_{a}$ computed w.r.t $b_i$, hence commutativity of the diagram completes the proof.
	- **Invariance across fields**: consider $F\subset E$ a field extension, then $\mu$ and $\chi$ over two fields are the same for fixed $a\in A$.
	  **Proof**: to put precisely, we're trying to show that $$\mu_{a/F}=\mu_{(a\otimes1)/E},\quad\chi_{a,A/F}=\chi_{(a\otimes1),(A\otimes E)/E}$$Since $F$ is a field, $a$ generates a free submodule $B$ of $A$ with basis $1,a,a^2,\cdots,a^{d-1}$ with $d=\deg\mu_{a/F}$ ([[Module]]). Then $B\otimes E$ is a free $E$-module with the same basis, hence $\deg\mu_{(a\otimes 1)/E}\ge d$. Since we also have $\mu_{(a\otimes1)/E}\mid\mu_{a/F}$ we immediately know that they're equal. Moreover, since the $F$-basis $a_1,\cdots,a_m$ of $A$ gives an $E$-basis $a_1\otimes1,\cdots,a_m\otimes 1$ of $A\otimes E$, with similar construction of generic elements, under substitution homomorphism we get that $\chi_{a,A/F}=\chi_{(a\otimes1),(A\otimes E)/E}$.
- **Matrix definition as a special case**: take $F=\mathbb R,A=M_n(\mathbb R)$, then we have $\dim_FA=n^2$ with basis $a_{ij}=E_{ij}$, hence $R=\mathbb R[X_1,\cdots,X_{n^2}]$. By taking $K=\Frac(R)$ we realize a **generic matrix** $$\gamma=\sum_{i=1}^{n^2} a_iX_i=\sum_{i,j=1}^na_{ij}X_{ij}\in M_n(\mathbb R)\otimes_\mathbb RK\cong M_n(K)$$Under polynomial evaluation $X_{ij}\mapsto r_{ij}\in\mathbb R$ we're evaluating the polynomial $\mu_{\gamma/K}$ at a specific $M=[r_{ij}]\in M_n(\mathbb R)$, which is specialized to the characteristic polynomial of $M$.
	- **Why pick $K$ as field of fraction**: this step is essentially generalizing the process of elementary matrix transformations. As described by the above expression, in this way polynomial of indeterminates are allowed as being entires of matrix, which is necessary for extension of scaler.
	- **Why $\mu_{\gamma/K}$ specialize to $\chi_{a,A/F}$**: by evaluation homomorphism we actually just find a polynomial that annihilates $M$. Moreover this is the **worst case** since we're requiring $\mu_{\gamma/K}$ to annihilates all matrices of the form $\gamma$, and it's very likely that after substitution the resulting matrix would have an annihilating polynomial of smaller degree. It's a subtle algebraic phenomenon that, **minimality is not preserved under specialization, while annihilation is**.
- **[[Determinant]]**: write $$\chi_{a,A/F}(x)=x^n-c_1(a)x^{n-1}+\cdots+(-1)^{n-1}c_{n-1}(a)x+(-1)^nc_n(a)$$then we define $$\det_\nolimits{A/F} a=c_n(a),\quad\tr_{A/F}=c_1(a)$$Several properties can be derived directly from this definition. 