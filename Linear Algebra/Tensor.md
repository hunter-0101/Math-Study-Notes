Let $V_1, V_2, \dots, V_n$ be finite-dimensional [[Vector Space]] over a [[Field]] $F$. The **tensor product** $$T=V_1 \otimes V_2 \otimes \dots \otimes V_n$$is the unique vector space (up to unique isomorphism) such that there exists a multilinear map $\phi: V_1 \times V_2 \times \dots \times V_n \to T$ satisfying the [[Universal Property]]: 
<p align="center"><img align="center" src="https://i.upmath.me/svg/%0A%5Cbegin%7Btikzcd%7D%0A%5Cprod_%7Bk%3D1%7D%5EnV_k%20%5Carrow%5Br%2C%20%22%5Cphi%22%5D%20%5Carrow%5Brd%2C%20%22f%22'%5D%20%26%20T%3D%5Cbigotimes_%7Bk%3D1%7D%5EnV_k%20%5Carrow%5Bd%2C%20%22%5Ctilde%20f%22%5D%20%5C%5C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%26%20W%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%0A%5Cend%7Btikzcd%7D%0A"/></p>
for any vector space $W$ and any multilinear map $f: V_1 \times \dots \times V_n \to W$. Elements of $T$ are called **tensors**. An element that can be written as $v_1 \otimes \dots \otimes v_n$ is a **simple (or rank-1) tensor**. 
- **Discussion**: the key of tensor product, compared to cartesian product, is that it introduces a way of adding tensors, turning the product set into a vector space itself. This is reflected by the universal property that $W$ is a vector space. 
	- **Tensor product of [[Module]]**: the above definition generalizes to tensor product of modules by replacing $W$ with $G$ an abelian [[Group]]. We usually denote the tensor product of two $R$-modules $M,N$ as $M\otimes_RN$ to emphasize dependence on the ring of scaler. 
- **Extrinsic definition**: given two $K$-vector spaces $V,W$ where $K$ is a [[Field]], define their tensor product as an abelian [[Group]] together with $\otimes:V\times W\to V\otimes W$ satisfying $$\begin{align}(v_1+v_2)\otimes w&=v_1\otimes w+v_2\otimes w\\v\otimes(w_1+w_2)&=v\otimes w_1+v\otimes w_2\\s(v\otimes w)&=(sv)\otimes w=v\otimes(sw)\end{align}$$for all $v_i\in V,w_i\in W,s\in K$. Tensor product of multiple vector spaces can be defined recursively. 
- **Tensor rank:** The rank of a tensor $T$ is the minimum number of simple tensors whose sum equals $T$. Unlike matrix rank, computing tensor rank for $n \ge 3$ is NP-hard.
- **Dimensionality**: by definition we have $$\dim\bigotimes_{k=1}^nV_k = \prod_{k=1}^n \dim V_k$$This can also be seen from the basis constructed below. 
- **Basis**: if each $V_k$ has a basis $\Set{e^{(k)}_{i}}_{1\le i\le \dim V_k}$, then the basis of $T$ can be constructed as $$\Set{e^{(1)}_{i_1} \otimes \cdots \otimes e^{(n)}_{i_n}:1\le i_k\le\dim V_k,1\le k\le n}$$This is quite natural, and can be verified directly. 
- **Isomorphism to multilinear maps:** There is a natural isomorphism $$V_1^* \otimes \dots \otimes V_n^* \cong\mathcal L(V_1, \dots, V_n; F)$$where the RHS is the space of scalar-valued multilinear forms.
- **Einstein summation convention**: in the field of tensor analysis we'll define the abbreviation $$x^iy_i\coloneqq\sum_{i}x^iy_i$$In general, whenever there exists an index that appears both as a superscript and a subscript in an expression, then it signifies a summation over all possible values of that index. 
## Tensor over a vector field
Below we'll assume that $V_k\in\Set{V,V^*},1\le k\le n$ in the definition above. In general, we define $$T^p_q(V) = \underbrace{V \otimes \dots \otimes V}_{p} \otimes \underbrace{V^* \otimes \dots \otimes V^*}_{q}$$Elements in this tensor product is called a $(p,q)$-tensor (or of type $(p,q)$). Here $p$ is called the **contravariance order**, $q$ the **covariance order**, and $p+q$ the **total order**. Motivation for these names is discussed below. 
- **Tensor algebra**: the tensor algebra over $V$ is constructed by adjoining all $k$-tensors together into a large vector space, with tensor product being the multiplication ([[Field]]): $$T(V)=\bigoplus_{k=0}^\infty T^kV=\bigoplus_{k=0}^\infty\bigotimes_{i=1}^kV$$This obviously a graded algebra with the set of $k$-tensors $T^kV$ being a vector space. 
- **Tensor component**: for a bidual system ([[Algebraic Dual Space]]) $\braket{e^j,e_i}=\delta_i^j$, the **components** of a tensor $T\in T^p_q(V)$ is defined as $$T=T^{i_1\cdots i_p}_{j_1\cdots j_q}e_{i_1}\otimes\cdots e_{i_p}\otimes e^{j_1}\otimes\cdots\otimes e^{j_q}\qiffq T^{i_1\cdots i_p}_{j_1\cdots j_q}=T\paren{e^{i_1},\cdots,e^{i_p},e_{j_1},\cdots,e_{j_q}}$$This is basically the same as the [[Matrix]] component of a linear operator. 
- **Contraction**: contraction operator sends a $(p,q)$-tensor to a $(p-1,q-1)$-tensor by applying the canonical pairing to the contravariant index $i$ and covariant index $j$:  $$C^i_j: T^p_q(V) \to T^{p-1}_{q-1}(V),\quad \paren{C^i_j T}^{i_1 \dots \hat{i} \dots i_p}_{j_1 \dots \hat{j} \dots j_q} =T^{i_1 \dots k \dots i_r}_{j_1 \dots k \dots j_s}$$This is the primary method for generating invariants. For instance, the Ricci scalar $R$ is obtained by contracting the Ricci tensor $R_{\mu\nu}$ with the inverse metric $g^{\mu\nu}$ (a $(2, 0)$ tensor). 
	- **Contraction as a generalization of trace**: any [[Matrix]] can be seen as the coordinate expression of $(1,1)$-tensor, and its trace is exactly $$\tr A=A_1^1+A_2^2=C_1^1A=A_k^k$$Hence contraction essentially generalizes the trace into the theory of tensors. 
## Covariance & Contravariance
For finite dimensional vector space $V$ and its [[Algebraic Dual Space]] $V^*$, we distinguish between vectors based on **how their components transform under change of basis**. Let $\set{e_i},\set{\tilde{e}_j}$ be bases for $V$, related by the transformation matrix $A$: $$\tilde e=Ae\qiffq\tilde{e}_j =A^i_j e_i $$Note that the expression above is purely formal for simplicity, rather than following the rigorous rules of [[Matrix]] representation. 
1. **Contravariance (upper indices)**: components $v^i$ of a vector $v \in V$ are contravariant if they transform against (opposite to) the basis transformation to keep the vector $v$ invariant: $$v =\sum_{i=1}^n v^i e_i =\tilde{v}^j \tilde{e}_j \imply \tilde{v}^j =\paren{A^{-1}}^j_i v^i$$Intuitively, when the basis changes the components need to change in a way contrary to the basis vector to cancel out the effect. For instance, if $\tilde e_i=2_i$ then obviously we need $\tilde v^i=v^i/2$ to get the same vector. 
	- **Example: velocity vectors**: consider a particle path $\gamma(t)$ in $\mathbb{R}^n$. The velocity $v = \frac{d\gamma}{dt}$ has components $v^i = \frac{dx^i}{dt}$. Under coordinate transformation $x^j = x^j(x^i)$, by chain rule: $$\frac{dx^j}{dt} = \frac{\partial x^j}{\partial x^i} \frac{dx^i}{dt}$$The components transform via the Jacobian matrix, characterizing $v$ as a contravariant tensor of rank 1.
 2. **Covariance (lower indices)**: components $\omega_i$ of a covector $\omega \in V^*$ are covariant if they transform "with" (in the same manner as) the basis vectors: $$\omega=\omega_i e^i=\tilde\omega_j\tilde e^j\imply \tilde{\omega}_j =A^i_j \omega_i$$Intuitively, when the basis changes the associated covectors would change in the opposite way, hence components of covectors need to change along the direction of basis to cancel the change in covectors out. 
	 - **Example: gradient of scalar field**: let $f: V \to F$ be a smooth function. Components of the gradient $\nabla f$ are $\omega_i = \frac{\partial f}{\partial x^i}$. Under a change of coordinates $x^i = x^i(\tilde{x}^j)$: $$\frac{\partial f}{\partial \tilde{x}^j} = \frac{\partial f}{\partial x^i} \frac{\partial x^i}{\partial \tilde{x}^j}$$Here, the components transform using the same matrix $A^i_j = \frac{\partial x^i}{\partial \tilde{x}^j}$ that relates the bases. This illustrates the gradient as a covariant tensor of rank 1.
The distinction between $V$ and $V^*$ necessitates a bookkeeping system for multilinear maps. A tensor of type $(p,q)$ acts on $r$ covectors and $s$ vectors to produce a scalar: $$\mathcal{T}: \underbrace{V^* \times \dots \times V^*}_{p} \times \underbrace{V \times \dots \times V}_{q} \to F$$The transformation law for a general tensor $T^{i_1 \dots i_r}_{j_1 \dots j_s}$ is a product of $r$ contravariant and $s$ covariant transformation matrices, providing a rigorous classification of physical and geometric quantities based on their symmetry and response to coordinate shifts.
## Example
- **Endomorphism tensor**: for $\dim V<\infty$ we have the canonical isomorphism $$\Psi: V \otimes V^* \to \text{End}(V), \quad \Psi(v \otimes \omega)(u) = \omega(u)v$$This allows us to treat linear operators as $(1, 1)$-tensors. The trace of an operator $L$ is then simply the tensor contraction of its $(1, 1)$ representation: $\tr L= L^i_i$. 