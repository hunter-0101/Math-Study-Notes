**Motivation**: from [[Vector Space]] it's known that any invariant subspace contains least one eigenvector (treated as a truncated sub-matrix). For $A=A^*$ we can easily verify that $A$ has the following property: $$\text{if }W\text{ is an invariant subspace, }x\in W\quad\Longrightarrow\quad W\cap\span(x)^\perp\text{ is also an invariant subspace}$$Combining the above two facts we can easily find an orthonormal basis $\set{e_i}$ that diagonalizes $A$. It turned out that this can be generalized to a wider range of matrices, resulting in the theory of eigen-decomposition.

---
An **eigenvalue** is a root to the [[Characteristic Polynomial]]. Note that most theory below actually applies to $M_n(F)$ for any algebraically closed [[Field]] $F$, although we'll proceed our discussion under the assumption that $F=\mathbb C$. 
An **eigenvalue-eigenvector pair** of a matrix $A$ is a pair $(\lambda,x)$ s.t. $$Ax=\lambda x$$If the set of eigenvectors form a basis $\set{e_i}$ then $A$ is said to be **diagonalizable**, and the decomposition of $V$ into eigen-spaces is the eigen-decomposition. We'll denote $$\sigma(A)=\set{\lambda:Ax=\lambda x\text{ for some }x},\quad\rho(A)=\max_{\lambda\in\sigma(A)}|\lambda|$$and call them the **spectrum** and **spectral radius** of $A$.
- **Continuity**: it's obvious that eigenvalues are continuous w.r.t. entry-wise perturbation since $|\lambda_i-A|$ is a polynomial in $\lambda$ ([[Polynomial]]). Further the eigenvectors are also continuous (in any norm) since it's the solution to the linear equation $(\lambda I-A)x=0$.
- **Diagonalization criterion**: $A$ diagonalizable iff for all its eigenvalues, $a_i=g_i$. 
	- **Minimal polynomial**: diagonalizability $\iff$ the minimal polynomial of $A$ splits and has distinct roots. [[Characteristic Polynomial]]
	  **Proof**: for $\Leftarrow$, we consider the minimal polynomial $m_A(T)=\prod(T-\lambda_i)$. It suffice to show that $V=\bigoplus E_i$ where $E_i$ is the eigenspace. The core idea is to **==construct projection via polynomial==**. We wanna construct some $h_i(x)$ such that $h_i(A)v$ is projected into $E_i$ and $v=\sum h_i(A)v$.
- **Finite dimensional spectral theorem**: Hermitian [[Matrix]] is diagonalizable. The essential idea of proof is stated in the the motivation section.
	- **Proof via triangularization**: by Schur decomposition ([[Matrix Decomposition]]) we always have $UHU^*=W$ where $U$ unitary and $W$ upper triangular. Since $H$ is Hermitian it can be verified that $W$ is also Hermitian, however the only Hermitian triangular matrix is $kI,k\in\mathbb C$, hence $H$ can be diagonalized via unitary matrix.
	- **Intuition**: it's know that for $\lambda_1\neq\lambda_2$ of Hermitian $H$ we have $$\lambda_1\braket{v_{\lambda_1},v_{\lambda_2}}=\braket{Hv_{\lambda_1},v_{\lambda_2}}=\braket{v_{\lambda_1},Hv_{\lambda_2}}=\lambda_2\braket{v_{\lambda_1},v_{\lambda_2}}\quad\Longrightarrow\quad v_{\lambda_1}\perp v_{\lambda_2}$$Consider the perturbation $H_i=H+\varepsilon_i$ s.t. all its eigenvalues are different (since such matrix is dense in $\mathbb C^{n\times n}$), then we get $H_i\to H$ in norm, hence by continuity of eigenvalue and eigenvectors we know that $H$ has a set of eigenvectors that forms an orthonormal basis. 
	- **Generalization**: normal matrix is also unitarily diagonalizable. [[Matrix]]
- **Simultaneous diagonalization**: we say that $A,B$ are simultaneously diagonalizable if there exists a $P$ s.t. $P^{-1}AP,P^{-1}BP$ are both diagonal matrices. Given that both $A,B$ are diagonalizable, or one of them has no multiple eigenvalues, then $A,B$ are simultaneously diagonalizable iff $AB=BA$. 
	- **Intuition**: the key of such property is **sharing a common eigenspace** for any of the conditions above we're essentially utilizing eigenspace as invariant subspace so that operators can be restricted to a smaller space to be discussed.
	- Simultaneous diagonalization essentially states that the conjugacy class under the same conjugator is the same, i.e., share the same inner automorphism. [[Group Actions]]
## Properties
- **Gershgorin circle theorem**: For $A=(a_{ij})_{n\times n}$, we have $$\sigma(A)\subset\bigcup_i\set{|z-a_{ii}|\le R_i},\quad R_i=\sum_{j\neq i}|a_{ij}|$$these discs are referred to as **Gershgorin discs**.
  **Proof**: find the largest element $x_i$ of eigenvector $x$, then based on the triangular inequality the results follows.
	- **Strengthening**: If the union of $k$ discs is disjoint from the union of the other $n-k$ discs then the former union contains exactly $k$ and the latter $n-k$ eigenvalues of $A$, when the eigenvalues are counted with their **algebraic multiplicities**.
	  **Proof**: Consider a masked matrix $D$ preserving only the diagonal entries in $A$, and let $B(t)=(1-t)D+tA$. The statement holds for $B(0)$, and as the eigenvalues are continuous with respect to $t$, the statements follows.
	- **Frobenius-Perron theorem**: assume $A$ is non-negative real matrix, then $A$ has a dominate real eigenvalue in absolute value. If $A$ is strictly positive, then the domination is strict, and the corresponding eigenvector can be normalized to have positive entries.
	  **Proof**: For positive matrix, without losing generality, assume that it has a spectral radius $\rho(A)=1$, then $A$ has a eigenvalue on unit circle. If such $\lambda\neq1$, then these exists $m$ such that $\lambda^m$ has negative real part. Let $\varepsilon$ be half the smallest diagonal entry of $A^m$, then $T=A^m-\varepsilon I$ is positive. Now $\lambda^m-\varepsilon$ is the eigenvalue of $T$ which is outside the unit circle, contradicting the Gelfand's formula $\rho(T)\le\rho(A^m)\le\rho(A)^m=1$.
- **Min-max theorem**: let $A$ be Hermitian on an inner product space $V$ with dimension $n$, with spectrum ordered in descending order $\lambda_1\ge\cdots\ge\lambda_n$. Then we have $$\begin{align}\lambda_k&=\max_{\mathcal{M}\subset V,\dim(\mathcal{M})=k}\quad\min_{x\in\mathcal{M},\|x\|=1}\langle x,Ax\rangle\\&=\min_{\mathcal{M}\subset V,\dim(\mathcal{M}=n-k+1)}\quad\max_{x\in\mathcal{M},\|x\|=1}\langle x,Ax\rangle\end{align}$$
	- **Poincare's inequality**: let $\xi_k=\lambda_{n+1-k}$, and $M\subset V,dim(M)=k$, then there exists unit vector $x,y\in M$ such that $$\langle x,Ax\rangle\le\lambda_k,\langle y,Ay\rangle\ge\xi_k$$**Proof**: for any list of $n+k-1$ vectors, take $x\in M\cap span(v_k,\cdots,v_n)$ and let $x=\sum a_iv_i$, then $\langle x,Ax\rangle=\sum|a_i|^2\lambda_i\le\lambda_k$. The second part can be achieved using $-A$.
	- **Proof of the theorem**: by Poincare's inequality, $\lambda_k$ is the upper bound to the right side. By setting $\mathcal{M}=span(v_1,\cdots,v_k)$, then upper bound is achieved.
- For $g(x)\in C[x],g(A)=0\Leftrightarrow g(\lambda)=0$. Use eigenvector to proof.
- Assume $f$ is the eigenpolynomial of $A$. Then the coefficient of $x^k=\sigma_k=(-1)^k\sum A_k$ where $\sigma_k=\sum\lambda_{i1}\lambda_{i2}\dots\lambda_{ik},A_k=A(i_1,\dots,i_k)$. 
- $\sum\lambda_k=tr(A)=tr(P^{-1}AP)$.
## Spectrum
For a square matrix $A$, its **spectrum** consists of all its eigenvalues $\sigma(A)=\{\lambda\in\mathbb{C}:|A-\lambda I|=0\}$. The **spectral radius** $\rho(A)=\max|\lambda_i|$.
- $\rho(A)\le\|A\|$ for any operator norm by definition ([[Matrix]]). Further we have $\rho(A)\le\|A^k\|^{1/k}$.
	- **Counterexample**: for $A=\begin{pmatrix}1&1\\0&1\end{pmatrix}$ we have $\rho(A)=1$ while $\|A\|>1$. This is mainly due to a lack of eigenvector.
- $\rho(A)<1\iff\lim_{k\to\infty}A^k=0$. $\Rightarrow$ follows from [[Jordan Normal Form]], while $\Leftarrow$ is obvious by contradiction.
- **Gelfand's formula**: for any operator norm $\|\cdot\|$, we have $$\rho(A)=\lim_{k\to\infty}\|A^k\|^{1/k}$$**Proof**: define $A_{\pm}=\frac{1}{\rho(A)\pm\varepsilon}A$, then $\rho(A_{\pm})=\frac{\rho(A)}{\rho(A)\pm\varepsilon}$, hence $$\lim_{k\to\infty}A_+^k=0\Rightarrow\|A_+^k\|<1\text{ for large }k\Rightarrow\|A^k\|^{\frac{1}{k}}<\rho(A)+\varepsilon$$Similarly we have another lower bound, leading to the formula.
	- **Corollary**: if $A_i$ are matrices that commutes, then $\rho(A_1\cdots A_n)\le\rho(A_1)\cdots\rho(A_n)$.
	- **General discussion**: see [[Spectral Theorem]] for discussion under the functional analysis framework.
## Rayleigh quotient
For Hermitian $A$, define the **Rayleigh quotient** induced by $A$ as $$\mathcal R(A,x)=\frac{x^*Ax}{x^*x},\quad x\in\mathbb C^n\backslash\set{0}$$When dealing with $M_n(\mathbb R)$ it reduces to one over $x\in\mathbb R^n\backslash\set{0}$. We have that $$\im\mathcal R(A,\cdot)=[\lambda_\min,\lambda_\max]$$where $\lambda_\min,\lambda_\max$ are the minimal, maximal eigenvalues.
**Proof**: by spectral theorem above we have $A=U^*\Lambda U$ where $\Lambda$ diagonal and $U$ unitary, hence $$\max_{x\in\mathbb C\backslash\set{0}}\mathcal R(A,x)=\max_{x\in\mathbb C^n\backslash\set{0}}\frac{(Ux)^*\Lambda(Ux)}{(Ux)^*(Ux)}=\max_{y\in\mathbb C^n\backslash\set{0}}\frac{\sum_{i=1}^n\lambda_i|y_i|^2}{\sum_{i=1}^n|y_i|^2}\where\sigma(A)=\set{\lambda_i}_{i=1}^n$$hence $\mathcal R(A,x)$ is the convex combination of $\lambda_i$, from which the bound follows naturally. Moreover, the bounds can be achieved by the corresponding eigenvectors. 
- **Generalized Rayleigh quotient**: given $A\in S^n(\mathbb R),B\in S_+^n(\mathbb R)$ ([[Quadratic Form]]), define the generalized Rayleigh quotient induced by $A,B$ as $$\mathcal R(A,B;x)=\frac{x^TAx}{x^TBx}$$then we have $$\im\mathcal R(A,B;x)=[\lambda_\min,\lambda_\max]\where\lambda_\min,\lambda_\max\in\sigma(B^{-1}A)$$**Proof**: since $B\in S_+^n$, by [[Quadratic Form]] it admits a quadratic root $B=(B^{1/2})^2$ with $B^{1/2}\in S_+^n$, hence the original quotient can be rewritten as $$\mathcal R(A,B;x)=\frac{x^TAx}{x^TB^{1/2}B^{1/2}x}=\frac{(B^{1/2}x)^T(B^{-1/2}AB^{-1/2})(B^{1/2}x)}{(B^{1/2}x)^T(B^{1/2}x)}$$Now by the original result we have $$\im\mathcal R(A,B;\cdot)=[\lambda_\max,\lambda_\min]\where\lambda_\max,\lambda_\min\in\sigma\left(B^{-1/2}AB^{-1/2}\right)$$At this point it suffices to show that $$\sigma\left(B^{-1/2}AB^{-1/2}\right)=\sigma(B^{-1}A)$$which is actually quite trivial, since $$B^{-1/2}AB^{-1/2}x=\lambda x\quad\iff\quad B^{-1/2}A(B^{-1/2}x)=\lambda B^{1/2}(B^{-1/2}x)\quad\iff\quad B^{-1}Ax=\lambda x$$that is, these two eigenvalue problems are actually equivalent, completing the proof. 