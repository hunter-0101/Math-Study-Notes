> [!note] Geometric intuition behind SVD
> In the general case $M\in\mathbb R^{m\times n}$ eigen-decomposition ([[Matrix]]) can not be generalized, since there is no such a concept as "rotation" or "stretching" for a linear transforms between two spaces of different dimension. However as a **bilinear form** there do exist pairs of $(u,v)\in\mathbb R^m\times\mathbb R^n$ (**left/right singular vectors**) that are stretched by the same **singular value**, i.e., $$Mv=\sigma u,\quad M^*u=\sigma v$$This turns out to be forming a basis for $\mathbb R^m$ and $\mathbb R^n$ respectively, and we'll utilize this intuition to derive such a thing as SVD.

The **singular value decomposition (SVD)** of a [[Matrix]] (linear transform) $M\in\mathbb C^{m\times n}$ is $$M=U\Sigma V^*\quad\text{where}\quad U,V \text{ unitary},\Sigma\text{ diagonal and non-negative}$$where $U,V$ serves as **rotations** (possibly combined with reflections), while $\Sigma$ serves as **scaling** by **singular values**. The decomposition is **unique up to reordering of singular values**.
**Proof**: notice that for any eigen-pair $(\lambda,v)$ of $M^*M$ we have $$\lambda\braket{v,v}=\braket{M^*Mv,v}=\braket{Mv,Mv}\ge0\quad\Longrightarrow\quad\lambda\ge0$$hence $M^*M$ is positive semi-definite. Since it's also Hermitian, by [[Eigendecomposition]] we have $$V^*M^*MV=\overline D=\begin{bmatrix}D&0\\0&0\end{bmatrix}\quad\text{where}\quad D\in\mathbb R_+^{l\times l},l\le\min(m,n)$$By expanding $V$ into chunked matrix corresponding to $D$ we have the more detailed equality $$\begin{bmatrix}V_1^*\\V_2^*\end{bmatrix}M^*M\begin{bmatrix}V_1&V_2\end{bmatrix}=\begin{bmatrix}V_1^*M^*MV_1&V_1^*M^*MV_2\\V_2^*M^*MV_1&V_2^*M^*MV_2\end{bmatrix}=\begin{bmatrix}D&0\\0&0\end{bmatrix}$$which implies $MV_2=0$. Now define $$U_1=MV_1D^{-1/2}\quad\Longrightarrow\quad U_1D^{1/2}V_1^*=M$$if we further expand $U_1$ into unitary $U=\begin{bmatrix}U_1&U_2\end{bmatrix}$ ([[Basis in Hilbert Space]]), and let $\Sigma=\begin{bmatrix}\overline D^{1/2}\\0\end{bmatrix}$, then we obtain the SVD $M=U\Sigma V^*$. To this end it's easy to verify $MM^*=U(\Sigma\Sigma^*)U^*$.
- **Remark on symmetric argument**: notice that both $MM^*,M^*M$ are Hermitian, we have $$MM^*=P_L\Lambda_LP_L^*,\quad M^*M=P_R\Lambda_RP_R^*$$where for $\lambda\neq0,\lambda\in\Lambda_L\iff\lambda\in\Lambda_R$ with the same multiplicity since $$MM^*x=\lambda x\Longrightarrow M^*M(M^*x)=\lambda(M^*x)\Longrightarrow MM^*(MM^*x)=\lambda(MM^*x)$$Assuming different multiplicity it's easy to deduce a contradiction of linear dependence of eigenvectors. On the other hand, expecting $M=U\Sigma V^*$ we have $$MM^*=U\Sigma V^*V\Sigma^*U^*=U(\Sigma\Sigma^*)U^*,\quad M^*M=V\Sigma^*U^*U\Sigma V^*=V(\Sigma^*\Sigma)V^*$$hence $U=P_L,V=P_R$, and $\Sigma$ being a rectangular diagonal matrix with entires of $\Lambda^{1/2}$.
- **Singular value**: by the above discussion, singular values are the square roots of eigenvalues of $M^*M$ (or $MM^*$), and are always real and non-negative.
- **Relation with eigen-decomposition**: although similar in form, two may differ when both exist for the same matrix. As illustrated in the intuition, SVD looks for vector pairs the same stretching factor rather than those without rotation. For $m=n$, $U,V$ are pairs of orthonormal basis that are mapped to each other with stretching, while $P$ is a set of basis that is mapped to itself with stretching.
	- **Counterexample**: for those not positive-semidefinite and Hermitian but still diagonalizable, its eigen-decomposition may not have orthogonal model matrix, but its singular vectors must be orthogonal.
## SVD viewed under linear transform
When viewing $M\in\mathbb C^{m\times n}$ as a linear transform, SVD is essentially decomposing such transform into three components: rotation of $\mathbb C^n$, stretching of singular vectors, and rotation in $\mathbb C^m$.
- **Singular values/vectors**: as noted above, $$M(v_i)=\sigma_iu_i,M^*(u_i)=\sigma_iv_i,\quad i=1,\cdots,\min(m,n)$$hence if we look at the unitÂ ball in $\mathbb R^n$, then $\set{u_j},\sigma_i$ are exactly the directions and magnitudes of semi-axises of the corresponding hyper-ellipsoid.
- **Dimension adjustment**: for equivalent  $MV=U\Sigma$, we have the geometric interpretation $$\begin{align}&MV&\text{the images of the orthonormal basis }\set{v_i}\in\mathbb R^n\text{ under }M\\&U\Sigma&\text{a stretched version of  the orthonormal basis }\set{u_i}\in\mathbb R^m\end{align}$$here $\Sigma\in\mathbb R^{m\times n}$ serves as another dimension/magnitude adjustor that pairs the basis of two spaces together. Symmetrically we have the corresponding interpretation for $U^*M=\Sigma V^*$.
## SVD viewed under data points
In application a matrix usually represents a set of data points, such as in [[Computer Vision]] and [[Dimensionality Reduction]]. In these cases SVD serves to provide some hidden information of the data points, and can be used to construct useful things, such as recommendation system.
- **Least square fit of linear system**: given approximation $Mx\approx b$, notice that $$\|Mx-b\|_2=\|\Sigma V^*x-U^*b\|_2$$for this to be minimized we must have $\hat x=V\Sigma^\dagger U^*=M^\dagger b$. [[Matrix]]
- **Best fit subspace**: consider $M$ as consisting of $m$ data points in its rows with $n$ features. Since $|\langle m_i,v\rangle|$ measures approximation of $m_i$ by unit vector $v$, finding best fit subspace is equivalent to solving $\arg\min_v|Mv|^2$. Hence if we recursively define $$v_k=\underset{v_k\perp v_i,\forall i<k,|v_k|=1}{\arg\max}|Av|,\quad\sigma_k=|Av_k|$$Then $V_k=\span(v_1,\cdots,v_k)$ is the best-fit $k$-dimensional subspace for $A$. Under $M=U\Sigma V^*$ it can be easily seen that these definition coincide exactly with singular values/vectors.
	- **Alternative definition**: if we first define $v_k,\sigma_k$ as above and let $u_k=Mv_k/\sigma_k$, then $M=\sum_i\sigma_i u_iv_i^*$ since a matrix is uniquely determined by all its actions on basis ([[Vector Space]]). In this case we're using the best fit subspace to define SVD.
- **Matrix splitting**: as discussed above we have $$M=\sum_{i}\sigma_iu_iv_i^*$$Assuming $\sigma_i\ge\sigma_{i+1}$ (**order of importance**), the truncated version of this splitting formula is exactly what we've obtained in [[Dimensionality Reduction#PCA Principle component analysis]].