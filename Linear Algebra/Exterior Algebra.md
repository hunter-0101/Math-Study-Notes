**Motivation**: exterior algebra rises natural under the desire to model multilinear functions. Since only some types of multilinear functions are important (e.g., symmetric or alternating), wedge products and other concepts are defined surrounding this topic. In particular, wedge product is a generalization of the volume form ([[Other Algebra Tools]]) to higher dimensions that simultaneously encodes information on orientation.

Note that definition of exterior algebra is motivated by covectors and their properties. For vectors the exterior can also be defined in a purely algebraic way, as discussed at the end.

---
The **tensor algebra** $T(V)$ over [[Vector Space]] $V$ is defined as $$T(V)=\bigoplus_{k=0}^\infty T^kV=\bigoplus_{k=0}^\infty\bigotimes_{i=1}^kV$$With the two-sided [[Ideal]] $I=(\set{x\otimes x:x\in V})$, we define the **exterior algebra** as the quotient algebra ([[Field]]) $$\bigwedge(V)=T(V)/I,\quad\alpha\wedge\beta=\alpha\otimes\beta\pmod{I}$$From definition we can deduce $\alpha\wedge\beta=-\beta\wedge\alpha$. Note that $T(V)$ is a graded algebra (below).
- **Motivation**: with wedge product of multi-covectors as multiplication, $A_*(V)$ becomes an anti-commutative graded algebra. Given $I=(i_1,\cdots,i_k)$ we denote $\alpha^I=\bigwedge_{j=1}^k\alpha_{i_j}$.
	- **Discussion on $\wedge$ and $\otimes$**: while in the concrete example of covectors we define $\wedge$ via alternating operator which is based on $\otimes$, in the formal definition they can actually be treated the same. When dealing with $\alpha\wedge\beta$ we can just juxtapose two expressions together without worrying about its coefficients.
- **Graded algebra structure**: $\bigwedge V$ forms a graded algebra ([[Field]]) with $\wedge$.
## Dual exterior algebra
Assuming $\dim V=n$ and $e_n$ be a orthonormal basis. A **covector** is an element $\alpha\in V^*=\mathcal L(V,\mathbb R)$ in the [[Algebraic Dual Space]].
- **Multilinear function**: $f:V^k\to\mathbb R$ is multilinear (a **$k$-tensor**) if it's linear in each argument. The set of $k$-tensors is denoted $L_k(V)$. Denote $$\sigma f(v_1,\cdots,v_k)=f\circ\sigma(v_1,\cdots,v_k)=f(v_{\sigma(1)},\cdots,v_{\sigma(k)}),\quad\sigma\in S_k$$then we can define **symmetric/alternating** functions by $$\begin{array}{ll}\text{Symmetric:}&f\in S_k(V)\iff\sigma f=f,&\forall\sigma\in S_k \\\text{Alternating:}&f\in A_k(V)\iff\sigma f=(\sgn\sigma) f,&\forall\sigma\in S_k\end{array}$$Here $S_k$ is the symmetric group. [[Group Gallery]]
	- **Symmetrizing/alternating operator**: define $$\sym f=\frac{1}{k!}\sum_{\sigma\in S_k}\sigma f\in S_k(V),\quad\alt f=\frac{1}{k!}\sum_{\sigma\in S_k}(\sgn\sigma)\sigma f\in A_k(V)$$Notice that if we already have $f\in S_k$ (resp. $f\in A_k$) then $\sym f=f$ (resp. $\alt f=f$).
- **Tensor product of covectors**: given $f\in L_k,g\in L_l$, define their tensor product as $$(f\otimes g)(v_1,\cdots,v_{k+l})=f(v_1,\cdots,v_k)g(v_{l+1},\cdots,v_{k+l})$$Obviously we have $(f\otimes g)\otimes h=f\otimes(g\otimes h)$.
### Wedge product
For $f\in A_k(V),g\in A_l(V)$ their **wedge (exterior) product** is defined as $$f\wedge g=\alt(f\otimes g)$$or explicitly written as $$(f\wedge g)(v_1,\cdots,v_{k+l})=\frac{1}{(k+l)!}\sum_{\sigma\in S_{k+l}}(\sgn\sigma)f(v_{\sigma(1)},\cdots,v_{\sigma(k)})g(v_{\sigma(k+1)},\cdots,v_{\sigma(k+1)})$$It's obvious then that $f\wedge g\in A_{k+l}(V)$, and wedge product is linear in each argument by linearity of tensor product. 
- **Equivalence of definition**: the purely algebraic definition for exterior algebra still applies here, and actually provides a more structural perspective on this concept. For equivalence, take for example $\alpha,\beta\in V^*$, then we have $$\alpha\wedge\beta=\alpha\otimes\beta=\frac{1}{2}(\alpha\otimes\beta-\beta\otimes\alpha)$$Notice that the alternating process is encoded in the step of taking quotient, since the resulting equivalence class consists of tensors that can be constructed via alternation of vector inputs.
- **Anti-commutativity**: for $f\in A_k(V),g\in A_l(V)$ we have $f\wedge g=(-1)^{kl}g\wedge f$, since we need $kl$ swaps to turn $\sigma\in S_k\times S_l$ into the corresponding $\sigma'\in S_l\times S_k$.
	- **Linear independence**: $\alpha_i\in V^*$ are linear independent iff $\bigwedge\alpha_i\neq0$.
	  **Proof**: for $\Leftarrow$ assume otherwise that $\sum_{i=1}^kc_i\alpha_i=0$. WLOG assume $c_1=1$, then $$\bigwedge_{i=1}^k\alpha_i=\left(\sum_{i=2}^k-c_i\alpha_i\right)\wedge\alpha_2\wedge\cdots\wedge\alpha_k=-\sum_{i=2}^kc_i\alpha_i\wedge\alpha_2\wedge\cdots\wedge\alpha_k=0$$where we used the fact $\alpha_i\wedge\alpha_i=-\alpha_i\wedge\alpha_i\Rightarrow\alpha_i\wedge\alpha_i=0$. Conversely if $\bigwedge\alpha_i=0$ and meanwhile $\alpha_i$ are linearly independent we have $\varepsilon_i=\sum_jb_{ij}\alpha_j$ where $\varepsilon_i$ are the standard dual basis ([[Algebraic Dual Space]]), and by the linear transform property below we have $\bigwedge\varepsilon_i=\det[b_{ij}]\bigwedge\alpha_i=0$, a contradiction. 
- **Associativity**: for $f\in A_k(V),g\in A_l(V),h\in A_m(V)$ we have $$(f\wedge g)\wedge h=f\wedge(g\wedge h)=\alt(f\otimes g\otimes h)$$by associativity of tensor product and $\alt(\alt f\otimes g)=\alt(f\otimes g)$.
- **Linear transformation**: we have that $$\beta_i=\sum_{j=1}^ka_{ij}\alpha_j\quad\Longrightarrow\quad\bigwedge_{i=1}^k\beta_i=\det[a_{ij}]\bigwedge_{i=1}^k\alpha_i$$by linearity and alternating property of wedge product.
- **Determinant as a wedge product**: given $\alpha_i\in L_1(V),v_i\in V,1\le i\le k$ we have $$\bigwedge_{i=1}^k\alpha_i(v_1,\cdots,v_k)=\det[\alpha_i(v_j)]_{k\times k}$$by a simple calculation and definition of [[Determinant]], hence the original determinant can be realized by setting $\alpha_i=\braket{\cdot,e_i}$.
	- **Natural isomorphism between $k$-tensor and $k$-linear functions**: notice that $$\bigwedge^\nolimits{k} V^*\cong\left(\bigwedge^\nolimits{k}V\right)^*\quad\text{by}\quad\bigwedge^\nolimits{k}\alpha_i\mapsto\left(\bigwedge^\nolimits{k} v_i\mapsto\det[\alpha_i(v_j)]\right)$$So we can essentially treat a $k$-tensor as really just a $k$-linear function.
- **Basis**: $\alpha^I,I=(i_1<\cdots<i_k)$ with $\alpha^I(e_J)=\delta_I^J,\forall J=(j_1<\cdots<j_k)$ forms a basis for $A_k(V)$, hence $\dim A_k(V)=\binom{n}{k}$. Note that $A_k(V)=0$ for $k>n$.
  **Proof**: for linear independence we assume $\sum c_I\alpha^I=0$, then by definition $$0=\sum_I c_I\alpha^I(e_J)=\sum_Ic_I\delta_I^J=c_J,\quad\forall J=(j_1<\cdots<j_k)$$For $A_k(V)=\span(\alpha_I)$ we only need to notice that $f=\sum f(e_I)\alpha^I$.
- **Intuition**: the dimension is $\binom{n}{k}$ because any element of $A_k(V)$ is alternating, hence we only need to consider strictly ascending sequences. Meanwhile it's obvious that all $k$-tensors $\bigotimes_I\alpha,|I|=k$ forms a basis for $L_k(V)$, hence $\dim L_k(V)=n^k$.
## Diagonalization of tensor
**Motivation**: in [[Matrix]] we can always transform a quadratic form $\sum a_{ij}x_ix_j$ via diagonalization of matrices, and in [[Linear PDE]] a second order PDE can also be transformed into its canonical form via the same method. It's quite tempting to generalize to $n$-th order "object", and the natural choice of such algebraic structure would be $n$-tensors.
In general consider $\xi_i$ a set of basis for, and denote $$V=\span(\xi:1\le i\le n),\quad\sigma=\sum_{|\alpha|=m}a_\alpha\xi^\alpha$$then we wanna find a proper $\xi=P\eta,P\in F^n$ that transforms the above into $$\sigma=\sum_{k=1}^n\lambda_k\eta_k^m$$A partial solution to this is the **Waring decomposition**, but in general this is not possible or not unique. Actually whether there exists one algorithm to decompose them all remains to be a NP-hard problem ([[P vs NP]]).