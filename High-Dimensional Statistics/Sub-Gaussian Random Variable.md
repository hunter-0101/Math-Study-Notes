**Preference of MGF**: tail bound is useful, but inequalities like Markov, etc. ([[Central Limit Theorem]]) are less investigated than moment generating functions ([[Random Variable]]), hence we consider $$P(x-\mu\ge t)=P\left(e^{\lambda(x-\mu)}\ge e^{\lambda t}\right)\le\frac{ Ee^{\lambda(x-\mu)}}{e^{\lambda t}}$$usually we take a logarithm on both sides to get $$\ln P(x-\mu\ge t)\le\inf_{\lambda}\left(Ee^{\lambda(x-\mu)}-\lambda t\right)$$Now we've successfully transforming the problem of tail estimation to the problem of MGF.

---
## Sub-Gaussian variable
**Motivation**: for the Gaussian distribution $X\sim N(\mu,\sigma^2)$ ([[Normal Distribution]]) we have $$Ee^{\lambda x}=e^{\lambda\mu+\frac{\sigma^2\lambda^2}{2}}$$hence by taking $X\sim N(\mu,\sigma^2)$ we get the tail bound for normal variable.
**Sub-Gaussian variable** is defined as $$X\sim\subg(\sigma^2)\quad\iff\quad Ee^{\lambda(X-\mu)}\le e^{\sigma^2\lambda^2/2},\quad\forall\lambda\in\mathbb R$$here $\sigma$ is called the **sub-Gaussian variance proxy**. Notice that variance proxy is not unique, but there is an optimal (smallest) one, which is called the **optimal variance proxy** $$\sigma_\opt^2=\inf\set{\sigma^2>0:X\sim\subg(\sigma^2)}$$Intuitively, by Markov inequality ([[Central Limit Theorem]]) and a simple optimization of $\lambda$ we have $$P(X-\mu>t)\le\exp\left(-\frac{t^2}{2\sigma^2}\right),\quad\forall t>0$$hence sub-Gaussianity implies that a random variables decays at least as fast as Gaussian. Note that we'll mostly assume $EX=0$ as part of the definition.
- **Why bound MGF not PDF**: it appears simpler to define sub-Gaussianity by comparing the PDE to that of an Gaussian variable, but we choose to bound the MGF because it's the most **operational**, **stable**, and **algebraically convenient**, in particular when we're dealing with sums, maxima, transformations, or other functions of RVs. More specifically: 
	- **MGF gives immediate Chernoff bound**: since $X\ge t\iff e^{\lambda X}\ge e^{\lambda t}$, as long as the MGF is bounded we can immediately derive the Chernoff bound.
	- **MGF is algebraically stable**: by [[Random Variable]] the MGF turned out to be a product for sum of independent RVs, which is very convenient.
- **Examples**: below we list some important RVs that are sub-Gaussian. 
	- **Rademacher variables**: a simple estimation shows that two-point uniform distribution is sub-Gaussian.
	- **Bounded random variables**: $X\in[a,b]$ satisfies $X\sim\subg(\sigma^2)$ where $\sigma=\frac{b-a}{2}$.
	  **Proof**: consider the CGF ([[Random Variable]]) $K_X(\lambda)=\ln Ee^{\lambda X}$. By direct computation, $$K_X'(\lambda)=\frac{E\left(Xe^{\lambda X}\right)}{Ee^{\lambda X}},\quad K_X''(\lambda)=\frac{E\left(X^2e^{\lambda X}\right)Ee^{\lambda X}-E\left(Xe^{\lambda X}\right)E\left(Xe^{\lambda X}\right)}{\left(Ee^{\lambda X}\right)^2}$$hence we know that $K_X(0)=0,K_X'(0)=EX$, and further $$K_X''(0)=E_\lambda\left(X^2\right)-(E_\lambda X)^2\quad\text{where}\quad E_\lambda f(X)=\frac{E\left(f(X)e^{\lambda X}\right)}{Ee^{\lambda X}}$$Below we try to bound $\sup_\lambda|K_X''(0)|$. Given any $Y\in[a,b]$ we have $(Y-a)(b-Y)\ge0$, hence by taking expectation and rearranging terms we have $$EY^2\le(a+b)EY-ab\quad\Longrightarrow\quad\var Y\le-(EY)^2+(a+b)EY-ab$$the maximal value of $\var Y$ is therefore obtained at $EY=\frac{a+b}{2}$, in which case $$\var Y=\frac{(b-a)^2}{4}\quad\Longrightarrow\quad \sup_{\lambda\in\mathbb R}|K_X''(0)|\le\frac{(b-a)^2}{4}$$since re-weighing expectation preserves supremum. At this point, by Taylor's theorem ([[Differential Calculus on Euclidean Space]]) with remainder term we have $$K_X(\lambda)\le0+EX\cdot\lambda+\frac{1}{2}\frac{(b-a)^2}{4}\cdot \lambda^2$$which completes the result after centering, since $K_X(\lambda)=\ln M_X(\lambda)$.
- **Mean & variance**: for an RV $X$, we have the implication $$Ee^{\lambda X}\le\exp\left(\lambda\mu+\frac{\lambda^2\sigma^2}{2}\right)\quad\Longrightarrow\quad EX=\mu,\quad\var X\le\sigma^2$$**Proof**: taking logarithm on both sides we get $$\lambda\mu+\frac{\lambda^2\sigma^2}{2}\ge K_X(\lambda)=\sum_{n=1}^\infty\kappa_n\frac{\lambda^n}{n!}$$hence $\mu=K'(0)=EX,\sigma^2\ge\kappa_2=\var X$. 
- **Sub-Gaussian norm**: the sub-Gaussian norm of an RV is defined as $$\|X-\mu\|_{\psi_2}=\inf\Set{c>0:E\exp\left(\frac{(X-\mu)^2}{c^2}\right)\le2}$$It's the Orlicz norm of $X$ generated by $\Phi(u)=e^{u^2}-1$ ([[Orlicz Space]]).
	- **Equivalence**: the sub-Gaussian norm and coefficient are equivalent up to an absolute factor. For $\Rightarrow$, utilizing the Chernoff's bound derived above we get $$P\left(|X-\mu|>t\right)\le2\exp\left(-\frac{t^2}{2\sigma^2}\right)$$since the random variable is positive, by [[Expectation]] we have $$\begin{align}E\exp\left(\frac{(X-\mu)^2}{c^2}\right)&=\int_1^\infty P\left(\exp\left(\frac{(X-\mu)^2}{c^2}\right)>t\right)dt\\&=\int_1^\infty P\left(|X-\mu|>c\sqrt{\ln t}\right)dt\\&\le\int_1^\infty2\exp\left(-\frac{c^2\ln t}{2\sigma^2}\right)dt\\&=2\int_1^\infty t^{-\frac{c^2}{2\sigma^2}}dt\end{align}$$hence it's no larger than $2$ when $c\ge2\sigma$. For $\Leftarrow$ just apply Cauchy-Schwarz inequality ([[Lp Space]]).
- **Sum of independent sub-Gaussian variables**: for $X_i\sim\subg(\sigma_i^2)$ pairwise independent, $$\sum_{i=1}^nX_i\sim\subg(\sigma^2)\quad\text{where}\quad\sigma^2=\sum_{i=1}^n\sigma_i^2$$by property of MGF ([[Random Variable]]), hence we immediately get the **Hoeffding bound** $$P\left(\sum_{i=1}^n(X_i-\mu_i)\ge t\right)\le \exp\left(-\frac{t^2}{2\sum_{i=1}^n\sigma_i^2}\right)$$
- **Hanson-Wright inequality**: given random vector $X=(X_1,\cdots,X_n)^T$ taking values from $\mathbb R^n$ and a [[Matrix]] $A\in\mathbb R^n$, consider the [[Quadratic Form]] $$Z=X^TAX$$when $X_k$ are i.i.d. with mean zero and sub-Gaussian norm $\|X_k\|_{\psi_2}\le K$, there exists universal constants $c$ s.t. $$P\left(|Z-EZ|>t\right)\le 2\exp\left(-c\min\left(\frac{t}{K^2\|Q\|_2},\frac{t^2}{K^4\|Q\|_F^2}\right)\right),\quad\forall t>0$$**Proof**: WLOG we assume $K=1$, below we consider to estimate the one-sided probability $p=P(Z-EZ>t)$. Denote $A=(a_{ij})$, then by independence and zero mean we have $$\begin{align}Z-EZ&=\sum_{1\le i,j\le n}a_{ij}X_iX_j-\sum_{1\le i\le n}a_{ii}EX_i^2\\&=\sum_{1\le i\le n}a_{ii}\left(X_i^2-EX_i^2\right)+\sum_{i\neq j}a_{ij}X_iX_j\end{align}$$this allows us to derive a simple estimation $p\le p_1+p_2$ where $$p_1=P\left(\sum_{1\le i\le n}a_{ii}\left(X_i^2-EX_i^2\right)>t/2\right),\quad p_2=P\left(\sum_{i\neq j}a_{ij}X_iX_j>t/2\right)$$Now for the first term $p_1$, notice that $X_i^2-EX_i^2$ are pairwise independent sub-exponential variables, by discussion on $\psi_1$-norm below we have $$\|X_i^2-EX_i^2\|_{\psi_1}\le2\|X_i^2\|_{\psi_1}\le 4\|X_i\|_{\psi_2}^2\le 4K^2$$hence by [[Martingale Method]] we have $$p_1\le\exp\left(-c\min\left(\frac{t^2}{\sum_ia_{ii}^2},\frac{t}{\max_i|a_{ii}|}\right)\right)\le\exp\left(-c\min\left(\frac{t^2}{\|A\|_F^2},\frac{t}{\|A\|_2}\right)\right)$$For the second term $p_2$, by some complex estimation (as stated in [this note](https://arxiv.org/pdf/1306.2872)) the problem can be reduced to one that involves only standard [[Normal Distribution]] variable. There are still many complicated estimations left for this result, and we skip it here.
## Sub-exponential variable
By simple calculation, for $X\sim\Exp(\lambda)$ we have that $$Ee^{\lambda(X-\mu)}= e^{\frac{\nu^2\lambda^2}{2}}$$Similarly, a **sub-exponential** variable with parameter $(\nu,\alpha)$ is defined by $$X\sim\sube(\nu,\alpha)\quad\iff\quad Ee^{\lambda(X-\mu)}\le e^{\frac{\nu^2\lambda^2}{2}},\quad\forall|\lambda|<\frac{1}{\alpha}$$This generalizes sub-Gaussian RV, since the condition is weakened to hold only on a finite interval. With similar Chernoff technique and optimization, we have that $$P(X-\mu\ge t)\le\begin{cases}\exp\left(-\dfrac{t^2}{2\nu^2}\right),&0\le t\le\dfrac{\nu^2}{\alpha}\\\exp\left(-\dfrac{t}{2\alpha}\right),&t>\dfrac{\nu^2}{\alpha}\end{cases}$$and with Hoeffding inequality, similar bounds can be derived for th e left-sided event.
- **Bernstein's condition**: we say that $X$ with variance $\sigma$ satisfies Bernstein condition with parameter $b$ if $$\left|E(X-\mu)^k\right|\le\frac{1}{2}k!\sigma^2 b^{k-2}$$we have that an RV satisfying Bernstein's condition is sub-exponential. This follows from a simple estimation based on Taylor expansion.
- **Sum of independent sub-exponential RVs**: for $X_i\sim\sube(\nu_i,\alpha_i)$, we have $$\sum_{i=1}^nX_i\sim\sube(\nu,\alpha_*)\quad\text{where}\quad(\nu,\alpha_*)=\left(\sqrt{\sum_{i=1}^n\nu_i^2}\ ,\ \max_{1\le i\le n}\alpha_i\right)$$with exactly the same reasoning as for sub-Gaussian. Concentration bound follows directly.
- **Example**
	- **Squared Gaussian variable**: consider $X=Z^2$ where $Z\sim N(0,1)$. we have $$Ee^{\lambda(X-\mu)}=\frac{1}{\sqrt{2\pi}}\int_\mathbb Re^{\lambda(z^2-1)}e^{-z^2/2}dz=\frac{e^{-\lambda}}{\sqrt{1-2\lambda}},\quad\forall\lambda<\frac{1}{2}$$hence for $\lambda\ge\frac{1}{2}$ the MGF is infinite, which implies that $X$ is not sub-Gaussian.
	- **$\chi^2$ variable**: by summing $n$ squared independent $Z_k\sim N(0,1)$ we have $$P\left(\left|\frac{1}{n}\sum_{k=1}^nZ_k^2-1\right|\ge t\right)\le 2e^{-nt^2/8},\quad\forall t\in(0,1)$$this bound plays an important role in the analysis of procedures based on taking random projections. 
## Application
We'll look at a commonly used randomization-based [[Dimensionality Reduction]] technique.
**Johnson-Lindenstrauss embedding**: consider a dataset $$\set{u^1,\cdots,u^N}\quad\text{where}\quad u^i\in\mathbb R^d$$we assume that $d\gg N$. For dimensionality reduction we'd like to find $F:\mathbb R^d\to\mathbb R^m$ to inherit some kind of features from the original dataset. Given a **tolerance** $\delta\in[0,1]$ we want $$1-\delta\le\frac{\|F(u^i)-F(u^j)\|_2^2}{\|u^i-u^j\|_2^2}\le 1+\delta,\quad\forall i\neq j$$that is, the projected dataset should preserve all pairwise squared Euclidean distances up to a multiplicative factor of $\delta$. The JL embedding is defined as follows: consider $$X\in\mathbb R^{m\times d}\quad\text{with}\quad X_{ij}\sim N(0,1)$$where the entires are pairwise independent, and define $F$ by $$F:\mathbb R^d\to\mathbb R^m,\quad F(u)=\frac{Xu}{\sqrt m}$$denoting $x_i\in\mathbb R^d$ the $i$-th row of $X$, then by property of [[Normal Distribution]] and definition of $\chi^2$ distribution we have $$Y\coloneqq\frac{\|Xu\|_2^2}{\|u\|_2^2}=\sum_{i=1}^m\Braket{x_i,\frac{u}{\|u\|_2}}^2\sim\chi^2_m$$hence we can apply the tail bound above to get $$P\left(\left|\frac{\|Xu\|_2^2}{m\|u\|_2^2}-1\right|\ge\delta\right)\le 2e^{-m\delta^2/8},\quad\forall\delta\in(0,1)$$Rearranging the terms, by definition of $F$ we have $$P\left(\frac{\|F(u)\|_2^2}{\|u\|_2^2}\notin[1-\delta,1+\delta]\right)\le 2e^{-m\delta^2/8},\quad\forall u\in\mathbb R^d$$since $u$ is arbitrary, we can take $u=u^i-u^j$, and by sub-additivity of probability [[Measure]], $$P\left(\frac{\|F(u^i-u^j)\|_2^2}{\|u^i-u^j\|_2^2}\notin[1-\delta,1+\delta]\text{ for some }i\neq j\right)\le 2\binom{N}{2}e^{-m\delta^2/8}$$Now for any $\varepsilon>0$ the probability can be driven below $\varepsilon$ by taking $m>\frac{16}{\delta^2}\ln(N/\varepsilon)$.

