See [[Martingale]] for definition and basic properties. 
**Doob construction**: given independent RVs $\set{X_k}_{k=1}^n$ with $f(X)=f(X_1,\cdots, X_n)$, define $$Y_k=E(f(X)|X_1,\cdots, X_k)$$and further assume that $E|f(X)|<\infty$, then $\set{Y_k}$ is a martingale relative to $\set{X_k}$.
**Proof**: on one hand, by Jensen's inequality ([[Convexity]]) we have $$E|Y_k|=E|E(f(X)|X_1,\cdots, X_k)|\le E|f(X)|<\infty$$On the other hand, by the tower law ([[Conditional Probability]]) we have $$E(Y_{k+1}|X_1,\cdots,X_k)=E(E(f(X)|X_1,\cdots,X_{k+1})|X_1,\cdots, X_k)=E(f(X)|X_1,\cdots, X_k)=Y_k$$these two results establish $Y_k$ as a martingale. 
- **Martingale difference sequence**: this refers to an adapted sequence $\set{(D_k,\Sigma_k)}$ s.t. $$E|D_k|<\infty\quad\text{and}\quad E(D_{k+1}|\Sigma_k)=0$$this can rise naturally as the difference of nearby terms of martingales $D_k=Y_k-Y_{k-1}$.
## Concentration bounds for MDS
**Theorem**: given $\set{(D_k,\Sigma_k)}$ a martingale difference sequence, if $$E(e^{\lambda D_k}|\Sigma_{k-1})\le e^{\lambda^2\nu_k^2/2}\quad\forall\ae|\lambda|<1/\alpha_k$$then their sum is sub-exponential with the following parameters:  $$\sum_{k=1}^nD_k\sim\sube(\nu,\alpha_*)\quad\text{where}\quad(\nu,\alpha_*)=\left(\sqrt{\sum_{k=1}^n\nu_k^2}\ ,\ \max_{1\le k\le n}\alpha_k\right)$$and this sum satisfies the concentration inequality $$P\left(\sum_{k=1}^nD_k\ge t\right)\le\begin{cases}\exp\left(-\dfrac{t^2}{2\sum_{k=1}^n\nu_k^2}\right),&0\le t\le\dfrac{\sum_{k=1}^n\nu_k^2}{\alpha_*}\\\exp\left(-\dfrac{t}{2\alpha_*}\right),&t>\dfrac{\sum_{k=1}^n\nu_k^2}{\alpha_*}\end{cases}$$**Proof**: for any $|\lambda|<1/\alpha_*$, by tower law ([[Conditional Probability]]) we can condition on $\Sigma_{n-1}$ to get $$Ee^{\lambda\sum_{k=1}^nD_k}=E\left(e^{\lambda\sum_{k=1}^{n-1}D_k}E\left(e^{\lambda D_n}|\Sigma_{n-1}\right)\right)\le Ee^{\lambda\sum_{k=1}^{n-1}D_k}\cdot e^{\lambda\nu_n^2/2}$$iterating this procedure yields the desired bound on MGF, hence $\sum_{k=1}^nD_k\sim\sube(\nu,\alpha_*)$. The concentration inequality is immediate ([[Sub-Gaussian Random Variable]]).
- **Azuma-Hoeffding inequality**: given $\set{(D_k,\Sigma_k)}$ a MDS for which there exists constants $a_k,b_k$ s.t. $D_k\in[a_k,b_k]$ almost surely for all $1\le k\le n$, then $$P\left(\sum_{k=1}^nD_k\ge t\right)\le \exp\left(-\frac{2t^2}{\sum_{k=1}^n(b_k-a_k)^2}\right),\quad\forall t>0$$**Proof**: this follows directly from the fact that $D_k\in[a_k,b_k]$ is sub-Gaussian with $\sigma=\frac{b_k-a_k}{2}$ ([[Sub-Gaussian Random Variable]]).
- **Bounded differences inequality**: we say that $f$ satisfies the bounded difference property with parameters $(L_1,\cdots, L_n)$ if $$|f(x)-f(x^{\backslash k})|\le L_k,\quad\forall x,x'\in\mathbb R^n,1\le k\le n\quad\text{where}\quad x^{\backslash k}=\begin{cases}x_j,&j\neq k\\x_k',&j=k\end{cases}$$under such condition, for a random vector $X=(X_1,\cdots,X_n)$ with independent components, $$P\left(f(X)-Ef(X)\ge t\right)\le\exp\left(-\frac{2t^2}{\sum_{k=1}^nL_k^2}\right)$$**Proof**: consider the associated MDS $$D_k=E(f(X)|X_1,\cdots,X_k)-E(f(X)|X_1,\cdots,X_{k-1})$$and define two RVs $$\begin{align}A_k&=\inf_xE(f(X)|X_1,\cdots,X_{k-1},x)-E(f(X)|X_1,\cdots,X_{k-1})\\B_k&=\sup_xE(f(X)|X_1,\cdots,X_{k-1},x)-E(f(X)|X_1,\cdots,X_{k-1})\end{align}$$Then we can verify easily that $A_k\le D_k\le B_k$ almost surely. Meanwhile, observe that $$E(f(X)|x_1,\cdots,x_k)=E_{k+1}f(x_1,\cdots,x_k,X_{k+1}^n),\quad\forall(x_1,\cdots,x_k\in\mathbb R^k)$$where $E_{k+1}$ denotes expectation over $X_{k+1}^n=(X_{k+1},\cdots,X_n)$, hence we have $$\begin{align}B_k-A_k&=\sup_{x}E_{k+1}f(X_1,\cdots,X_{k-1},x,X_{k+1})-\inf_xE_{k+1}f(X_1,\cdots,X_{k-1},x,X_{k+1}^n)\\&\le\sup_{x,y}\left|E_{k+1}f(X_1,\cdots,X_{k-1]x,X_{k+1}^n})-f(X_1,\cdots,X_{k-1},y,X_{k+1}^n)\right|\\&\le L_k\end{align}$$where we used the bounded difference assumption. Hence we can apply Azuma-Hoeffding inequality to obtain the desired result. 
	- **Deviation inequality in Hilbert space**: let $X_k,1\le k\le n$ be a sequence of independent RVs taking values in a [[Hilbert Space]] $\mathcal H$, and suppose $\|X_k\|_\mathcal H\le b_k$ almost surely. Denote $S_n=\left\|\sum_{k=1}^nX_k\right\|$, then we have the deviation inequality $$P\left(\left|S_n-ES_n\right|\ge n\delta\right)\le 2\exp\left(-\frac{n\delta^2}{2b^2}\right)\quad\text{where}\quad b^2=\frac{1}{n}\sum_{k=1}^nb_k^2$$and as a result, we have $$P\left(\frac{S_n}{n}\ge a+\delta\right)\le \exp\left(-\frac{n\delta^2}{2b^2}\right)\quad\text{where}\quad a=\sqrt{\frac{1}{n}\sum_{k=1}^n E\|X_k\|_\mathcal H^2}$$**Proof**: take $\Sigma_k\subset\Sigma$ the subset of probability space on which $\|X_k\|_\mathcal H\le b_k$, and denote $D_k=X_k(\Sigma_k)$. Define $f:\mathcal H^n\to\mathbb R$ by $$f(x_1,\cdots,x_n)=\begin{cases}\|x_1+\cdots+ x_n\|_\mathcal H,&x_k\in D_k,\forall1\le k\le n\\0,&\text{otherwise}\end{cases}$$then $S_n=f(X_1,\cdots,X_n)$ almost surely, and moreover, by the triangle inequality ([[Vector Space]]), $f$ satisfies the bounded difference property with parameters $L_k=2b_k$. Since $X_k$ are pairwise independent by assumption, by bounded difference inequality we get $$P(|S_n-ES_n|\ge n\delta)\le2\exp\left(-\frac{n\delta^2}{2b^2}\right)$$for the second bound, by triangle inequality and Cauchy-Schwarz ([[Lp Space]]) we have $$\frac{ES_n}{n}\le\frac{1}{n}\sum_{k=1}^nE\|X_k\|_\mathcal H\le\frac{1}{n}\sum_{k=1}^n\sqrt{E\|X_k\|^2_\mathcal H}\le\left(\frac{1}{n}\sum_{k=1}^nE\|X_k\|^2_\mathcal H\right)^{1/2}=a$$hence $ES_n\le na$, therefore $$\Set{\frac{S_n}{n}\ge a+\delta}\subset\Set{S_n-ES_n\ge n\delta}$$as a result, we have $$P\left(\frac{S_n}{n}\ge a+\delta\right)\le P(S_n-ES_n\ge n\delta)\le P(|S_n-ES_n|\ge n\delta)$$now the desired result follows from the one-sided version of the previous bound.
		- **Motivation**: in the field of functional data analysis observations are functions rather than tuples of numbers; for kernel methods we usually work with random elements in RKHS; and there are also many more things involving Hilbert space random variables. 
## Application
- **Classic Hoeffding for bounded difference**: given $X_i\in[a,b]$, their sum $S_n=\sum_{i=1}^n$ obviously has the bounded difference property, hence by the bounded difference inequality we have $$P\left(\left|\sum_{i=1}^n(X_i-\mu_i)\right|\ge t\right)\le2\exp\left(-\frac{2t^2}{n(b-a)^2}\right)$$which is exactly the classic Hoeffding result.
- **$U$-statistics**: let $g:\mathbb R^2\to\mathbb R$ be symmetric w.r.t two arguments, and consider an i.i.d. sequence $X_i,1\le i\le n$ of RVs. Define the **pairwise $U$-statistics** as $$U=\frac{1}{\binom{n}{2}}\sum_{j<k}g(X_j,X_k)$$viewing $U$ as a function $f=f(x)=f(x_1,\cdots,x_n)$, then by triangle inequality we can verify that $$|f(x)-f(x^{\backslash k})|\le\frac{1}{\binom{n}{2}}\sum_{j\neq k}|g(x_j,x_k)-g(x_j,x_k')|\le\frac{(n-1)(2b)}{\binom{n}{2}}=\frac{4b}{n}$$which means it satisfies the bounded difference property, hence the bounded difference inequality above can be applied to yield $$P(|U-EU|\ge t)\le 2\exp\left(-\frac{nt^2}{8b^2}\right)$$
- **Clique number in random graphs**: consider an undirected [[Graph]] $G=(V,E)$. When $E$ is constructed according to some random process then $C(G)$ is an RV. Below we consider the **Erdös-Rényi ensemble**: we pick $p\in(0,1)$, and construct $E$ by including $(i,j)$ into $E$ with probability $p$, independently for all possible edges. That is, we have the Bernoulli edge-indicator variable $X_{ij}\sim B(1,p)$. Denote $Z=(X_{ij})_{i\neq j}$, then in this case we have a function $$C(G):Z\mapsto f(Z)$$If we denote $Z'$ obtained from $Z$ by changing one coordinate, then obviously $$|C(G')-C(G)|\le1$$hence $C(G)$ satisfies the bounded difference property with $L=1$, therefore $$P\left(\frac{1}{n}|C(G)-EC(G)|\ge\delta\right)\le2\exp(-2n\delta^2)$$Consequently, the clique number of an Erdös-Rényi random graph is very sharply concentrated around its expectation.
- **Concentration of empirical mean embedding in RKHS**: in kernel methods and modern non-parametric statistics we often embed probability distributions into a RKHS $\mathcal H_k$ via the kernel mean embedding $$\mu_P=E_{X\sim P}\phi(X)\in\mathcal H_k$$where $\phi:\mathcal X\to\mathcal H_k$ is the feature map. Given i.i.d. samples $X_k\sim P$, we estimate $\mu$ by $$\hat\mu_P=\frac{1}{n}\sum_{k=1}^n\phi(X_k)$$we then case about how close $\hat\mu_P$ is to $\mu_P$ in $\mathcal H_k$ norm, since many kernel methods rely on this proximity. Now since each $X_k$ contributes $X_k'=\phi(X_k)-E\phi(X_k)$ to the centered sum, if $\|X_k\|\le B$ (which holds for bounded kernels like Gaussian RBF), we have $\|X_k'\|\le 2B$, and applying the Hilbert-space deviation inequality with $b_i=2B$ we get $$P\left(\|\hat\mu_P-\mu_P\|-E\|\hat\mu_P-\mu_P\|\ge\delta\right)\le2\exp\left(-\frac{n\delta^2}{8B^2}\right)$$this provides a dimension-free concentration rate for kernel mean embedding.
- **Uncertainty quantification for functional data average**: suppose we record $n$ daily temperature curves on fixed interval $[0,24]$ from $100$ different weather stations. Each day's curve is a random function $X_k(t)$ viewed as an element on the Hilbert space $L^2([0,24])$. Now we wanna estimate the average temperature curve $$\overline X(t)=\frac{1}{n}\sum_{k=1}^n X_k(t)$$and also quantify uncertainty: "how much can the average curve deviate from the true mean curve $\mu(t)=EX_k(t)$?" To this end, define $Y_k=X_k-\mu$, then $Y_k$ are i.i.d. with $\|Y_k\|_{L^2}\le b$ (assuming temperatures are bounded), then $$S_n=\left\|\sum_{k=1}^nY_k\right\|_{L^2}=n\left\|\overline X-\mu\right\|_{L^2}$$by the Hilbert-space deviation inequality, $$P\left(\left\|\overline X-\mu\right\|_{L^2}\ge E\left\|\overline X-\mu\right\|_{L^2}+\delta\right)\le 2\exp\left(-\frac{n\delta^2}{8b^2}\right)$$so with probability at least $1-\alpha$, we have $$\left\|\overline X-\mu\right\|_{L^2}\le E\left\|\overline X-\mu\right\|_{L^2}+b\sqrt{\frac{8\ln(2/\alpha)}{n}}$$This bound provides a non-parametric confidence radius in the space of functions.