All concepts in this note have their correspondence in [[Series]]. 
Given an open subset $\Omega\subset\mathbb R^d$ equipped with the ordinary Euclidean metric and norm we assume that $f:\Omega\to\mathbb R$ is a real-valued function. $|\cdot|$ denotes the usual Euclidean norm on $\mathbb R^d$.
- **Support**: the support of a function is defined as $$\supp f=\overline{\set{x\in\Omega:f(x)\neq0}}$$we say that $f$ has **compact support** if its support is compact.
- **Limit**: given $x_0\in\mathbb R^d$ (possibly infinite) we define $\lim_{x\to x_0}f(x)=A$ if for any $\varepsilon>0$ there exist a neighborhood of $x_0$, denoted $U(x_0)$, s.t. $$f(U(x_0))\subset B(A,\varepsilon)\quad\text{where}\quad B(A,\varepsilon)=\set{x\in\mathbb R:|x-A|<\varepsilon}$$In this definition $U(x_0)$ can be replaced by $d(f(x),A)<\varepsilon$ for convenience.
- **Landau notation**: for given $f:\mathbb R\to\mathbb C,g\neq0$ we define $$f=O(g)\iff\limsup_x \frac{|f(x)|}{|g(x)|}=\alpha<\infty\iff|f|\le C|g|$$given that $x$ is approaching some limit, which is also denoted $f= O_\alpha(g)$. When $\alpha=0$ we denote $f=o(g)$.
	- **Existence of limit**: it might happen that $\limsup f/g>\liminf f/g$ given $f=O(g)$, since in definition we're only restricting the upper limit.
	- **Asymptotic equivalence**: when $f(x)/g(x)\to1$ we write $f\sim g$.
## Continuity
Note that in [[Point Set Topology]] there is a more general definition using openness.
We say that $f:\Omega\to \mathbb{R}^n$ is **continuous** at $x_0\in\Omega$ if $$\lim_{x\to x_0}f(x)=f(x_0)$$We say that $f$ is continuous on $\Omega$ if it's continuous at every point of $\Omega$. Set of such functions is denoted $C(\Omega)$. Similarly we can define $C(\overline\Omega)$ for closed set $\overline\Omega$.
- **[[Banach Space]] structure**: $C(\overline\Omega)$ is Banach under the norm $\|f\|_{C(\overline\Omega)}=\|f\|_\infty$ ([[Lp Space]]), since uniform limit of continuous function on compact set is also continuous, as discussed below.
- **Oscillation**: oscillation of a function on open $E$ and at a point $x$ are defined as $$\omega_f(E)=\diam(f(E)),\quad\omega_f(x)=\lim_{r\searrow 0}\omega_f(B(x,r))$$Continuity at $x$ is equivalent to $\omega_f(x)=0$.
- **Uniform continuity**: Let $f(x)$ be defined on $I$. If for any$\epsilon > 0$, there exits $\delta > 0$ such that $\forall |x_1 - x_2| < \delta, |f(x_1) - f(x_2)| < \epsilon$, then $f(x)$ is said to be **uniformly continuous** on $I$. Uniform continuity pertains to the whole interval, while continuity only considers a single point.
	- **Continuous function is uniformly continuous on compact set**: just use compactness to translate continuity condition to bounded oscillation over a finite open cover, hence ensuring uniform continuity.
	- **Uniform continuity in each variable**: if $F(t,x)$ is continuous on $K\times X$ given $K$ compact, then it's uniformly continuous in $x$. 
	  **Proof**: fix $x_0$, for any $\varepsilon>0$ pick a proper subset $B_\delta(t)\times B_\delta(x_0)$ for each $(t,x_0)$ and further pick a finite sub-cover $B_{\delta_i}(t_i)$ by compactness, yielding a minimum $\delta$. Now take $|x-x_0|<\delta$, then we have $(t,x)\in B_{\delta_i}(t_i)\times B_\delta(x_0)$ for some $i$, hence $$|F(t,x)-F(t,x_0)|\le|F(t,x)-F(t_i,x)|+|F(t_i,x)-F(t_i,x_0)|<2\varepsilon$$hence the result follows. Intuitively the independence of $t$ is derived by separating $K$ into finite sub-covers, on each of which the upper bound is uniform.
- **Hölder continuous**: $f:\mathbb R^n\to\mathbb R$ is said to be $\gamma$-Hölder continuous ($0<\gamma\le 1$) if $$|f|_{C^{0,\gamma}(\overline U)}=\sup_{x,y\in U}\frac{|f(x)-f(y)|}{|x-y|^\gamma}<\infty$$Here $\gamma$ is referred to as the Hölder exponent. More can be found in [[Hölder Space]].
- **Lipschitz continuity**: a real value function $f$ is said to be Lipschitz continuous if $$\lip f=\sup\Set{\frac{|f(x)-f(y)|}{|x-y|}:x,y\in\Omega,x\neq y}<\infty$$where the supremum is referred to as the **Lipschitz constant**. 
	- In Lipschitz continuity the derivative is bounded by the Lipschitz constant, but uniform continuity is not affected as long as the function itself is bounded (such as $y=\sqrt x$). Thus Lipschitz continuity always implies uniform continuity, but not vice versa.
For a bounded interval $[a,b]$ of the real line we have $$\text{Continuously differentiable}\subset\text{Lipschitz continuous}\subset\alpha\text{-Hölder continuous}\subset\text{Uniform continuous}$$where $0<\alpha\le1$.
### Equicontinuity
A collection $A\subset C([a,b])$ is said to be **equicontinuous** at $x_0$ if for any $\varepsilon>0$ there exists $\delta>0$ s.t. $$|x-x_0|<\delta\quad\Longrightarrow\quad|f(x)-f(x_0)|<\varepsilon,\quad\forall f\in A$$that is, all the elements in $A$ shares the same oscillation at $x_0$. When this property holds for all points on the domain of definition we say that $A$ is **pointwise equicontinuous**. $A$ is said to be **uniformly equicontinuous** if for any $\varepsilon>0$ there exists $\delta>0$ s.t. $$|x_1-x_2|<\delta\quad\Longrightarrow\quad|f(x_1)-f(x_2)|<\varepsilon,\quad\forall f\in A$$that is, in spite of pointwise equicontinuity we further require that the oscillation is uniform in both the domain of definition and element-wise.
 

---
# Series of Functions
**Motivation**: usually in practice we have a sequence of functions that converges pointwise, and would like to pass some properties of the elements within the sequence to its limit. However pointwise convergence is usually not sufficient for good behavior under limit, and it turns out that we need to strengthen the convergence to uniform.
## Uniform Convergence
$\{f_n(x)\}$ converges uniformly to $f(x)$ on $E$ if for any $\varepsilon > 0$, there exists a positive integer $N$ (at most depending on $\varepsilon$) such that when $n > N$, we have $$|f_n(x) - f(x)| < \varepsilon, \qquad \forall x \in E$$In general we have $f_n(x)\to f$ uniformly on $E \iff\lim_{n \to \infty} \|f_n - f\|_\infty = 0$. [[Lp Space]]
- If $\{f_n(x)\}$ converges uniformly on $E$ to $f(x)$, and $x_0$ is a cluster point of $E$, then if $\forall n > 0, \lim_{x \to x_0, x \in E} f_n(x)$ exists, we have $$\lim_{n \to \infty} \lim_{x \to x_0} f_n(x) = \lim_{x \to x_0} f(x) = \lim_{x \to x_0} \lim_{n \to \infty} f_n(x)$$
- **Convergence test**: similar to [[Series]], there are many criterions for uniform convergence.
	- **Cauchy test**: $\{f_n(x)\}$ converges uniformly iff $\forall \varepsilon > 0, \exists N, \text{ s.t. }$$$|f_n(x) - f_m(x)| < \varepsilon, \quad \forall x \in E, \quad m,n > N$$In other words, $\set{f_n}$ is Cauchy in $\|\cdot\|_\infty$ norm.
	- **Weierstrass test**: Assume there is a sequence $\{a_n\}$ such that $|u_n(x)| < a_n, \forall n > 0, x \in E$. If the series $\sum_{n=1}^\infty a_n$ converges, then the series of functions $\sum_{n=1}^\infty u_n(x)$ converges uniformly on $E$.
	- **Abel test**: If $\sum_{n=1}^\infty b_n(x)$ converges uniformly, and $\forall x \in E, \{a_n(x)\}$ is monotonic and uniformly bounded, then $\sum_{n=1}^\infty a_n(x)b_n(x)$ converges uniformly.
	  **Proof**: since $\sum b_n$ is uniformly convergent we have $|\sum_n^m b_n|<\varepsilon$ for $m,n$ large enough, hence apply the summation by parts we can get the convergence. This is finer than the Weierstrass test since summation by parts makes the error smaller. [[Series]]
		- **Intuition**: monotonicity and boundedness of $a_n$ ensures its stable behavior, making it possible to view it as a "coefficient" term for $\sum b_n(x)$. Summation by part formalizes this intuition.
	- **Dirichlet test**: If the series $\sum_{n=1}^\infty b_n(x)$ has a sequence of partial sums $\{B_n(x)\}$ uniformly bounded on $E$, and $\forall x \in E, \{a_n(x)\}$ is monotonic and uniformly convergent on $E$, then $\sum_{n=1}^\infty a_n(x)b_n(x)$ converges uniformly. The proof is similar to the above.
- **Continuous limit**: for $f_n\in C([a,b]),\forall n$ with $f_n\nearrow f\in C([a,b])$ the convergence is uniform.
  **Proof**: if otherwise the convergence is not uniform we have a sequence $\set{x_n}\subset[a,b]$ at which $f-f_n>\varepsilon$. Since $[a,b]$ is finite the set has a limit point $x\in[a,b]$, and by continuity of $f_n,f$ we have $f(x)-f_n(x)\ge\varepsilon$, violating the convergence.
	- **Converse**: the uniform limit of a sequence of continuous functions is also continuous. 
	  **Proof**: this is a direct application of generalization of Moore-Osgood theorem ([[Series]]) with similar proof.
- **Termwise integration**: if $f_n\to f$ uniformly then $\int f_n\to\int f$, which is obvious by linearity of integral since $f\to0$ uniformly implies $\int f\to0$.
	- **Generalization**: the above result applies only to functions on finite interval. For general result, see MCT or DCT in [[Lebesgue Integral]]. 
- **Termwise differentiation**: if $F_n(x_0)\to F(x_0)$ and $F_n'=f_n\to f$ uniformly, then $F_n\to F$ uniformly with $f=F'$.
  **Proof**: assuming the above conditions we have $f$ continuous, hence integrable. Set $F(x)=F(x_0)+\int_{x_0}^x f$, then by termwise integration we have $$F(x)-F(x_0)=\int_{x_0}^xf(x)dx=\lim_{n\to\infty}\int_{x_0}^xf_n(x)dx=\lim_{n\to\infty }(F_n(x)-F_n(x_0))$$by $F(x_0)\to F(x_0)$ we have that $F_n(x)\to F(x)$, and uniform convergence follows from a simple estimation. 
	- **Counterexample**: the main issue with pointwise convergence for $f'\to f$ is that it doesn't guarantee differentiability, e.g. $\phi(x)=|x|,x\in[-1,1]$ with $$\phi_{n}=\phi*K_\delta,\quad \text{where}\quad K_\delta(x)=\delta^{-1}K_1(\delta^{-1}x),K_1(x)=\frac{1}{\sqrt\pi}e^{-x^2}$$here uniform convergence for $\phi_n\to\phi$ is guaranteed by [[Convolution#Good kernel]], and pointwise convergence of $\phi'_n\to\phi'$ is obvious.
	- **Alternative proof under stronger condition**: assuming $F_n\to F,F_n'=f_n\to f$ uniformly, then we have $f=F'$. To show this we notice that $$\lim_{h\to0}\frac{F(x+h)-F(x)}{h}=\lim_{h\to0}\lim_{n\to\infty}\frac{F_n(x+h)-F_n(x)}{h}=\lim_{n\to\infty}\lim_{h\to0}\frac{F_n(x+h)-F_n(x)}{h}=F_n'(x)$$Here the interchange of limits is guaranteed by uniform convergence ([[Series]]), hence $F'=\lim F_n'=\lim f_n=f$.
## Power series
The general form of a power series is $\sum_{n\in\mathbb N}a_nx^n$. The more general discussion is in [[Complex Series]]
- **Abel's lemma**: if $\sum a_nx^n$ converges at $x=\xi$, then it converges absolutely on $(-|\xi|,|\xi|)$; if it diverges at $x=\xi$, then it diverges for $|x|>|\xi|$. The proof uses comparison with geometric series. [[Series]]
- **Radius of convergence**: by Cauchy test ([[Series]]) the formula is $$R=\frac{1}{\limsup |a_n|^{1/n}}$$Note that the limit also equals $\lim|a_{n+1}|/|a_n|$ when it exists. 
- **Abel's theorem on power series**: given $\sum a_nx^n$ with radius of convergence $R$, it's inner closed uniformly convergent, and if it converges at $R$ then $$\sum a_nR^n=\lim_{x\to R^-}\sum a_nx^n$$**Proof**: inner closed uniform convergent is due to Weierstrass test, and notice that $$a_nx^n=\frac{x^n}{R^n}\cdot a_nR^n\quad\text{where}\quad\frac{x^n}{R^n} \text{ uniformly bounded and monotonic}$$hence by Abel test the convergence in uniform on $[0,R]$, and then by the fact that uniform limit of continuous function is also continuous we get the second result. 