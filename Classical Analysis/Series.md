All discussion below are restricted to [[Real Number System]], although some of the arguments can be generalized to [[Metric Space]] or even weaker conditions without modification. See [[Separation Axioms]] for more general discussion, and [[Normed Vector Space]] for general discussion on convergence under different norms.
## Limit of series
The $\varepsilon-N$ definition of limit kind of "go around" the blurry notation like $\lim x_n=x$ by focusing on the "potential" of a series - it's always possible to find large $N$ such that any term within the series with index larger than $N$ is arbitrarily close to the limit.
- **Limit superior/inferior**: For a sequence $\{x_n\}$, define its limit superior/inferior as $$\begin{align}&\limsup_{n\to\infty} x_n=\lim_{k\to\infty}\alpha_k\quad\text{where}\quad\alpha_k = \inf_{n \ge k}x_n\\&\liminf_{n\to\infty} x_n=\lim_{k\to\infty}\beta_k\quad\text{where}\quad \beta_k=\sup_{n \ge k} x_n\end{align}$$These two are the maximal/minimal limit points. Generalization can be found in [[Set Theory]].
- **Arithmetic property**
	- **Sum of series**: given that both $x_n,y_n$ converges, we have $$\lim_{n\to\infty}(x_n+y_n)=\lim_{n\to\infty} x_n+\lim_{n\to\infty} y_n$$this can be show directly using the $\varepsilon$-$N$ argument.
- **Stolz's theorem**: given real series $\set{x_n},\set{y_n}$, if $\set{y_n}$ is strictly increasing and divergent, or both $x_n,y_n\to0$ with $\set{y_n}$ strictly decreasing, then $$\lim_{n\to\infty}\frac{x_n}{y_n}=\lim_{n \to \infty} \frac{x_n-x_{n-1}}{y_n-y_{n-1}}$$in the extended sense whenever RHS exists. This follows from simple estimation.
- **Moore-Osgood theorem**: given a sequence $\set{a_{mn}}_{m,n\in\mathbb N}$ if $\lim_{n\to\infty} a_{mn}=b_m$ uniformly in $m$, and $\lim_{m\to\infty}a_{mn}=c_n$ for each large $n$, then both $\lim b_m,\lim c_n$ exists and equals $\lim a_{mn}$, i.e., $$\lim_{m\to\infty}\lim_{n\to\infty}a_{mn}=\lim_{n\to\infty}\lim_{m\to\infty}a_{mn}=\lim_{m,n\to\infty}a_{mn}$$**Proof**: by uniform convergence in $m$ we have $|a_{mn}-a_{mk}|<\varepsilon,\forall m\in\mathbb N,n,k>N$. Take $m\to\infty$ in both sides yields $|c_n-c_k|<\varepsilon$, hence $c_n$ is Cauchy, and converges to some $L$. Now since $$|b_m-L|\le|b_m-a_{mn}|+|a_{mn}-c_m|+|c_n-L|\to0,\quad m,n\text{ large}$$we have $\lim_{b_m}=L$, and finally it's obvious from a similar estimation that $\lim a_{mn}=L$.
	- **Intuition**: uniform convergence in one axis is enough to force the overall convergence to be also uniform. This is most intuitive if we consider $\lim_{x,y\to0}f(x,y)$, since uniform convergence restricts all directions instead of only $x,y$.
	- **Invalid converse**: existence, or even equality between iterated limits, does not imply existence of/equality to double limit, such as $f(x,y)=\frac{xy}{x^2+y^2}$.
## Convergence Analysis
Some general discussion of convergence of series can be found in [[Complex Series]].
- **Sparsity**: given $\set{a_k}$ positive with $n_a(x)\le Cx^r$ where $n_a(x)=|\set{k:a_k\le x}|$, we have $\sum a_k^{-s}<\infty$ for all $s>r$ by a simple estimation. The lower bound is optimal by $a_k=k^{1/r}$.
### Convergence test
- **Direct comparison test**: if the series $\sum b_n$ converges absolutely and $|a_n|<|b_n|$ for sufficiently large $n$, then $\sum a_n$ converges absolutely.
- **==Limit comparison test==**: if $\{a_n\},\{b_n\}>0$, the limit $\lim_{n\to\infty}\frac{a_n}{b_n}$ exists, is finite and non-zero, then either both series converge or both series diverge.
- **==Integral test==**: let $f$ be a *non-negative* and *monotonically decreasing* function such that $f(n)=a_n$, then the series converges iff the integral $$\int_1^\infty f(x)dx$$converges.
	- **Corollary ($p$-series test)**: $\sum_{n=k}^\infty\big(\frac{1}{n^p}\big)$ converges if $p>1$.
	- We have $\sum \frac{1}{n\ln n}$ diverges as it is the derivative of $\ln\ln x$.
- **Root test (Cauchy's criterion)**: Consider $$R = \lim\sup \sqrt[n]{|a_n|}$$if $R<1$ then the series converges absolutely; if $R>1$ then the series diverges; if $R=1$ the test is inconclusive.
	- Derived from comparison test with geometric series.
	- Here $R$ is the convergence radius.
- **Ratio test (d'Alembert's criterion)**: consider two limits $$l =\lim\sup\frac{|a_{n+1}|}{|a_n|},\quad L= \lim\inf\frac{|a_{n+1}|}{|a_n|}$$if $l>1$ then the series diverges; if $L<1$ then the series converges absolutely.
	- This test is less applicable than Root test, but sometimes more convenient.
- **Raabe's test**: when the standard ratio test fails, consider $$\rho_n = n\Big(\frac{a_n}{a_{n+1}}-1\Big)$$if $\lim\sup \rho_n<1$ then the series converges; if $\lim\inf \rho_n>1$ then the series diverges.
	- This theorem essentially derives from the comparison test with $\sum\frac{1}{n^s}$.
- **Gauss's test**: assume that suitable coefficients can be found such that $$\frac{a_n}{a_{n+1}} = 1 + \frac{1}{n} + \frac{\beta}{n\ln n} + o(\frac{1}{n\ln n})$$then the series converges when $\beta>1$ and diverges when $\beta<1$.
	- Derived from the comparison test with $\frac{1}{n(\ln n)^\alpha}$.
- **Cauchy's condensation test**: let $\{a_n\}$ be non-negative non-increasing sequence. Then the sum $A=\sum a_n$ converges iff the sum $A^*=\sum 2^na_{2^n}$ converges. Moreover, if they converge, then $A\le A^*\le 2A$.
- **Abel transformation (summation by parts)**: suppose $\{f_n\},\{g_n\}$ are two sequences, then $$\sum_{k=m}^n f_k(g_{k+1}-g_k)=(f_{n+1}g_{n+1}-f_mg_m)-\sum_{k=m}^ng_{k+1}(f_{k+1}-f_k)$$We derive the **Abel's test**: if $\{a_n\}$ is convergent, $\{b_n\}$ is monotonic and bounded, then $\sum a_nb_n$ is also convergent.
- **Leibniz criterion**: if a positive series is monotonic and converge to $0$, then the corresponding alternating series converges.
### Absolute convergence and rearrangement
A series $\sum x_n$ is said to b **absolutely convergent** if $\sum|x_n|$ converges, otherwise it's said to be **conditionally convergent**. A rearrangement of the series is $\sum x_{\sigma(n)}$ where $\sigma\in S_{\mathbb N}$.
- **Discussion**: consider the separation of positive and negative parts $$\set{x_n}=\set{x_n^+}\sqcup\set{x_n^-}\where x_n^+\ge0,x_n^-<0$$then we can easily observe that $$\sum_{n\in\mathbb N}|x_n|<\infty\quad\iff\quad\sum_{n\in\mathbb N} x_n^+<\infty,\sum_{n\in\mathbb N}(-x_n^-)<\infty$$while conditional convergence only ensure that they're arranged in a way that cancels each other out, so the overall partial sum converges.
- **Dirichlet theorem**: For a absolutely convergent series, any rearrangement of its elements will results in a absolutely convergent series with the same limit.
  **Proof**: for positive series, proof is down in a symmetric manner: for series $\sum a_n$  with partial sum $\alpha_n$ and its permutation $\sum b_n$ with partial sum $\beta_n$, it can be shown that $\beta_n\le\alpha$, thus $\beta\le\alpha$. Symmetrically, $\alpha\le\beta$, thus $\alpha=\beta$. For general cases, just consider the positive and negative part decomposition.
	- **Fubini's theorem**: for a double-indexed series of real numbers $\{a_{mn}\}_{m=1,n=1}^\infty$, if $\sum_{m,n}a_{mn}$ is absolutely convergent, then $$\sum_{(m,n)\in N\times N}a_{mn}=\sum_{m=1}^\infty\sum_{n=1}^\infty a_{mn}=\sum_{n=1}^\infty\sum_{m=1}^\infty a_{mn}$$**Proof**: considering the partial sum $S_{mn}=\sum_{i=1}^m\sum_{j=1}^n a_{ij}$, then absolute convergence of $\sum S_{mn}$ implies uniform convergence in $m$ and $n$, thus the result follows.
- **Riemann rearrangement theorem**: for a conditionally convergent series, its terms can be rearranged such that the new series converges to any finite real number, positive/negative infinity, or does not converge.
### Convergence acceleration
- **Kummer's transformation**: consider the estimation of a convergent **positive** series $\sum a_n$. By limit comparison test if $\frac{a_n}{b_n}\to 1$ then $\sum b_n$ also converge, and if we happen to know the exact value of $\sum_{n=1}^\infty b_n=B$, we have $$\sum_{n=1}^\infty a_n=\sum_{n=1}^\infty b_n+\sum_{n=1}^\infty(a_n-b_n)=B+\sum_{n=1}^\infty\left(1-\frac{b_n}{a_n}\right)a_n$$the series on the RHS converges faster than $\sum a_n$.
	- As an application, to calculate $\sum\frac{1}{n^2}$, we use $\sum\frac{1}{n(n-1)}$ as a approximation series and apply the above identity. Repeat this process $k$ times yields $$\sum_{n=1}^\infty\frac{1}{n^2}=\sum_{j=1}^k\frac{1}{j^2}+\sum_{n=1}^\infty\frac{k!}{n^{\overline{k+1}}n}$$
- **Euler's transformation**: consider the estimation of a convergent alternating series $\sum_{n=0}^\infty(-1)^na_n$. We consider $$\sum_{n=0}^\infty(-1)^na_n=\frac{a_0}{2}+\sum_{n=0}^\infty(-1)^n\frac{a_n-a_{n+1}}{2}$$the new series may have a faster decay rate, thus applying the transformation multiple times can accelerate the convergence in an impressive way.
- **Intermediate growth rate**: if $x_n\prec y_n$, there are sequences $\{z_n^{(k)}\},k\in Z_+$ such that $$x_n\prec z_n^{(1)}\prec z_n^{(2)}\prec\cdots\prec y_m$$**Proof**: obviously the following sequences satisfy the condition: $$z_n^{(k)}=x_n^{1/(k+1)}y_n^{1-1/(k+1)}$$
## Series product
Given two series $\{a_m\},\{b_n\}$, their product is canonically defined by the **Cauchy product** $$\left(\sum_{m\in\mathbb N} a_m\right)\left(\sum_{n\in\mathbb N} b_n\right)=\sum_{k\in\mathbb N} c_k\where c_k=\sum_{m+n=k}a_mb_n$$Cauchy product of two power series is defined similarly.
- (Cauchy) The Cauchy product of two absolutely convergent series is absolutely convergent, with the limit equaling the product of two limits.
- **Mertens' theorem**: for two convergent series, if at least one of them is absolutely convergent, then their Cauchy's product converges to the product of their limits.
The hyper-geometric series:$$F(\alpha,\beta,\gamma,x) = 1+\sum_{n=1}^\infty\frac{\alpha(\alpha+1)\dots(\alpha+n-1)\beta(\beta+1)\dots(\beta+n-1)}{n!\gamma(\gamma+1)\dots(\gamma+n-1)}x^n$$