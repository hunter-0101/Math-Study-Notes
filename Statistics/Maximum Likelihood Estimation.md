Given a vector of parameters governing the joint distribution of samples $\theta=(\theta_1,\cdots,\theta_k)^T$ and the corresponding parametric family of distributions $\set{f(\cdot;\theta):\theta\in\Theta}$, the joint density at the observed data samples $x=(x_1,\cdots,x_n)$ is given by the **likelihood function** $$L_n(\theta)=L_n(\theta;x)=f_n(x;\theta)\quad\text{where}\quad f_n(x,\theta)=\prod_{i=1}^nf_i(x_i,\theta)$$the goal of **maximum likelihood estimation (MLE)** is to find the optimal parameter $$\hat\theta=\underset{\theta\in\Theta}{\arg\max}\ L_n(\theta;x)$$this is mostly done via techniques in [[Differential Calculus on Euclidean Space]]. 
- **Intuition**: the main idea of MLE is to choose $\hat\theta$ to make the observed sample data most probable, i.e., likely. 
- **Practical calculation**: in practice, we often work with the log-likelihood function, $$\ell_n(\theta) = \ln L_n(\theta)$$since the maximum of $L_n(\theta)$ is the same as the maximum of $\ell_n(\theta)$ (as $\ln(\cdot)$ is a strictly increasing function), and the product turns into a more computationally tractable sum: $$\ell_n(\theta)=\sum_{i=1}^n \log f_i(x_i;\theta)$$The MLE $\hat\theta$ is typically found by solving the score equations (or likelihood equations): $$\frac{\partial \ell_n(\theta)}{\partial \theta_j} = 0, \quad \text{for } j=1, \ldots, k$$Note that when we're asked to obtain the MLE of some $g(\theta)$ we'd usually rewrite $\ell_n(\theta)$ as function of $g(\theta)$, and then taking partial derivative w.r.t $g(\theta)$ itself. 
## Property
The popularity of MLE stems from its powerful theoretical properties for an [[Estimator]], especially for large samples.
- **Asymptotic consistency**: Under mild regularity conditions (e.g., the true distribution belongs to the family), the MLE $\hat\theta$ is **consistent** - it converges in probability to the true parameter $\theta_0$ as $n \to \infty$.
- **Asymptotic efficiency**: The MLE $\hat\theta$ is **asymptotically efficient**, meaning that it achieves the **Cram√©r-Rao Lower Bound (CRLB)** in the limit. This implies that no other consistent and asymptotically normal estimator can have a smaller asymptotic variance.
- **Asymptotic Normality**: The MLE $\hat\theta$ is **asymptotically normally distributed** (discussed in [[Estimator]]).
- **Invariance Property**: If $\hat\theta$ is the MLE for $\theta$, and $g(\cdot)$ is a function of $\theta$, then the MLE for $g(\theta)$ is simply $g(\hat\theta)$. This is a major advantage over methods like MoM.
- **Sufficiency**: If a sufficient statistic $T(\mathbf{X})$ for $\theta$ exists, the MLE $\hat\theta$ must be a function of $T(\mathbf{X})$.
## Expectation-maximization (EM) algorithm
The **Expectation-Maximization (EM) Algorithm** is an iterative technique for finding the MLE when the data is **incomplete** or when the likelihood function involves **latent (unobserved) variables**. It operates on the idea of iterating between two steps:
1. **E-step (Expectation)**: calculate the expected value of the complete-data log-likelihood, conditioning on the observed data $\mathbf{x}$ and the current parameter estimates $\theta^{(t)}$. This expectation is typically denoted $Q(\theta, \theta^{(t)})$: $$Q(\theta, \theta^{(t)}) = E[\ell_{\text{complete}}(\theta; \mathbf{X}_{\text{complete}}) | \mathbf{X}_{\text{observed}}=\mathbf{x}, \theta^{(t)}]$$
2. **M-step (Maximization)**: Find the new parameter estimate $\theta^{(t+1)}$ that maximizes the function $Q(\theta, \theta^{(t)})$ calculated in the E-step: $$\theta^{(t+1)} = \underset{\theta}{\arg\max}\ Q(\theta, \theta^{(t)})$$The algorithm is guaranteed to converge to a local maximum of the incomplete-data likelihood function $L_n(\theta)$.
### Minor example: Gaussian mixture model
Given data $x_i\in\mathbb R^d$ and indicator variables $z_i$ with $$z_i=j\quad\iff\quad x_i\text{ was generated by the }j\text{-th Gaussian component}$$the EM algorithm iteratively:
- **E-step:** calculates the probability that each data point $X_i$ belongs to each component $j$, based on the current parameter estimates (means, variances, mixing proportions).
- **M-step:** uses these calculated probabilities (the "soft assignments") to re-estimate the component parameters, effectively treating the data as weighted by the assignments, and maximizing the expected complete log-likelihood.