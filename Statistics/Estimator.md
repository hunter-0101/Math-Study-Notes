**Philosophy of statistical estimation**: while we've covered many useful techniques for paraeter estimation in this folder, keep in mind that **no single estimator provides the _true_ value (the "correct" answer)**. Estimation is fundamentally a task under uncertainty, where the goal is to make the **least wrong** inference based on limited, noisy sample data. Here the measure of "less wrong", i.e., the **performance** of an estimator, is also quite flexible - as discussed in [[Method of Moment]], [[Interval Estimation]] and other related notes, many criterion are available, and it's up to us to pick the one that is most suitable for a specific task. More about this topic can be found in [[Philosophy of Probability & Statistics]]
- **Role of asymptotic**: while finite sample properties (like unbiasedness) are nice, the most powerful and general results in estimation theory are **asymptotic properties** (like consistency and asymptotic efficiency).
	- **Finite sample vs asymptotic**: many estimators, like [[Maximum Likelihood Estimation]], only display their desirable properties when the sample size is large. This reflects a key statistical reality: **more data makes for a better estimate, regardless of the method.**     
	- **The trade-off**: in practice, we often trade a small bias for a large reduction in variance (e.g., in regularization methods). The universal measure that captures both is the Mean Squared Error (MSE) defined below, balancing the **accuracy** (low bias) and **precision** (low variance) of the estimator.

---
An **estimator** is a rule $\hat\theta=\hat\theta(x)$ for estimating a given parameter $\theta$ based on [[Sampling]] $x=\set{x_i}_{i=1}^n$. The set of all possible values of the parameter $\Theta$ is called the **parameter space**.
- **Relation with [[Statistic]]**: an estimator is a specific kind of statistics, and the key property is that an estimator is a function of the samples related to some quantity of the distribution, rather than just a statistical property.
- **Error**: the error of the estimator $\hat\theta$ is defined as $$e(x)=\hat\theta(x)-\theta$$where $\theta$ is the given quantity to be estimated.
- **Mean square error (MSE)**: MSE of an estimator $\hat\theta$ is given by $$\mse(\hat\theta)=E_\theta(\hat\theta-\theta)^2$$it's an priori property of an estimator.
	- **Relation with variance and bias**: we have $$\mse(\hat\theta)=\var_\theta(\hat\theta)+B(\hat\theta,\theta)^2$$**Proof**: by direct calculation and properties of [[Expectation]] we have $$\begin{align}\mse(\hat\theta)&=E_\theta(\hat\theta-\theta)^2\\&=E_\theta\left(\hat\theta-E_\theta\hat\theta+E_\theta\hat\theta-\theta\right)^2\\&=E_\theta\left(\hat\theta-E_\theta\hat\theta\right)^2+2E_\theta\left(\left(\hat\theta-E_\theta\hat\theta\right)\left(E_\theta\hat\theta-\theta\right)\right)+E_\theta\left(E_\theta\hat\theta-\theta\right)^2\\&=E_\theta\left(\hat\theta-E_\theta\hat\theta\right)^2+\left(E_\theta\hat\theta-\theta\right)^2\\&=\var_\theta\hat\theta+B(\hat\theta,\theta)^2\end{align}$$which completes the proof.
## Properties of estimator
There are several basic properties of an estimator, measuring their abilities of approximating the parameter of interest. 
- **Bias**: the bias of $\hat\theta$ is defined as $$B(\hat\theta)=E(\hat\theta)-\theta$$when $B(\hat\theta)=0$ we say that $\hat\theta$ is **unbiased**, otherwise it's **biased**.
	- **Asymptotic bias**: for a sequence of estimators $T_n$ of $\theta$, we say that it's asymptotically unbiased if $\lim B(T_n)=0$.
	- **Example**: the revised estimator of variance in the worked example below is unbiased, while the population variance itself is biased. 
- **Consistency**: an estimator $T_n$ of $\theta$ is **weakly consistent** if $$\lim_{n\to\infty}P(|T_n-\theta|>\varepsilon)=0,\quad\forall\varepsilon>0$$and is **strongly consistent** if $$P\left(\lim_{n\to\infty}T_n=\theta\right)=1$$More rigorously, since $\theta$ is unknown we usually replace $\theta$ with $g(\theta)$ and require the the above equality holds for all $\theta\in\Theta$.
	- **Example**: for any $X$ with mean $\mu$ and variance $\sigma^2$, the sample mean $$\mu=\frac{1}{n}\sum_{i=1}^nx_i$$is consistent due to the LLN ([[Central Limit Theorem]]), while the first observation $\hat\mu=x_1$ is not consistent, since this estimator does not converge to $\mu$, but instead stay fixed with a variance of $\sigma^2$. 
	- **Criterion for consistency**: for $\hat\theta=\hat\theta(x_1,\cdots,x_n)$, if we have $$\lim_{n\to\infty}E\hat\theta=\theta,\quad\lim_{n\to\infty}\var\hat\theta=0$$then $\hat\theta$ is a consistent estimator for $\theta$. This follows from a simple estimation based on the Chebyshev's inequality ([[Central Limit Theorem]]).
- **Efficiency**: given $T$ unbiased, its **efficiency** is defined as $$e(T)=\frac{1/\mathcal I(\theta)}{\var T}$$where $\mathcal I(\theta)$ is the Fisher information ([[Entropy & Information]]). $T$ is **efficient** if $e(T)=1$ for all values of the estimator.
	- **Comparison between estimators**: when $\var\hat\theta_1<\var\hat\theta_2$ we say that $\hat\theta_1$ is more efficient than $\hat\theta_2$.
	- **Example**: consider $x\sim N(\mu,\sigma^2)$ the [[Normal Distribution]] and two estimators for $\mu$: $$\hat\mu_1=\overline x=\frac{1}{n}\sum_{i=1}^nx_i,\quad\hat mu_2=\frac{x_1+x_2}{2}$$by property of normal distribution we have that $$\var\hat\mu_1=\frac{\sigma^2}{n},\quad\var\hat\mu_2=\frac{\sigma^2}{2}$$hence $\hat\mu_1$ is more efficient than $\hat\mu_2$ as long as $n\ge3$. 
- **Asymptotic normality**: estimator $\hat\theta_n$ of a parameter $\theta_0$ is **asymptotically normal** if $$\sqrt n\left(\hat\theta_n-\theta_0\right)\xto{d}N\left(0,\mathcal I(\theta_0)^{-1}\right),\quad n\to\infty$$where $\mathcal{I}(\theta)$ is the Fisher Information matrix ([[Entropy & Information]]).
## Worked example: unbiased $\hat\sigma^2$
The unbiased estimator of sample variance is given by $$\hat\sigma^2=\frac{1}{n-1}\sum_{i=1}^n(x_i-\overline x)^2$$Algebraically this is straightforward - we just calculate $E\hat\sigma^2$, and notice that the coefficient need to be $\frac{1}{n-1}$ so that it's unbiased. More concretely, notice that $$\sum_{i=1}^n(x_i-\mu)^2=\sum_{i=1}^n(x_i-\overline x)^2+n(\overline x-\mu)^2$$by taking expectation on both sides we find out the proper coefficient. Geometrically, the model we're using is $$X_i=\mu+\varepsilon_i\quad\Longrightarrow\quad X=\mu\mathbf 1+\varepsilon\quad\text{where}\quad\mathbf 1=\begin{pmatrix}1&\cdots&1\end{pmatrix}^T$$and estimating $\mu$ by $\overline x$ is to project $X$ onto the 1-dimensional subspace $\span(\mathbf 1)$. The residual $$r=X-\overline x\mathbf 1$$lies in the $(n-1)$-dimensional orthogonal complement, hence the expected variance along each independent direction would be obtained via dividing $\frac{1}{n-1}$.
- **Another trivial intuition**: the sampling process itself is more likely to output data that accumulates on the area on higher probability, and these data are also closer to the sample expectation. These two factors makes sample variance a bit smaller than the actual variation. 
- **Interaction with degree of freedom**: strictly speaking the fact that coefficient equals the degree of freedom is just a coincidence. There exists estimator whose coefficient is much more complicated, e.g., the sample standard deviation $$S=\sqrt{\hat S^2},\quad ES=\sigma\sqrt{\frac{2}{n-1}}\frac{\Gamma(n/2)}{\Gamma((n-1)/2)}$$here we're having a [[Gamma Function]] expression. In general, “degrees of freedom” is the linear-algebraic / combinatorial count of independent directions left after estimation. If your quantity of interest is a _quadratic form_ in independent, identically distributed errors (like the sum of squared residuals), dividing by the number of independent directions gives an unbiased estimator. If the estimand is a **nonlinear** function of those **residuals** (e.g. standard deviation, inverse mean, parameters in other families), the exact unbiased correction is generally some function of $n$ that depends on the distribution (gamma functions, etc.), and there may or may not be a simple multiplicative correction.
## Bayes estimator
Let $f(x|\theta)$ be the likelihood function and $\pi(\theta)$ be the **prior distribution** for the parameter $\theta$. By Bayes' theorem ([[Conditional Probability]]) the **posterior distribution** is given by $$\pi(\theta|x)=\frac{f(x|\theta)\pi(\theta)}{\displaystyle\int f(x|\theta')\pi(\theta')d\theta'}$$A **loss function** $L(\theta, \hat\theta)$ quantifies the penalty incurred when the true parameter is $\theta$ but the estimate is $\hat\theta$. A common choice is the **squared error loss (SEL)**: $$L(\theta, \hat\theta) = (\theta - \hat\theta)^2$$The **Bayes estimator** is the value $\hat\theta_B(x)$ that minimizes the posterior expected loss: $$\hat\theta_B = \underset{\hat\theta}{\arg\min} \ E(L(\theta, \hat\theta) |x) = \underset{\hat\theta}{\arg\min} \ \int L(\theta, \hat\theta) \pi(\theta | x) d\theta$$For the commonly used Squared Error Loss (SEL), $L(\theta, \hat\theta) = (\theta - \hat\theta)^2$, and the Bayes Estimator $\hat\theta_B$ is the mean of the posterior distribution: $$\hat\theta_B^{\text{SEL}} = E[\theta | x] = \int \theta \pi(\theta |x) d\theta$$
- **Conjugate prior**: if $\pi(\theta|x)$ and $\pi(\theta)$ belongs to the same distribution family, then we say that it's a conjugate prior. 
- **Example**: consider $x_i\sim B(p)$ with $k=\sum_{i=1}^nx_i$. The likelihood is given by $$f(k|p)\propto p^k(1-p)^{n-k}$$and the conjugate prior given by $$\pi(p)\propto p^{\alpha-1}(1-p)^{\beta-1}$$by conjugacy the posterior distribution is also a Beta distribution: $$\pi(p|k)\propto f(k|p)\pi(p)\propto p^{k+\alpha-1}(1-p)^{n-k+\beta-1}$$hence $\pi(p|k)\sim\text{Beta}(\alpha',\beta')$ where $\alpha'=k+\alpha,\beta'=n-k+\beta$. The Bayes estimator is given by $$\hat p_B=E(p|k)=\frac{k+\alpha}{n+\alpha+\beta}$$