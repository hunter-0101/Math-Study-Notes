Related sample statistics can e found in [[Statistic]].

---
**Sampling** is the selection of a subset of a **statistical sample** of individuals from within a specific **statistical population** of interest to estimate characteristics of the whole population.
- **Empirical distribution function**: given $(X_1,\cdots,X_n)$ i.i.d real [[Random Variable]] with common distribution $F(x)$, the empirical distribution function is defined as $$\hat F_n(x)=\frac{\text{number of elements in the sample}\le x}{n}=\frac{1}{n}\sum_{i=1}^nI_{(-\infty,x]}(X_i)$$That is, it's the estimated distribution deduced from the frequency of samples.
	- **Asymptotic property**: by the strong LLN ([[Central Limit Theorem]]) we have that $$\hat F_n(t)\xto{\text{a.s.}}F(t),\quad\forall t\in\mathbb R$$We have a stronger result called the **Glivenko-Cantelli theorem** (also referred to as the **fundamental theorem of statistics**), stating that $$\|\hat F_n-F\|_\infty=\sup_{x\in\mathbb R}|F_n(x)-F(x)|\xto{\text{a.s.}}0$$**Proof**: we only consider continuous RV for simplicity. Fix $$-\infty=x_0<x_1<\cdots<x_{m-1}<x_m=\infty\quad\text{with}\quad F(x_j)-F(x_{j-1})=\frac{1}{m},\quad\forall1\le j\le m$$Now for all $x\in\mathbb R$ there exists $j\in[m]$ s.t. $x\in[x_{j-1},x_j]$. Notice that $$\begin{align}F_n(x)-F(x)&\le F_n(x_j)-F(x_{j-1})=F_n(x_j)-F(x_j)+\frac{1}{m}\\ F_n(x)-F(x)&\ge F_n(x_{j-1})-F(x_j)=F_n(x_{j-1})-F(x_{k-1})-\frac{1}{m}\end{align}$$therefore we have the estimation $$\|F_n-F\|_\infty=\sup_{x\in\mathbb R}|F_n(x)-F(x)|\le\max_{j\in[m]}|F_n(x_j)-F(x_j)|+\frac{1}{m}$$since the first term converges to $0$ almost surely by strong LLN, by taking $m\to\infty$ we have that $\|F_n-F\|_\infty<\varepsilon$ for $n$ sufficiently large, which implies almost sure convergence.
- **Sample mean & variance**: the same mean and sample variance are defined as $$\overline x=\frac{1}{n}\sum_{i=1}^nx_i,\quad s^2=\frac{1}{n-1}\sum_{i=1}^n(x_i-\overline x)^2$$here we're using the unbiased [[Estimator]] for sample variance. See more discussion there.
## Sampling from normal distribution
In this section we assume that all $x_i$ are sampled from $N(\mu,\sigma^2)$.
- **Sample mean & variance**: by definition ([[Normal Distribution]]) we have $$\overline x\sim N(\mu,\sigma^2/n),\quad\frac{(n-1)s^2}{\sigma^2}\sim\chi^2_{n-1}$$and these two statistics are independent.
	- Utilizing definitions we further have $$t=\frac{\sqrt n(\overline x-\mu)}{s}\sim t_{n-1}\quad F=\frac{s_x^2/\sigma_1^2}{s_y^2/\sigma_1^2}\sim F(m-1,n-1)$$and in particular, when $\sigma_1=\sigma_2$ we have $F=s_x^2/x_y^2\sim F(m-1,n-1)$.