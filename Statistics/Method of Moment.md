The **method of moment (MoM)** works by equating the **[[Sampling]] moments** to the **population moments**. The estimator is found by solving the resulting system of equations, linking the sample moments directly to the unknown parameters.
- **Intuition & motivation**: the underlying intuition is based on the LLN ([[Central Limit Theorem]]), which ensures that the sample moments converge to their corresponding population moments as the sample size $n\to\infty$.
## Method
Suppose that the parameter $\theta$ = ($\theta_1, \theta_2, \dots, \theta_k$) characterizes the [[Distribution Function]] $f_X(x; \theta)$ of the [[Random Variable]] $X$. Assume the first $k$ moments of the true distribution can be expressed as functions of the  $\theta$'s: $$\mu_i= EX^i= g_i(\theta_1, \theta_2,\cdots,\theta_k),\quad 1\le i\le k$$When a sample of size $n$ is drawn, resulting in the values $x_1,\cdots,x_n$. Let $$\hat\mu_j = \frac{1}{n} \sum_{i=1}^n x_i^j,\quad 1\le j\le k$$be the $j$-th sample moment, an estimate of $\mu_j$. The method of moments estimator for $\theta_i$ denoted by $\hat\theta_i$ is defined to be the solution (if one exists) to the equations: $$\hat \mu_i= g_i(\hat\theta_1, \hat\theta_2, \cdots, \hat\theta_k),\quad 1\le i\le k$$The method described here for single random variables generalizes in an obvious manner to multiple random variables leading to multiple choices for moments to be used. Different choices generally lead to different solutions. 
- **Non-uniqueness**: it's obvious that a parameter can be expressed as more than one function of different kind of moments, for instance, for $\Exp(\lambda)$ ([[Distribution Function Gallery]]) we have $$\lambda=\frac{1}{EX}=\frac{1}{\sqrt{\var X}}$$hence the method of moment is not unique. In general we would prefer moments of lower order. 