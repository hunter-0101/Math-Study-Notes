**Interval estimation** constructs an **interval**, which is likely to contain the true parameter $\theta$. The main idea is to use the sample data $x=(x_1,\cdots,x_n)$ to find two statistics, $L(x)$ (**Lower Bound**) and $U(x)$ (**Upper Bound**), such that the interval $(L(x), U(x))$ traps the true parameter $\theta$ with a specified probability. In particular, an interval with $$P(L(x)\le\theta\le U(x))=1-\alpha,\quad\forall\theta\in\Theta$$is called a **confidence interval** of $\theta$ with **confidence level** $\gamma=1-\alpha$. Here $\alpha$ is also called the **significance level**, and is usually taken to be $0.90,0.95,0.99$. The **interval width** $W=U(x)-L(x)$ is usually used as a measure of precision.
- **Motivation**: while **point estimation** like [[Estimator]], [[Maximum Likelihood Estimation]] and [[Method of Moment]] provide a single, best guess for an unknown population parameter $\theta$, it offers no measure of the **precision** or **uncertainty** associated with that estimate. Interval estimation is used as a refinement, providing explicit estimation for the level of certainty
## Pivotal method
A **pivotal quantity (pivot)** $Q(x, \theta)$ is a function of the sample $x$ and the parameter of interest $\theta$, such that its **probability distribution does not depend on the parameter $\theta$ or any other unknown parameters (nuisance parameters)**.
The main steps goes as follows:
1. **Identify a pivot**: Find a suitable pivotal quantity $Q(x, \theta)$.
2. **Determine critical values**: based on the known distribution of $Q$, find two constants $q_L$ and $q_U$ such that $$P(q_L < Q(x, \theta) < q_U) = 1-\alpha$$For a two-sided interval, these are usually symmetric quantiles of the pivot's distribution.
3. **Invert the inequality**: algebraically manipulate the inequality $q_L < Q(x, \theta) < q_U$ we can get $$L(x) < \theta < U(x)$$The resulting functions $L(x)$ and $U(x)$ form the $(1-\alpha)100\%$ confidence interval.    
Below we use the CI for [[Normal Distribution]] mean (with $\sigma$ known) to illustrate this process.
Assume $X_i \sim N(\mu, \sigma^2)$ where $\sigma^2$ is **known**, and the parameter of interest is $\theta = \mu$.
- **Pivot**: the standardized sample mean, is the pivot $$Q(x, \mu) = Z = \frac{\bar{X} - \mu}{\sigma/\sqrt{n}}$$here $Z\sim N(0, 1)$, which is independent of $\mu$ and $\sigma$.
- **Critical values**: for confidence level $1-\alpha$, we find $z_{\alpha/2}$ such that $P(Z > z_{\alpha/2}) = \alpha/2$, then $$P(-z_{\alpha/2} < Z < z_{\alpha/2}) = 1-\alpha$$
- **Inversion**: substitute the pivot and isolate $\mu$ $$ -z_{\alpha/2}< \frac{\overline{X} - \mu}{\sigma/\sqrt{n}} < z_{\alpha/2} \quad\iff\quad \overline{X} - z_{\alpha/2} \frac{\sigma}{\sqrt{n}} < \mu < \overline{X} + z_{\alpha/2} \frac{\sigma}{\sqrt{n}}$$The resulting $(1-\alpha)100\%$ CI for $\mu$ is $\bar{X} \pm z_{\alpha/2} \frac{\sigma}{\sqrt{n}}$.
## CI for the normal distribution
Assuming $x_i\sim N(\mu, \sigma^2)$ are iid samples, the confidence interval formula changes depending on whether the variance is known and which parameter is being estimated.
- **CI for $\mu$ with $\sigma$ known**: as derived above, the pivot is $Z = \frac{\bar{X} - \mu}{\sigma/\sqrt{n}} \sim N(0, 1)$, and CI is given by $$\text{CI}(\mu) = \bar{X} \pm z_{\alpha/2} \frac{\sigma}{\sqrt{n}}$$
- **CI for $\mu$ with $\sigma$ unknown**: when $\sigma$ is unknown, we replace $\sigma^2$ with $S^2 = \frac{1}{n-1}\sum (X_i - \overline{X})^2$. The pivot is now the $T$-statistic ([[Normal Distribution]]) $$T = \frac{\bar{X} - \mu}{S/\sqrt{n}}$$Accordingly, the CI is given by $$\text{CI}(\mu) = \bar{X} \pm t_{n-1, \alpha/2} \frac{S}{\sqrt{n}}$$
- **CI for $\sigma^2$ with $\mu$ unknown**: the parameter of interest is $\sigma^2$. The appropriate pivot is the statistic related to the sample variance $$Q = \frac{(n-1)S^2}{\sigma^2}$$here $Q\sim\chi^2_{n-1}$ ([[Normal Distribution]]), and the interval is constructed by finding critical values $\chi^2_{L}$ and $\chi^2_{U}$ as $$P(\chi^2_{L} < \frac{(n-1)S^2}{\sigma^2} < \chi^2_{U}) = 1-\alpha$$Inverting the inequality yields: $$\text{CI}(\sigma^2) = \left(\frac{(n-1)S^2}{\chi^2_{U}}, \frac{(n-1)S^2}{\chi^2_{L}}\right)$$
- **CI for large sample size**: for a general distribution (not necessarily normal) with mean $\mu$ and variance $\sigma^2$, provided the sample size $n$ is large ($n \gtrsim 30$), the [[Central Limit Theorem]] states that the sample mean $\overline{X}$ is approximately normally distributed. The pivot $Z = \frac{\bar{X} - \mu}{S/\sqrt{n}}$ is used, where $S$ is the sample standard deviation, approximating $\sigma$. We treat this pivot as approximately $N(0, 1)$, then $$\text{CI}(\mu) \approx \bar{X} \pm z_{\alpha/2} \frac{S}{\sqrt{n}}$$This approximation is crucial for constructing confidence intervals for parameters in non-Normal populations when $n$ is large.