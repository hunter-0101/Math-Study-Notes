A **(sample) statistic** is any quantity computed from values in a [[Sampling]] $x_i$ given by $$T=T(x_1,\cdots,x_n)$$which is considered for a statistical purpose. 
- **Sufficient statistics**: $T=T(x)$ is **sufficient for the parameter $\theta$** if the [[Conditional Probability]] distribution of the $x$, given the statistic $T=T(x)$, does not depend on $\theta$.
	- **Definition via mutual information**: $T=T(x)$ is sufficient for $\theta$ if the mutual information between $\theta,t$ equals the mutual information between $\theta,x$, i.e., the data processing inequality ([[Entropy & Information]]) becomes an equality. 
	- **Fisher's factorization theorem**: given PDF $f$, a statistic $T$ is sufficient for $\theta$ iff non-negative functions $g,h$ can be found s.t. $$f(x;\theta)=h(x)g(\theta,T(x))$$the proof is a bit tedious and complex, hence we skip it here. 
	- **Example**: consider $x_i\sim B(p)$. The sample sum $$T(x)=\sum_{i=1}^nx_i$$is sufficient since $$f(x;p)=p^{\sum_{i=1}^nx_i}(1-p)^{n-\sum_{i=1}^n x_i}=p^{T(x)}(1-p)^{n-T(x)}$$hence Fisher's factorization theorem applies. In contrast, the first observation $T'(x)=x_1$ is not sufficient for similar reason.
## Sample moment
The moment of [[Sampling]] is defined exactly the same as [[Random Variable]]. The central and standardized moment of a population $x_i$ is given by $$\mu_k=\frac{1}{n}\sum_{i=1}^n\left(x_i-\overline x\right)^k,\quad\tilde\mu_k=\frac{1}{n}\sum_{i=1}^n\left(\frac{x_i-\overline x}{\sigma}\right)^k=\frac{\mu_k}{\sigma^k}$$In particular, we have two important kinds of moments:
- **Skewness**: defined as the third standardized moment $$\hat\beta_s=\tilde\mu_3=\frac{\mu_3}{\mu_2^{3/2}}$$when $\hat\beta_s\gg0$ there are more extremely larger values in the population, and when $\hat\beta_s\ll0$ there are more extremely smaller values in the population.
- **Kurtosis**: defined as the fourth standardized moment $$\hat\beta_k=\tilde\mu_4=\frac{\mu_4}{\mu_2^2}$$Note that we more often use the excess kurtosis $$\hat\beta_k=\frac{\mu_4}{\mu_2^2}-3$$
## Order statistic
The $k$-th order statistic of a statistical sample is the $k$-th smallest value. 
- **Distribution of one order statistic**: assuming that the underlying distribution is $F$, then $$\int_{x}^{x+\Delta x}f_k(x)dx=\binom{n}{k-1,1,n-k}\left(F(x)\right)^{k-1}\left(F(x+\Delta x)-F(x)\right)(1-F(x))^{n-k}$$and by dividing $\Delta x$ on both sides and letting $\Delta x\to0$, we have $$f_k(x)=\frac{n!}{(k-1)!(n-k)!}(F(x))^{k-1}f(x)(1-F(x))^{n-k}$$here $f(x)$ is the distribution for $F$.
- **Distribution of two order statistics**: via similar method we can deduce that $$f_{ij}(y,z)=\frac{n!}{(i-1)!(j-i-1)!(n-j)!}(F(y))^{i-1}(F(z)-F(y))^{j-i-1}(1-F(z))^{n-j}f(y)f(z)$$we can actually further generalize the result to arbitrarily many order statistics.
In general, the distribution of the order statistics of $(i_1,\cdots,i_t)$ is $$f_{i_1,\cdots,i_t}(x_1,\cdots,x_t)=\binom{n}{i_1-1,1,i_2-i_1-1,1,\cdots,1,n-i_t}(F(x_1))^{i_1-1}\cdots(F(x_t))^{n-i_t}\cdot f(x_1)\cdots f(x_t)$$and the derivation is essentially the same. 