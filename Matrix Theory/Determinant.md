Given [[Field]] $F$, the **determinant** is a function on the space of [[Matrix]] $$\det:M_n(F)\to F$$that is explicitly expressed as the following **Leibniz formula** $$\det(A) = \sum_{\tau\in S_n}\sgn\tau \prod_{i = 1}^n a_{i\tau(i)} = \sum_{\sigma \in S_n} \sgn\sigma \prod_{i = 1}^n a_{\sigma(i)i}$$where $\sgn$ is the sign function of the permutation ([[Symmetric Group]]).
**Theorem**: there exists exactly one function $F:\Lambda^nF^n\to F$ ([[Exterior Algebra]]) that is **alternating**, **multilinear**, and such that $F(I)=1$. 
**Existence** is guaranteed by the Leibniz formula of determinant $\det(A)$.
**Uniqueness**: Let $F$ be such a function, and let $A = (a_i^j)_{i = 1, \dots, n}^{j = 1, \dots , n}$ be an $n \times n$ matrix. Call $A^j$ the $j$-th column of $A$, i.e. $A_j =\paren{a_i^j}_{i = 1, \dots , n}$, so that $A = \paren{A^1, \dots, A^n}$. Now one writes each of the $A^j$'s in terms of the $E^k$:$$A^j = \sum_{k = 1}^n a_k^j E^k$$As $F$ is multilinear, one has
$$\begin{align}F(A)& = F\paren{\sum_{k_1 = 1}^n a_{k_1}^1 E^{k_1}, \dots, \sum_{k_n = 1}^n a_{k_n}^n E^{k_n}} = \sum_{k_1, \dots, k_n = 1}^n \paren{\prod_{i = 1}^n a_{k_i}^i} F\paren{E^{k_1}, \dots, E^{k_n}}\end{align}$$
From alternation it follows that any term with repeated indices is zero. The sum can therefore be restricted to tuples with non-repeating indices, i.e. permutations:
$$F(A) = \sum_{\sigma \in S_n} \paren{\prod_{i = 1}^n a_{\sigma(i)}^i} F\paren{E^{\sigma(1)}, \dots , E^{\sigma(n)}}$$
Because F is alternating, the columns <math>E</math> can be swapped until it becomes the identity.  The $\mathrm{sgn}(\sigma)$ is defined to count the number of swaps necessary and account for the resulting sign change. One finally gets:$$\begin{align}F(A)& = \sum_{\sigma \in S_n} \sgn(\sigma) \paren{\prod_{i = 1}^n a_{\sigma(i)}^i} F(I)\\& = \sum_{\sigma \in S_n}\sgn(\sigma) \prod_{i = 1}^n a_{\sigma(i)}^i\end{align}$$as $F(I)$ is required to be equal to $1$. Therefore no function besides the function defined by the Leibniz Formula can be a multilinear alternating function with $F\paren{I}=1$. 
- **Motivation & intuition**: the weird-looking Leibniz formula for determinant is actually the unique (hence natural) multiplier required in the change of basis formula in [[Exterior Algebra]] (since $\dim\Lambda^n V=1$). The resulting properties makes it a **volume form** of the parallelotope spanned by the vectors as the variables. 
- **Historical remark**: the object we now refer to as "determinant" was originally studied (by Seki Takakazu and Leibniz in 1683 simultaneously) for [[Solving Linear System]]. Cramer (1750) provides a general rule for obtaining the solution of linear system (what we now call the Cramer's rule), and this object was given the name "determinant" and systematically studied by Gauss in 1801. The notation $\det A=|A|$ was first used by Cayley (1841). 
	- **Independent development without [[Matrix]] theory**: Leibniz first studied determinant systematically as a standalone object for solving linear system ("resultant"), when the matrix theory wasn't developed yet. 
## Properties
- **Arithmetic properties**
	- **Multi-linearity & alternating property**: these are just the alternative definitions.
	- **Determinant with repetitive vectors**: by alternating property we have $$\det(w,w,v_3,\cdots,v_n)=-\det(w,w,v_3,\cdots,v_n)\imply\det(w,w,v_3,\cdots,v_n)=0$$That is, once there is a repetitive term in the column vectors, their determinant will immediately vanish. 
- **Multiplicativity**:  $\det(AB)=\det A\det B$.
  **Proof**: by writing $A=(A^1,\cdots,A^n)$ the column vectors we have $$\begin{align}\det(AB)&=\det\paren{\paren{A^1,\cdots,A^n}B}\\&=\det\paren{\sum_{i=1}^nb_{i1}A^i,\cdots,\sum_{i=1}^n b_{in}A^i}&\text{by matrix multiplication}\\&=\det A\cdot\sum_{\sigma\in S_n}\sgn(\sigma)\prod_{i=1}^nb_{i\sigma(i)}&\text{by alternating and multilinearity}\\&=\det A\det B&\text{by axiomatic definition of det}\end{align}$$Intuitively, using the volume form intuition above this property is basically saying that change in volume form is multiplicative, which is quite straightforward. 
	- **Inverse matrix**: as a corollary we have $\det(A^{-1})=(\det A)^{-1}$.
- **Laplace expansion**: denote $M_{ij}$ the minor at $(i,j)$, then $$\det A=\sum_{j=1}^n(-1)^{i+j}a_{ij}M_{ij}$$this is a direct consequence of the Leibniz formula, and is useful for reducing the order of the matrix when calculating determinant. Note that this generalizes to multi-column or multi-row expansion. For a fixed $H\in S=\set{I\subset[n]:|I|=k}$ we have $$\det A=\sum_{L\in S}\varepsilon^{H,L}b_{H,L}c_{H,L}\quad\text{where}\quad\varepsilon^{H,L}=(-1)^{\sum_{h\in H}h+\sum_{l\in L}l}$$here $b_{H,L},c_{H,L}$ are the square minor of $A$ where $b_{H,L}$ is obtained by deleting from $A$ rows and columns with indices in $H,L$ respectively, while $c_{H,L}$ is obtained by taking complementary indices.
- **Invertibility criterion**: in general we have $$A\in\gl(F,n)\qiffq\det A\neq0$$**Proof**: for $(\Leftarrow)$, by [[Matrix]] the inverse matrix can be constructed by  $$A^{-1}=\frac{\adj A}{\det A}$$For $(\Rightarrow)$, by arithmetic properties we can apply Gaussian elimination ([[Solving Linear System]]) without changing the determinant, hence $\det A=0$ implies existence of zero diagonal entires in its upper triangular form, from which we conclude having found a linear dependent sub-collection of column vectors. Therefore $\dim \im A<n$, implying that $A$ is not invertible. 
	- **Alternative proof of linear dependence criterion**: by the natural identification stated in [[Exterior Algebra]] we have $$\Lambda^n V^\#\cong\paren{\Lambda^n V}^\#$$Since $\dim\Lambda^nV=1$, we have $\det$ being the unique multiplier maps induced by UP, hence $\det A=0$ implies that the wedge product is zero, therefore the column vectors of $A$ are linearly dependent. 
- **Differential at identity**: notice that $T_I\gl(n,\mathbb R)\cong\mathbb R^{n\times n}$, we have $$\det_\nolimits{*,I}:\mathbb R^{n\times n}\to\mathbb R,\quad\det_\nolimits{*,I}(X)=\tr X$$This can be shown by using the curve $c(t)=e^{tX}$. [[Differential Calculus on Smooth Manifold]]
## Determinant of block matrix
Below we consider some properties of $$\det A\quad\text{where}\quad A=[A_{ij}],\quad A_{ij}\in\ F^{m_i\times m_j}$$We're mainly relying on the multi-linearity and alternating properties to perform transformation on block matrices, and for obvious reasons they preserves determinant.
**Theorem**: given [[Field]] $F$ (or commutative ring) and a commutative sub-[[Ring]] $R\subset F^{n\times n}$ we have $$\det_FM=\det_F\det_RM,\quad\forall M\in R^{m\times m}$$**Proof**: notice that we have $$\begin{pmatrix}A&b\\c&d\end{pmatrix}\begin{pmatrix}dI_{m-1}&0\\-c&1\end{pmatrix}=\begin{pmatrix}dA-bc&b\\0&d\end{pmatrix}$$Denote the first matrix as $M$, then by applying $\det_R$ to each side we get $$\paren{\det_RM}d^{m-1}=\paren{\det_R(dA-bc)}d$$Again we apply $\det_F$ to each side, then we get $$\det_F\paren{\det_RM}\paren{\det_F d}^{m-1}=\det_F\paren{\det_R(dA-bc)}\cdot\det_Fd$$Meanwhile, applying $\det_F$ directly on both sides of the original identity we get $$\det_RM\cdot\paren{\det_F d}^{m-1}=\det_F(dA-bc)\cdot\det_Fd$$Now we use induction on $m$ to prove the result. The case for $m=1$ is trivial, and by induction assumption applied on $m-1$ we compare two identities above to get $$\paren{\det_FM-\det_F\det_RM}\paren{\det_Fd}^{m-1}=0$$If we work in $F[x]$ and replace $d$ with $d_x=d+xI_n$ and revise other symbols accordingly, then each term become a polynomial in $x$, however we know that $\det_Fd$ is a monic polynomial of degree $n$, hence it's not a zero divisor, and we must have the first term being zero, i.e., $$\det_M=\det_F\det_RM$$which completes the proof.
- **Discussion**: the theorem on determinant of block diagonal matrix, as well as the formulas below, are essentially a compact way of applying the invariance of determinant under fundamental row/column operations. While essentially equivalent, the new perspective it provides is sometimes igniting. 
- **Determinant of $2\times 2$ block matrix**: we have the result that $$\begin{vmatrix}A_n&B_{n\times m}\\-C_{m\times n}&I_m\end{vmatrix}=|A_n||I_m+C_{m\times n}A_n^{-1}B_{n\times m}|=|A_n+B_{n\times m}C_{m\times n}|$$For cases where $I$ is replaced by $D$ another square matrix, we have $$\begin{vmatrix}A&B\\C&D\end{vmatrix}=\begin{cases}|A|\cdot|D-CA^{-1}B|,&\text{if }|A|\neq0\\|D|\cdot|A-BD^{-1}C|,&\text{if }|D|\neq0\end{cases}$$Of course both are valid if both $A,D$ are invertible.
	- **Corollary**: $|A+uv^T|=|A|(1+v^TA^{-1}u)=|A|+v^TA^*u$.
	- **Sylvester's determinant theorem**: based on this formula, notice that $$\begin{vmatrix}I_m&B\\A&I_n\end{vmatrix}=|I_m-AB|=|I_n-BA|,\quad\forall A\in F^{n\times m},N\in F^{m\times n}$$Hence the $\chi_{AB},\chi_{BA}$ only vary by a coefficient $\lambda^{n-m}$. [[Characteristic Polynomial]]
- **Schur formula**: for square matrices $A,B,C,D$, $$\begin{vmatrix}A&B\\C&D\end{vmatrix}=\begin{cases}|AD-ACA^{-1}B|,& \text{if }|A|\neq0\\|AD-BD^{-1}CD|,&\text{if }|D|\neq0\\|AD-CB|,&\text{if }AC=CA\\|AD-BC|,&\text{if } CD=DC\end{cases}$$this is sometimes useful in special calculations.
## Vandermonde determinant
The **generalized Vandermonde matrix** given $x=(x_1,\cdots,x_n),a=(a_1,\cdots,a_m)$ is define as $$G_{mn}(x,a)=[x_j^{a_i}]_{mn}$$The common Vandermonde matrix is $V_n=G_{nn}(x,(0,1,\cdots,n-1))$. 
- **Vandermonde determinant**: we have that $$\det V_n=\prod_{1\le i<j\le n}(x_j-x_i)$$**Proof**: consider the [[Vector Space]] isomorphism $$\phi:\mathbb P_{n-1}\to F^n,\quad p(x)\mapsto(p(x_1),\cdots,p(x_n))$$Obviously $V_n$ is exactly the matrix for $\phi$ under the canonical basis $x_k,0\le k<n$ for $\mathbb P_{n-1}$, while by Newton interpolation formula ([[Polynomial Interpolation]]) the set $\prod_{i=0}^k(x-x_i),0\le k<n$ is also a basis, and the change of basis matrix is an upper triangular matrix with $1$'s along the diagonal, hence has determinant $1$. Now notice that matrix of $\phi$ in the new basis is $$\begin{bmatrix}1&0&0&\cdots&0\\1&x_1-x_0&0&\cdots&0\\1&x_2-x_0&(x_2-x_0)(x_2-x_1)&\cdots&0\\\vdots&\vdots&\vdots&\ddots&\vdots\\1&x_n-x_0&(x_n-x_0)(x_n-x_1)&\cdots&(x_n-x_0)(x_n-x_1)\cdots(x_n-x_{n-1})\end{bmatrix}$$which yields the desired formula by change of basis formula. [[Vector Space]]
	- **Proof via induction**: since elementary row/column transformation does not change determinant we can easily prove the formula using induction on $n$.
	- **Proof via factorization**: by Leibniz formula we have $$\det V_n\in \mathbb Z[x_1,\cdots,x_n],\quad\deg(\det V_n)=\frac{n(n+1)}{2}$$Notice that $\det V_n=0$ when $x_i=x_j$, hence by factor theorem ([[Polynomial]]) we have $(x_j-x_i)\mid\det V_n$. By arbitrariness we further deduce $$\det V_n=Q\prod_{1\le i<j\le n}(x_j-x_i)\quad\text{where}\quad Q\in F[x_1,\cdots,x_n]$$Now since the product in RHS has degree $\frac{n(n+1)}{2}$ we have $Q\in F$, and by the diagonal entries we have $Q=1$, completing the proof.
- The coefficients of the unique polynomial $c_0+c_1x+\cdots+c_{n-1}x^{n-1}$ that passes through $n$ points $(x_i,y_i)\in C^2$ with distinct $x_i$ are $$[c_0,c_1,\cdots,c_{n-1}]=[y_1,y_2,\cdots,y_n]G_n^{-1}(x,I_n)$$where $I_n=[0,1,\cdots,n-1]$
- Assume that $A=(a_{ij})_n,f_i(x)=a_{i1}+a_{i2}x+\dots+a_{in}x^{n-1}$. By additive properties of determinant, for $x_i\in C$,$$\begin{vmatrix}f_1(x_1)&f_1(x_2)&\dots&f_1(x_n)\\f_2(x_1)&f_2(x_2)&\dots&f_2(x_n)\\\vdots&\vdots&&\vdots\\f_n(x_1)&f_n(x_2)&\dots&f_n(x_n)\end{vmatrix}=|A|\det V_n(x_1,\dots,x_n)$$