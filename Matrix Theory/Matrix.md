The collection of all matrices over a [[Field]] $F$ is denoted $F^{m\times n}$. Collection of square matrices is also denoted $M_n(\mathbb F)$. The definition can actually be defined over arbitrary [[Ring]] with most of the concepts below well-defined, although absence of commutativity of the scaler field may bring some extra issues to be dealt with. 
- **Linear operator nature**: as discussed in [[Vector Space]], a matrix is the representation of a linear [[Operator on Hilbert Space]] given some basis: $$F^{m\times n}\cong\mathcal L(F^n,F^m)\quad\text{given}\quad F^n=\span(e_i),F^m=\span(f_j)$$Any pair of a linear operator and a basis determines a unique matrix, and conversely any matrix uniquely represents a linear operator in the pre-defined canonical basis.
- **Algebraic structure**: the triple $(M_n(F),+,\cdot)$ (operations defined below) forms an $F$-[[Algebra]]. $M_n(F)$ is sometimes called the matrix algebra. 
- **Invariance property**: by [[Vector Space]] similar matrices are essentially the same linear transform represented under different coordinates, hence several properties are invariant under matrix similarity: trace, [[Determinant]], eigenvalues/eigenvectors ([[Eigendecomposition]]), [[Characteristic Polynomial]], minimal polynomial, rank, etc.
## Matrix operations
- **Multiplication**: given $A_{m\times n},B_{n\times p}$ we define $C_{m\times p}=AB$ by $$c_{ij}=\sum_{k=1}^na_{ik}b_{kj},\quad\forall 1\le i\le m,1\le j\le p$$This is essentially a realization of linear operator ([[Vector Space]]) composition given sets of compatible basis. 
	- **Triple multiplication**: given matrices $A,B,C$ making $ABC$ well-defined we have $$ABC=\begin{bmatrix}a_1^T\\a_2^T\\\vdots\\ a_m^T\end{bmatrix}B\begin{bmatrix}c_1&c_2&\cdots&c_n\end{bmatrix}=[a_i^TBc_j]_{m\times n}$$where $a_i,c_j$ are row/column vectors. 
	- **Product between basis for $M_n(K)$**: obviously we have that $$M_n(K)=\span_K(E_{ij}:1\le i,j\le j)\quad\text{where}\quad E_{ij}=(\delta_i^s\delta_j^t)_{n\times n}$$their product is given by $$E_{ij}E_{st}=\delta_j^sE_{it}$$This is actually quite obvious: denote $e_i$ the canonical basis for $K^n$, then $$E_{ij}E_{st}=e_ie_j^Te_se_t^T=e_i(e_j^Te_s)e_t^T=\delta_j^s E_{it}$$This relation is sometimes useful.
- **Adjoint (conjugate transpose) matrix**: for $A\in M_n(\mathbb C)$ define $$A^*=\overline A^T=\overline{A^T}$$It's obvious that when $A\in M_n(\mathbb C)$ we have $A^*=A^T$. A useful identity is that $$\mathbb R^n=\im T\oplus\ker T^*=\im T^*\oplus\ker T$$as discussed in [[Operator on Hilbert Space]], since in finite-dimensional space any subspace is closed. Note that adjoint is sometimes denoted $A^H$ since it's Hermitian adjoint. 
	- **Discussion on definition**: in [[Operator on Hilbert Space]] an adjoint is defined via the equality condition $$\braket{Ax,y}=\braket{x,A^*y},\quad\forall x,y\in\mathbb C^n$$As it turns out by direct computation, this is realized exactly by conjugate transpose.
- **Adjugate matrix**: the adjugate of $A$ is defined as $$\operatorname{adj}A=C^T\where C=\paren{M_{ij}}$$Here $C$ is called th **cofactor matrix** consisting of the minors $$M_{ij}=\det \paren{A_{pq}}_{p\neq i,q\neq j}$$That is, it's the [[Determinant]] of the sub-matrices obtained by discarding the $i$-th row and $j$-th column. 
	- **Construction of inverse function**: by the Laplace expansion ([[Determinant]]) we have $$A\adj A=\det A\cdot I$$Therefore for $\det A\neq0$ the inverse of $A$ can be constructed by $$A^{-1}=\frac{\adj A}{\det A}$$This formula provides a quite computationally convenience method for calculating the inverse matrix. 
- **Inverse matrix**: $B\in M^n(K)$ is called the inverse of $A\in M_n(K)$ if $AB=BA=I_n$. The set of invertible matrices is denoted $\gl(n,K)$. By [[Group]] since $I$ is a two-sided identity we know that a left inverse is immediately a right inverse, and vise versa.
	- **Inverse under operations**: we have $$(A^T)^{-1}=(A^{-1})^T,\quad\overline A^{-1}=\overline{A^{-1}}\quad\Longrightarrow\quad (A^*)^{-1}=(A^{-1})^*$$Which can be checked directly.
- **Matrix similarity**: $A,B,\in M_n(\mathbb C)$ are said to be similar if there exists $P\in\gl(n,\mathbb C)$ s.t. $B=P^{-1}AP$. By change of basis formula ([[Vector Space]]) $A\sim B$ iff they're the representations of the same linear operator under two different basis.
	- **Matrix congruence**: $A,B\in M_n(F)$ are congruent if there exists $P\in\gl(n,F)$ s.t. $$P^TAP=B$$this is obviously an equivalence [[Relation]]. Matrix congruence arises as the effect of change of basis on the Gram matrix attached to a [[Bilinear Form]], or [[Quadratic Form]] on a finite-dimensional vector space. 
## Rank
For $A\in F^{m\times n}$, its **rank** is defined as taking it as an [[Operator on Hilbert Space]], i.e., $$\rank A=\dim(\im A),\quad\forall A\in\mathcal L(F^n,F^m)$$Equivalently, we can define $\rank A$ as the maximal number of linearly independent column (row) vectors.
**Proof of equivalence**: obviously $\dim(\im A)$ is the maximal number of linearly independent column vectors, and it suffices to show equivalence between definition via column and row vectors. Denoting the row vectors as $a_1,\cdots,a_m$, then an elementary row operation realizes $$(a_1,\cdots,a_m)\longmapsto(a_1,\cdots,a_i+\lambda a_j,\cdots,a_m)$$notice that two sets of vectors are equivalent, hence has the same rank. On the other hand, denoting the column vectors as $b_1,\cdots,b_n$, then an elementary row operation preserves the set of solutions ([[Solving Linear System]]), hence rank as well. Now by property of [[Field]] we can use elementary row/column operation to reduce  to the form $$A\sim\begin{bmatrix}I&0\\0&0\end{bmatrix}$$whose column rank equals row rank, hence so is $A$.  
- **Rank-nullity theorem**: for any $A\in M_n(F)$ we have $$\rank A+\dim\ker A=n$$this follows naturally from the direct sum decomposition of $F^n$ discussed above. 
- **Rank of operations of matrices**: by operator definition it's obvious that $$\rank(A+B)\le\rank A+\rank B$$for multiplication we have $$\rank(AB)\le\min(\rank A,\rank B)$$which is also obvious by investigating the dimensionality of image under composition.
- **Sylvester rank inequality**: for any $A,B\in M_n(F)$ we have $$\rank(AB)\ge\rank A+\rank B-n$$**Proof**: the desired inequality is, by rank-nullity theorem, equivalent to $$\dim(\ker A)+\dim(\ker B)\ge\dim(\ker(AB))$$Consider the restriction $B|_{\ker(AB)}:\ker (AB)\to F^n$, we have that $$\ker B|_{\ker(AB)}=\ker B,\quad\im B_{\ker(AB)}\subset\ker A$$Hence invoking rank-nullity theorem again we have $$\begin{align}\dim(\ker(AB))&=\dim(\ker B|_{\ker(AB)})+\rank B|_{\ker(AB)}\\&\le\dim(\ker B)+\dim(\ker A)\end{align}$$which completes the proof. 
	- **Equality condition**: by the estimation above we have that equality holds iff $$\im B|_{\ker(AB)}=\ker A\quad\iff\quad\ker A\subset\im B$$**Proof of equivalence**: $(\Rightarrow)$ is trivial, and for $(\Leftarrow)$, take any $y\in\ker A$, then by assumption there exists $x$ s.t. $Bx=y$, hence $$ABx=A(Bx)=Ay=0\imply x\in\ker (AB)\imply y\in\im B|_{\ker(AB)}$$Arbitrariness of $y$ implies that $\ker A\subset\im B|_{\ker(AB)}$, while the inverted inclusion is also trivial, hence completing the proof. 
	- **Corollary**: by Sylvester's rank inequality we can further deduce that for $AB=0$ we have $\rank A+\rank B\le n$.
## Trace
Given $A\in M_n(\mathbb C)$ we define $\tr(A)=\sum a_{ii}$. Given an orthonormal basis $(e_i)$ we have $$\tr A=\sum_{i=1}^n\braket{e_i,Ae_i}=\sum_{i=1}^n e_i^TAe_i$$This serves as more general definition (as in [[Operator on Hilbert Space]]) since trace is invariant under matrix similarity (discussed above & proved below).
- **Algebraic interpretation**: as discussed in [[Tensor]], we have the isomorphism $$M_n(F)\cong\mathcal \End(V)\cong V\otimes V^*$$Therefore the trace operator $\tr A$ is exactly the contraction of its corresponding tensor. 
- **Characterization properties**: $\tr:M_n(F)\to F$ is characterized, up to a scaler multiple, by the following three rules: $$\begin{align}\tr(A+B)&=\tr A+\tr B\\\tr(cA)&=c\tr A\\\tr(AB)&=\tr(BA)\end{align}$$The key observation for this is that, there is only one degree of freedom under these conditions: consider the basis $E_{ij}$ defined above, then $$\tr(E_{ii})=\tr(E_{ij}E_{ji})=\tr(E_{ji}E_{ij})=\tr(E_{jj}),\quad\forall i\neq j$$hence $E_{ii}=C,\forall1\le i\le n$, while $$\tr(E_{ij})=\tr(E_{ii}E_{ij})=\tr(E_{ij}E_{ii})=\tr(0)=0,\quad\forall i\neq j$$The above discussion means specifying $C$ immediately determines $\tr A,\forall A\in M_n(K)$.
	- **Cyclic property**: from $\tr(AB)=\tr(BA)$ we further deduce that $$\tr(A_1A_2\cdots A_n)=\tr(A_2A_3\cdots A_nA_1)$$and this can be applied recursively to yield an equality chain.
	- To put in another way, the trace is the unique linear functional ([[Algebraic Dual Space]]) that vanishes on the commutator subspace ([[Normal Subgroup]]) $[M_n(F),M_n(F)]$. 
- **Eigenvalue definition**: we have $\tr(AB)=\tr(BA)$, and applying this to $B=A^{-1}X$ we get $$\tr(A^{-1}XA)=\tr X,\quad\forall A\in\gl(b,\mathbb C)$$Hence we can define trace via [[Eigendecomposition]]: $$\tr A=\sum_{\lambda\in\sigma(A)}\lambda$$This can also be verified by Schur decomposition ([[Matrix Decomposition]]).
## Special types of matrix
- **Hermitian matrix**: $A=A^*$. 
	- **Eigenvalues**: for Hermitian $A$ we have $\lambda\in\mathbb R$. 
	- The Hermitian matrix can be diagonalized by a unitary matrix.
- **Orthogonal matrices**: $A\in M_n(\mathbb R)$ is orthogonal if $AA^T=A^TA=I$.
	- **Eigenvalue**: orthogonal matrix is always diagonalizable, with $|\lambda|=1$. The reasoning is basically the same as that of unitary matrix.
- **Unitary matrix**: $A\in M_n(\mathbb C)$ is unitary if $AA^*=A^*A=I\iff A^*=A^{-1}$. 
	- **Diagonalization**: unitary $A$ is normal (define below) by definition, hence is **unitarily similar** to a diagonal matrix, i.e., there exist $U$ unitary s.t. $A=U\Lambda U^*$. This follows from the fact that unitary matrix is inherently normal.
	- **Eigenvalue**: for $A$ unitary we have $|\lambda|=1$, since for eigenvector $v$ we have $$|\lambda|^2\braket{v,v}=\lambda\overline\lambda \braket{v,v}=\braket{\lambda v,\lambda v}=\braket{Av,Av}=\braket{v,A^*Av}=\braket{v,v}$$which immediately yield the result. 
- **Normal matrix**: $A\in M_n(\mathbb C)$ is normal if $AA^*=A^*A$. By [[Singular Value Decomposition]] or Schur decomposition ([[Matrix Decomposition]]) a matrix is normal iff it's unitarily similar to a diagonal matrix.
- A **circulant matrix** is defined by its first row, and each subsequent row is obtained by shifting the previous row one position to the right, wrapping the elements around to the first column if necessary.
	- **Eigenvalues**: Set $F(i,j)=\frac{1}{\sqrt{n}}\omega^{(i-1)(j-1)},\omega=e^{\frac{2\pi i}{n}}$ as the DFT matrix, then $C=FDF^{-1}$ where $D$ is the diagonalized matrix of $C$. [[Fourier Series]]. More explicitly, $\lambda(k)=c_0+c_1\omega^k+\dots+c_{n-1}\omega^{(n-1)k}$ for $k=0,1,\dots,n-1$.
- **Block matrix**: given $A\in K^{m\times n}$, a partition of $A$ is of the form $$A=\begin{bmatrix}A_{11}&A_{12}&\cdots&A_{1q}\\A_{21}&A_{22}&\cdots&A_{2q}\\\vdots&\vdots&\ddots&\vdots\\A_{p1}&A_{p2}&\cdots&A_{pq}\end{bmatrix}\quad\text{with}\quad A_{ij}\in K^{m_i\times n_j},m=\sum_{i=1}^pm_i,n=\sum_{j=1}^qn_j$$Addition and multiplication can be performed for two block matrices in the usual way when they're partitioned in a proper manner. In particular, for two matrices that are partitioned in the same manner, this happens only when the partition is symmetric in size.
## Matrix exponential
A partial motivation for matrix exponential is that, to calculate the differential of functions on $\gl(n,\mathbb R)$ we need a sequence of non-singular matrices, and since $\gl(n,\mathbb R)$ is only dense in $M_n(\mathbb R)$ matrix exponential is a natural choice. [[Differential Calculus on Smooth Manifold]]
The exponential of a matrix $A$ is defined as $$e^A=\sum_{k=0}^\infty\frac{A^k}{k!}=\lim_{k\to \infty}\left(I+\frac{A}{k}\right)^k$$The limit exists since $\Set{\sum_{k=0}^n\frac{\|A\|^k}{k!}}$ is a Cauchy sequence, where $\|\cdot\|$ is the Euclidean norm ([[Vector Space]]). [[Banach Space]]
- **Normed algebra structure**: $M_n(\mathbb R)$ is a normed algebra under the above norm. [[Field]]
- **Basic calculation properties**
	- If $XY=YX$, then $e^{X+Y}=e^Xe^Y$, by distributivity of normed vector space ([[Field]]).
	- $\frac{d}{dt}e^{tX}=Xe^{tX}$, which follows from similar proof as above.
	- If $|T|\neq0$ then $\exp(T^{-1}AT)=T^{-1}\exp(A)T$.
	- $\det(e^A)=e^{\tr(A)}$. First prove the result for upper triangular matrix, then make use of the Schur decomposition. [[Matrix Decomposition]]
- **Computation**: from Cayley-Hamilton ([[Characteristic Polynomial]]) we know that any $A^m$ with $m>n$ can be reduced to a polynomial of $A$ of degree less than $n$, thus we can write $$e^A=a_{n-1}A^{n-1}+\cdots+a_1A+a_0I$$Assume that $A$ has eigenvalues $\lambda_i(1\le i\le n)$, then we have $e^{\lambda_i}=\sum_{k=0}^{n-1}a_k\lambda_i^k$. For eigenvalues with $a_i>1$, we also have $e^{\lambda_i}=\frac{d^s}{d\lambda^s}\sum_{k=0}^{n-1}a_k\lambda^k|_{\lambda=\lambda_i}(1\le s\le a_i-1)$. Combining all the equations, would determine the value of $a_k$.
## Other info
### Cyclic vector
A vector $v\in F^n$ is called a **cyclic vector** for $A\in\mathcal L(F^n)$ if $$F^n=\span(v,Av,\cdots,A^{n-1}v)$$we have that $A$ has a cyclic vector iff $\chi_A=\mu_A$.
  **Proof**: for $\Rightarrow$, denote $v$ a cyclic vector for $A$, then the matrix of $A$ under the cyclic basis is $$A=\begin{bmatrix}0&0&\cdots&\cdots&0&*\\1&0&\cdots&\cdots&0&*\\0&1&0&\cdots&0&*\\\vdots&\vdots&\ddots&\ddots&\vdots&\vdots\\0&\cdots&\cdots&1&0&*&\end{bmatrix}$$hence we can easily verify that $\lambda I-A$ has rank $n-1$, which means for each eigenvalue there is only one eigenvector, which means $\chi_A=\mu_A$ by [[Characteristic Polynomial]]. For $\Leftarrow$ we construct $v$ via generator of Jordan chain ([[Jordan Normal Form]]): $$v=\sum_{i=1}^kv_i\where(\lambda_iI-A)^{a_i-1}v_i\neq0,\quad\forall 1\le i\le k$$If we assume that there exists a list of coefficients $\alpha_j,0\le j\le n-1$ s.t. $$0=\sum_{j=0}^{n-1}\alpha_jA^jv=\sum_{j=0}^{n-1}\alpha_j\sum_{i=1}^kA^jv_i=\sum_{i=1}^k\sum_{j=0}^{n-1}\alpha_jA^jv_i$$then by linear independence of generalized eigenvectors of different eigenvalues, we have $$\left(\sum_{j=0}^{n-1}\alpha_jA^j\right)v_i=0,\quad\forall 1\le i\le k$$On the other hand, denote $A_i=A|_{V_i}$ where $V_i$ the invariant subspace generated by $v_i$, then by [[Module]] and the construction of $v_i$ we know that $$\ann_{F[A_i]}(v_i)=\left((\lambda_iI-A)^{a_i}\right)\quad\Longrightarrow\quad(\lambda_iI-A)^{a_i}\ \left|\ \sum_{k=0}^{n-1}\alpha_jA^j\right.,\quad\forall1\le i\le k$$now since $(\lambda_i-x)^{a_i}$ are pairwise coprime ([[Polynomial Ring]]) we can further deduce that $$\prod_{i=1}^k(\lambda_iI-A)^{a_i}\ \left|\ \sum_{k=0}^{n-1}\alpha_jA^j\right.$$which immediately produces a contradiction by looking at the degree.
- **Matrix as polynomial in another**: if $A$ has a cyclic vector $v$, then for any $B$ that commutes with $A$, i.e., $AB=BA$ we have the implication $$Bv=\sum_{i=0}^{n-1}a_iA^iv\quad\Longrightarrow\quad B=\sum_{i=1}^{n-1}a_iA^i$$we can verify this directly for any $w\in F^n$. This provide an explicit method for expressing $B$ as the polynomial in $A$.