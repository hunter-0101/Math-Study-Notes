The **singular value decomposition (SVD)** of a [[Matrix]] $M\in\mathbb C^{m\times n}$ is the decomposition $$M=U\Sigma V^*\where U,V \text{ unitary},\Sigma\text{ diagonal and non-negative}$$Here $U,V$ serves as **rotations** (possibly combined with reflections), while $\Sigma$ serves as **scaling** by **singular values**. The decomposition is **unique up to reordering of singular values**.
**Proof**: notice that for any eigen-pair $(\lambda,v)$ of $M^*M$ we have $$\lambda\braket{v,v}=\braket{M^*Mv,v}=\braket{Mv,Mv}\ge0\quad\Longrightarrow\quad\lambda\ge0$$Hence $M^*M\in S_+^n(\mathbb C)$ ([[Quadratic Form]]). Since it's also Hermitian, by [[Eigendecomposition]] there exists unitary matrix $V$ that diagonalizes it: $$V^*M^*MV=\overline D=\begin{bmatrix}D&0\\0&0\end{bmatrix}\quad\text{where}\quad D\in\mathbb R_+^{l\times l},l\le\min(m,n)$$By dividing $V$ into block matrix corresponding to $D$ we have the more detailed equality $$\begin{bmatrix}V_1^*\\V_2^*\end{bmatrix}M^*M\begin{bmatrix}V_1&V_2\end{bmatrix}=\begin{bmatrix}V_1^*M^*MV_1&V_1^*M^*MV_2\\V_2^*M^*MV_1&V_2^*M^*MV_2\end{bmatrix}=\begin{bmatrix}D&0\\0&0\end{bmatrix}$$which implies $MV_2=0$. Now define $$U_1=MV_1D^{-1/2}\quad\Longrightarrow\quad U_1D^{1/2}V_1^*=M$$By extending $U_1$ into unitary basis $U=\begin{bmatrix}U_1&U_2\end{bmatrix}$ ([[Basis in Hilbert Space]]) and $D^{1/2}$ into $\Sigma$ by adding or removing unnecessary $0$'s to match the shape, then we get $M=U\Sigma V^*$. Moreover, it's easy to verify that $MM^*=U(\Sigma\Sigma^*)U^*$. 
- **Intuition**: SVD essentially identifies two orthonormal basis $F^n=\span(v_i),F^m=\span(u_j)$ that are corresponded under the action of $M$. 
	- **Relation with [[Eigendecomposition]]**: SVD applies to all shapes of matrices, while eigendecomposition only applies to square matrices, and two may differ for the same square matrix. Intuitively, this is because they identifies different kinds of objects: SVD finds two corresponding orthonormal basis, while eigendecomposition finds a basis that is purely stretched. 
- **Singular value & singular vectors**: the diagonal entries $\sigma_i$ of $\Sigma$ are called the singular values of $M$, which are explicitly given by $$\set{\sigma_i}=\sqrt{\sigma(MM^*)}$$The column vectors of $U,V$ are the **left/right singular vectors** 
	- **Non-negativity**: since $MM^*$ is positive semi-definite, we have $$\sigma_i\ge0,\quad\forall i$$Geometrically, it says that the directions of singular vectors can never be reverted. 
- **Remark on symmetric argument**: notice that both $MM^*,M^*M$ are Hermitian, we have $$MM^*=P_L\Lambda_LP_L^*,\quad M^*M=P_R\Lambda_RP_R^*$$where for $\lambda\neq0,\lambda\in\Lambda_L\iff\lambda\in\Lambda_R$ with the same multiplicity since $$MM^*x=\lambda x\Longrightarrow M^*M(M^*x)=\lambda(M^*x)\Longrightarrow MM^*(MM^*x)=\lambda(MM^*x)$$Assuming different multiplicity it's easy to deduce a contradiction of linear dependence of eigenvectors. On the other hand, expecting $M=U\Sigma V^*$ we have $$MM^*=U\Sigma V^*V\Sigma^*U^*=U(\Sigma\Sigma^*)U^*,\quad M^*M=V\Sigma^*U^*U\Sigma V^*=V(\Sigma^*\Sigma)V^*$$hence $U=P_L,V=P_R$, and $\Sigma$ being a rectangular diagonal matrix with entires of $\Lambda^{1/2}$. 
## Properties & Applications
- **SVD of Hermitian matrix**: given $M=M^*$, we have $$\sqrt{\sigma(M^*M)}=\sqrt{\sigma(M^2)}=\sigma(M)$$Therefore for $M$ Hermitian its singular values and eigenvalues equates. 
- **Length stretching ratio**: for any matrix $M$ we have $$\sigma_\min\le\frac{\|Mv\|}{\|v\|}\le\sigma_\max$$**Proof**: the action of $M$ on a vector $v\in F^n$ is specified by $$v=\sum_ia_iv_i\imply Mv=\sum_i\sigma_ia_iv_i$$Now assume $\|v\|=1$, then we get $$1=\|v\|^2=\sum_ia_i^2,\quad\|Mv\|^2=\sum_i\sigma_i^2a_i^2$$The estimation is trivial by this. 
	- **Bounding eigenvalue with singular value**: by definition of eigenvalue, it's trivial from the estimation of stretching ratio that $$\sigma_\max\ge\max|\lambda_i|$$This is quite useful in some sort of estimations. 
## SVD viewed under linear transform
When viewing $M\in\mathbb C^{m\times n}$ as a linear transform, SVD is essentially decomposing such transform into three components: rotation of $\mathbb C^n$, stretching of singular vectors, and rotation in $\mathbb C^m$.
- **Singular values/vectors**: as noted above, $$M(v_i)=\sigma_iu_i,M^*(u_i)=\sigma_iv_i,\quad i=1,\cdots,\min(m,n)$$hence if we look at the unitÂ ball in $\mathbb R^n$, then $\set{u_j},\sigma_i$ are exactly the directions and magnitudes of semi-axises of the corresponding hyper-ellipsoid.
- **Dimension adjustment**: for equivalent  $MV=U\Sigma$, we have the geometric interpretation $$\begin{align}&MV&\text{the images of the orthonormal basis }\set{v_i}\in\mathbb R^n\text{ under }M\\&U\Sigma&\text{a stretched version of  the orthonormal basis }\set{u_i}\in\mathbb R^m\end{align}$$here $\Sigma\in\mathbb R^{m\times n}$ serves as another dimension/magnitude adjustor that pairs the basis of two spaces together. Symmetrically we have the corresponding interpretation for $U^*M=\Sigma V^*$.
## SVD viewed under data points
In application a matrix usually represents a set of data points, such as in [[Computer Vision]] and [[Dimensionality Reduction]]. In these cases SVD serves to provide some hidden information of the data points, and can be used to construct useful things, such as recommendation system.
- **Least square fit of linear system**: given approximation $Mx\approx b$, notice that $$\|Mx-b\|_2=\|\Sigma V^*x-U^*b\|_2$$for this to be minimized we must have $\hat x=V\Sigma^\dagger U^*=M^\dagger b$. [[Matrix]]
- **Best fit subspace**: consider $M$ as consisting of $m$ data points in its rows with $n$ features. Since $|\langle m_i,v\rangle|$ measures approximation of $m_i$ by unit vector $v$, finding best fit subspace is equivalent to solving $\arg\min_v|Mv|^2$. Hence if we recursively define $$v_k=\underset{v_k\perp v_i,\forall i<k,|v_k|=1}{\arg\max}|Av|,\quad\sigma_k=|Av_k|$$Then $V_k=\span(v_1,\cdots,v_k)$ is the best-fit $k$-dimensional subspace for $A$. Under $M=U\Sigma V^*$ it can be easily seen that these definition coincide exactly with singular values/vectors.
	- **Alternative definition**: if we first define $v_k,\sigma_k$ as above and let $u_k=Mv_k/\sigma_k$, then $M=\sum_i\sigma_i u_iv_i^*$ since a matrix is uniquely determined by all its actions on basis ([[Vector Space]]). In this case we're using the best fit subspace to define SVD.
- **Matrix splitting**: as discussed above we have $$M=\sum_{i}\sigma_iu_iv_i^*$$Assuming $\sigma_i\ge\sigma_{i+1}$ (**order of importance**), the truncated version of this splitting formula is exactly what we've obtained in [[Dimensionality Reduction#PCA Principle component analysis]].