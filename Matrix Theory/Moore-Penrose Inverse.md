Given a [[Field]] $F$ and integers $m,n\in\mathbb N$, the **Moore-Penrose inverse (pseudoinverse)** of a [[Matrix]] $A\in F^{m\times n}$ is a matrix $A^\dagger\in F^{n\times m}$ satisfying:
1. **Weak inverse**: $A^\dagger$ is the weak left & right inverse, in the sense that $$AA^\dagger A=A,\quad A^\dagger AA^\dagger=A^\dagger$$The first equation ensures that $AA^\dagger$ acts identically on $\im A$, while the second equation does the **reflexive** part for $A^\dagger A$. 
	- **Generalized inverse**: a matrix satisfying only the first equation is called a **generalized inverse**, and when the second equation is also satisfied it's said to be a **reflexive generalized inverse**. 
	- **Operator construction**: we deduce from the equations that $$\paren{AA^\dagger}^2=AA^\dagger,\quad\paren{A^\dagger A}^2=A^\dagger A$$Therefore both $AA^\dagger,A^\dagger A$ are projectors. 
2. **Hermitian condition**: the product of $A$ and $A^\dagger$ satisfies $$\paren{AA^\dagger}^*=AA^\dagger,\quad\paren{A^\dagger A}^*=A^\dagger A$$These conditions forces $AA^\dagger,A^\dagger A$ to be unitary, therefore **orthogonal projectors** ([[Hilbert Space]]), ensuring its uniqueness. 
This is the mostly widely used generalization of the notion of inverse matrix. Its applications includes providing the minimal-norm least-squares solution to $Ax=b$. Specifically, $x = A^\dagger b$ minimizes $\|Ax - b\|_2$, and among all such minimizers, it minimizes $\|x\|_2$. 
- **Operator point of view**: viewing $A$ as a linear operator ([[Vector Space]]) $\mathcal{A}: V \to W$, then $A^\dagger$ acts as an inverse on the restriction $\mathcal{A}|_{(\ker A)^\perp} \to \text{im}(A)$, while mapping $(\text{im} A)^\perp$ to $\{0\}$.
## Properties
- **Uniqueness**: for every $A \in F^{m \times n}$, there exists a unique $A^\dagger$.
  **Proof**: assume $X$ and $Y$ satisfy all four conditions. By algebraic manipulation of the Penrose equations (e.g., $X = XAX = X(AX)^* = X X^* A^* = \dots$), one can show $X = Y$.
- **Explicit expression**: using [[Singular Value Decomposition]] $A^\dagger$ can be explicitly represented as $$A=U\Sigma V^*\qiffq A^\dagger=V\Sigma^\dagger U^*$$where the corresponding singular values are constructed via $$\Sigma=\diag(\sigma_1,\cdots,\sigma_r,0,\dots,0)\qiffq\Sigma^\dagger=\diag\paren{\sigma_1^{-1},\cdots,\sigma_r^{-1},0\cdots,0}$$This can be verified directly, and highlights that $A^\dagger$ effectively inverts the non-zero singular values and leaves the null space intact.
- **Full-rank specializations**: if $A$ has **full column rank** ($n \le m$), then $A^*A$ is invertible: $$A^\dagger = (A^*A)^{-1}A^*$$Similarly, if $A$ has **full row rank** ($m \le n$), then $AA^*$ is invertible: $$A^\dagger = A^*(AA^*)^{-1}$$
	- **Geometric intuition**: this result is also quite trivial if we view $A$ as a linear operator ([[Vector Space]]) - as long as the map is injective, we can also find its partial inverse by mapping the corresponding vectors back to their preimage. 
- **Orthogonal projector**: as discussed above, we have the orthogonal projectors $$P_{\im A} = AA^\dagger,\quad P_{\im A^*} = A^\dagger A$$By the four fundamental classes ([[Operator on Hilbert Space]]) we can further construct $$P_{\ker A}=I - A^\dagger A,\quad P_{\ker A^*}=I-AA^\dagger$$These operators are quite commonly used in many different fields. 
## Actionable Theoretical Tools
-  **Reverse order law condition**: unlike the standard inverse, $(AB)^\dagger = B^\dagger A^\dagger$ does **not** hold in general. We have that $$(AB)^\dagger = B^\dagger A^\dagger\qiffq\begin{cases}\text{im}(A^*AB) \subseteq \text{im}(B)\\\text{im}(BB^*A^*) \subseteq \text{im}(A^*)\end{cases}$$**Proof**: 
- **Limit representation**: we have $$A^\dagger = \lim_{\delta \to 0^+} (A^*A + \delta I)^{-1} A^* = \lim_{\delta \to 0^+} A^* (AA^* + \delta I)^{-1}$$This serves as a powerful regularization tool (Tikhonov regularization). When $A^*A$ is ill-conditioned, using a small $\delta > 0$ provides a numerically stable approximation of $A^\dagger$.

- **Rank consistency lemma**: in general we have $$\text{rank}(A) = \text{rank}(A^\dagger) = \text{rank}(A A^\dagger) = \text{rank}(A^\dagger A)$$
	- **Counter-example:** One might assume $(A^2)^\dagger = (A^\dagger)^2$. This is false for general $A$. For example, if $A = \begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix}$, then $A^2 = 0 \implies (A^2)^\dagger = 0$, but $(A^\dagger)^2 = \begin{pmatrix} 0 & 0 \\ 1 & 0 \end{pmatrix}^2 = 0$. While it holds here, it fails to preserve the mapping properties if $A$ is not EP (i.e., $\ker A \neq \ker A^*$). 