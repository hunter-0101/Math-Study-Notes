A **matrix norm** is a norm on the [[Vector Space]] of matrices $F^{m\times n}$ over some [[Field]] $F$, making it into a [[Normed Vector Space]]. Below we'll primarily assume that $F=\mathbb C$.
- **Equivalence of norms over matrix space**: notice that $$\dim F^{m\times n}=mn<\infty$$Therefore all norms on the space are pairwise equivalent. It's for this reason that in some scenarios we only say "matrix norm" without specifying which norm it actually is, although it's still quite important to distinguish between them due to different computational scenarios. 
## Operator norm
Following [[Normed Vector Space]], given vector norms $\|\cdot\|_\alpha,\|\cdot\|_\beta$ on $F^n,F^m$, any matrix $A_{mn}$ admits the operator norm $$\|A\|_{\alpha,\beta}=\sup\Set{\norm{Ax}_\beta:x\in F^n,\norm{x}_\alpha=1}=\sup\Set{\frac{\|Ax\|_\beta}{\|x\|_\alpha}:x\in F^n,x\neq0}$$When $\alpha=\beta=p$, the norm is denoted $\|A\|_p$.
- **Column norm**: $\|A\|_1=\max_j\sum_{i=1}^m|a_{ij}|$.
- **Row norm**: $\|A\|_\infty=\max_i\sum_{j=1}^n|a_{ij}|$.
- **Spectral norm**: $\|A\|_2=\sup\{x^*Ay:x\in K^m,y\in K^n,\|x\|=\|y\|=1\}$ by Cauchy Schwarz inequality.
	- **Square matrix**: since $AA^*$ is self-adjoint, we have $\|AA^*\|_2=\rho(AA^*)$ via diagonalization ([[Eigendecomposition]]). Since $\|A\|_2=\sqrt{\|AA^*\|_2}$ ([[Operator on Hilbert Space]]), $$\|A\|_2=\sqrt{\rho(AA^*)}=\sqrt{\rho(A^*A)}$$where we used the fact that $\sigma(AA^*)=\sigma(A^*A)$ except for possible zeros ([[Singular Value Decomposition]]).
		- **Corollary**: for $A=A^*$ we have $\|A\|_2=\rho(A)$.
- **Sub-multiplication property**: $\|AB\|\le\|A\|\cdot\|B\|$ for any operator norm by its definition.
- **Relation with spectral radius**: as long as $\|A\|_*<1$ for one $\|\cdot\|_*$ we'll have $\rho(A)<1$ by linearity of norm ([[Operator on Hilbert Space]]). Intuitively, norm controls the ensure unit ball, while [[Eigendecomposition]] only controls a small set of vectors.
## Entry-wise norm
We could also define $$\|A\|_{p,p}=\norm{\operatorname{vec}(A)}_p=\paren{\sum_i\sum_j|a_{ij}|^p}^{\frac{1}{p}}$$This kind of norm just reshape the matrix into vector and apply the vector norm, and the general form is usually denoted as $L_{p,q}$ norm. In this case $$\|A\|_{p,q}=\paren{\sum_j\paren{\sum_i|a_{ij}|^p}^{\frac{q}{p}}}^\frac{1}{q}$$Note that this kind of norm is relatively rarely applied in math, but more commonly used in engineering scenarios due to their computational efficiency. 
- **Frobenius norm**: we define $$\|A\|_F=\|A\|_{2,2}=\sqrt{\sum_{1\le i,j\le n}|a_{ij}|^2}=\sqrt{\tr(AA^*)}=\sqrt{\sum_{i=1}^n\sigma_i^2}$$here $\sigma_i$ are singular values ([[Singular Value Decomposition]]) of $A$. By the result on length stretching ratio ([[Singular Value Decomposition]]) we have $$\sigma_\min\le\|A\|_2\le \sigma_\max\imply \|A\|_F\ge\|A\|_2$$Note that we also have $\|A\|_F\le\sqrt{\rank A}\|A\|_2$, although this relies on some more subtle estimation. 
	- **Sub-multiplicativity**: using Cauchy-Schwarz inequality ([[Lp Space]]) and the cyclic property of matrix trace ([[Matrix]]). for $A,B\in M_n(\mathbb C)$ we have that $$\begin{align}\|AB\|_F^2&=\tr\paren{ABB^*A^*}=\tr\paren{A^*ABB^*}\\&=\sum_{i,j}(A^*A)_{ij}\cdot(BB^*)_{ji}\\&\le\sqrt{\paren{\sum_{i,j}(A^*A)_{ij}^2}\paren{\sum_{i,j}(BB^*)_{ji}^2}}\\&=\|A^*A\|_F\|BB^*\|_F\\&=\sqrt{\paren{\sum_i\sigma_i^4(A)}\paren{\sum_i\sigma_i^4(B)}}\\&\le\paren{\sum_i\sigma_i^2(A)}\paren{\sum_i\sigma_i^2(B)}\\&=\|A\|_F^2\|B\|_F^2\end{align}$$where we've used the fact that $AA^*$ is Hermitian, and a simple vector-norm inequality $$\sqrt{\sum_ix_i^2}\le\sum_ix_i,\quad\forall x_i>0$$Therefore the F-norm is sub-multiplicative. Note that we could alternatively apply Cauchy-Schwartz inequality directly to the entry-wise expression and derive a simper proof. 
## Hilbert-Schmidt norm
The **HS norm** is define as $$\|A\|_\text{HS}^2=\sum_{i\in I}\|Ae_i\|^2$$where $\set{e_i}_{i\in I}$ is a orthonormal basis.
- **Well-defined-ness**: consider two orthonormal basis $\set{e_i},\set{f_j}$, we have $$\sum_i\|Ae_i\|^2=\sum_{i,j}\left|\braket{Ae_i,f_j}\right|^2=\sum_{i,j}\left|\braket{e_i,A^*f_j}\right|^2=\sum_j\|A^*f_j\|^2$$hence by taking $\set{e_i}=\set{f_j}$ we get $\|A\|_{HS}=\|A^*\|_{HS}$, and replace $A$ with $A^*$ we get $\sum_i\|Ae_i\|^2=\sum_j\|Af_j\|^2$, which yields independence.
- **Relation with Frobenius norm**: by picking $e_i$ as the standard orthonormal basis we can verify that the HS-norm actually coincide with F-norm. However, a crucial distinction between these two definitions is that, F-norm is defined only for matrices, and are basis-dependent, while HS-norm can be defined for any [[Operator on Hilbert Space]] on [[Hilbert Space]] and is independent of basis.