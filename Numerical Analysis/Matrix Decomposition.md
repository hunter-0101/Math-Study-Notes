In numerical analysis, there are different kinds of matrix decomposition designed for different tasks and matrix algorithms. Here we'll mainly focus on those for solving linear equations.
## LU decomposition
Utilizing Gaussian elimination ([[Solving Linear System]]) we get a genera representation $$\begin{pmatrix}u_{11}&u_{12}&u_{13}&\cdots&u_{1n}\\l_{21}&u_{22}&u_{23}&\cdots&u_{2n}\\l_{31}&l_{32}&u_{33}&\cdots&u_{3n}\\\vdots&\vdots&\vdots&\ddots&\vdots\\l_{n1}&l_{n2}&l_{n3}&\cdots&u_{nn}\end{pmatrix},\quad\begin{cases}u_{1j}=a_{1j},&j=1,\cdots,n\\l_{i1}=\frac{a_{i1}}{u_{11}},&i=2,\cdots,n\\u_{ij}=a_{ij}-\sum_{k=1}^{i-1}l_{ik}u_{kj},&i=2,\cdots,n;j=i,\cdots,n\\l_{ij}=\frac{a_{ij}-\sum_{k=1}^{j-1}l_{ik}u_{kj}}{u_{jj}},&j=2,\cdots,n-1;i=j+1,\cdots,n\end{cases}$$which we call the **compact form**, since in $A=LU$ the diagonal entries of $L$ are all $1$.
- **Variants**: **Doolittle decomposition** refers to the above form. Another version that exchanges order of column and line is called **Crout decomposition**.
	- **Hermitian matrix**: for $A=A^*$ we have that $A=LU=LDD^{-1}U\Rightarrow D^{-1}U=L^*$, hence $A=LDL^*$. We can this **improved square root method**.
	- **Cholesky decomposition**: for $A$ is symmetric and positive-definite, by rearrangement we have$A=(LD^{1/2})(D^{1/2}L^T)=GG^T$. Solving linear system via this process is called the **square root method**.
- If $|A|\neq0$, and its leading principle minors are nonzero, then $A=LU=LDU$, obtained with Gaussian elimination.
- Block LDU decomposition$$\begin{pmatrix}A&B\\C&D\end{pmatrix}=\begin{pmatrix}I&0\\CA^{-1}&I\end{pmatrix}\begin{pmatrix}A&0\\0&D-CA^{-1}B\end{pmatrix}\begin{pmatrix}I&A^{-1}B\\0&I\end{pmatrix}$$
## QR decomposition
Any real matrix $A$ can be decomposed as $A=QR$ where $Q$ orthogonal and $R$ upper triangular.
- **Givens rotation**: given $i,j,\theta$, the Givens matrix is of the form $$G(i,j,\theta)=\begin{bmatrix}1\\&\ddots\\&&\cos\theta&&\sin\theta\\&&&\ddots\\&&-\sin\theta&&\cos\theta\\&&&&&\ddots\\&&&&&&1\end{bmatrix}$$Obviously $G(i,j,\theta)$ is orthogonal, and serves as a rotation within the plane specified by $i,j$.
	- Given $x\in\mathbb R^n$ there exists finite product of Givens rotations $G=\prod_{k=1}^m G_k$ s.t. $Gx=\|x\|_2e_i$ where $e_i$ is the $i$-th unit vector chosen arbitrarily.
	- **QR based on Givens**: for any $A$, perform "Gaussian elimination" based on Givens transforms to procedurally transform $A$ into upper triangular $R$, then we have $P_s\cdots P_1A=R\iff A=P^TR=QR$, since $P$ is orthogonal.
- **Householder reflection**: given $\|u\|_2=1$, the matrix $H_u=I-2uu^*$ is a reflection matrix w.r.t the plane orthogonal to $u$, and expressed as an operator we have $H_u(x)=x-2\langle x,u\rangle u$.
	- Given $x\in\mathbb R^n$ and some unit vector $v$ there exists $H_u$ s.t. $H_u(x)=\sigma v,\sigma\in\mathbb R$. Actually we only need to choose $u=\frac{x-\sigma v}{\|x-\sigma v\|_2}$.
	- **QR based on Householder**: totally similar to the Givens rotation case.
- **Geometric interpretation**: this decomposition essentially performs a Gram-Schmidt process ([[Basis in Hilbert Space]]). Geometrically, $Q$ is a base and $R$ is the coordinate, i.e., $$Q=\begin{bmatrix}e_1&\cdots&e_n\end{bmatrix}=\operatorname{GS}(a_1,\cdots,a_n),\quad R=[\langle e_i,a_j\rangle]_{i\le j}$$given that $A=[a_1,\cdots,a_n]$.
- **Application**: QR decomposition is unique for full-rank matrices, and is usually used to solve ill linear equations
## Schur decomposition
**Schur decomposition theorem**: for any $A\in\mathbb C^n$ there exists unitary $Q$ s.t. $Q^*AQ=U$ an upper triangular matrix. $A=QUQ^*$ is called the **Schur form**.
**Proof**: pick a eigenvalue/vector pair $(\lambda_1,v^{(1)}_1)$ and expand $v_1$ into orthonormal basis $v_i^{(1)}(1\le i\le n)$. Let $Q_1=[v_1^{(1)},\cdots,v_n^{(1)}]$, then $Q_1$ is unitary, and by checking image of $v_1$ under $A$ as well as that $Q_1^*AQ$ is unitary we have  $$Q_1^*AQ=\begin{bmatrix}\lambda_1&*&\cdots&*\\0&*&\cdots&*\\\vdots&\vdots&&\vdots\\0&*&\cdots&*\end{bmatrix}$$denote the bottom-right $n-1\times n-1$ sub-matrix as $B$, and choose a new pair $(\lambda_2,v_2)$ as above, and expand it into $V^{(2)}=[v_i^{(2)}](2\le i\le n)$, and consider $$Q_2=\begin{bmatrix}1&\mathbf 0\\\mathbf 0&V^{(2)}\end{bmatrix}$$then it can be verified that $Q_2$ changes second row-column into $\lambda_2$ and $0$'s. Repeat the above until $A$ is turned into an upper triangular matrix.
- **Intuition**: from the perspective of linear operator ([[Vector Space]]) Schur decomposition is essentially trying to find an **orthonormal basis** $\set{e_k}$ for operator $T$ s.t. $$T(e_k)\subset\span(e_1,\cdots,e_k),\quad\forall 1\le k\le n$$This is obviously achievable via induction, since after picking $k$ vectors there will be $n-k-1$ degree of freedom (up to permutation) for choosing a new vector in orthogonal complement ([[Hilbert Space]]), while there are only $n-k-2$ restrictions for choosing it.
- **Difference with [[Spectral Theorem]]**: in spectral theorem the induction is ensured by the fact $$T:W\to W\quad\Longrightarrow\quad T:W^\perp\to W^\perp$$hence the restricted operator $T|_{W^\perp}$ is meaningful. For Schur decomposition after picking $e_1,\cdots,e_k$ we're not restricting the operator itself but rather **ignoring** its image lying within $\span(e_1,\cdots,e_k)$ so that a "pseudo-eigenvector" can be picked, resulting in upper triangular matrix rather than diagonal matrix.
- **Uniqueness issue**: in general the decomposition is not unique, although in practical case we would usually only use its existence.