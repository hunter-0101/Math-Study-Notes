Given $f\in C([a,b])$ and $X_n=\set{x_i}_{0\le i\le n}$ there exists a unique polynomial $p(x)$ with $\deg p(x)\le n$ that interpolates $f$ at $X$, by the Vandermonde determinant ([[Determinant]]). 
**Polynomial interpolation theorem**: given a [[Field]] $F$ and pairwise distinct $\lambda_k\in F,1\le k\le n$ and any $a_k\in F,1\le k\le n$ there exist $p\in F[X]$ s.t.. $p(\lambda_k)=a_k,\forall k$.
**Proof**: since $F$ is a field we know that $(X-\lambda_k)$ are pairwise coprime, and since $$p(\lambda_k)=a_k\quad\iff\quad p\equiv a_k\pmod{X-\lambda_k}$$This is essentially a congruence system, hence by CRT ([[Ideal]]) there exists $$\overline p\in F[X]\left/\bigcap_{k=1}^n\right.(X-\lambda_k),\quad \overline p\mapsto(a_1\bmod(X-\lambda_1),\cdots,a_n\bmod(X-\lambda_k))\in\prod_{k=1}^nF[X]/(X-\lambda_k)$$Hence any element in $\overline p$ is an interpolation polynomial.
- **Linearity**: the interpolation operator$L_n:C([a,b])\to\mathbb P_n$ is linear for fixed interpolating points.
  **Proof**: note that what we're essentially doing is interpolating across $(y_i)\in\mathbb R^{n+1}$ given $(x_i)$ fixed, hence $L_n(af)+L_n(bg)$, being a polynomial interpolating $(af+bg)(x_i)$ with degree at most $n$, must be the unique interpolant $L_n(af+bg)$.
	- **Alternative proof**: this result is also obvious by any of the explicit interpolation formulas below, by investigating the coefficient for each term.
- **Remainder**: $R_n(x)=f(x)-p(x)$ is given by $$R_n(x)=\frac{f^{(n+1)}(\xi)}{(n+1)!}\prod_{i=0}^n(x-x_i),\quad\xi\in[x_0,x_n]$$**Proof**: define an auxiliary function $$Y(t)=R(t)-\frac{R(x)}{W(x)}W(t)\quad\text{where}\quad W(x)=\prod_{i=0}^n(x-x_i)$$then $Y^{(n+1)}(t)=f^{(n+1)}(t)-n!R(x)/W(x)$ since $\deg p=n$. On the other hand $Y(x_i)=0$, hence by repetitively applying Rolle's theorem ([[Differential Calculus on Euclidean Space]]) there exists some $\xi$ lying between $x_i$ s.t. $Y^{(n+1)}(\xi)=0$, hence yielding the result.
	- **Motivation**: to find $r=\frac{R(x)}{W(x)}$ a first intuition is to apply Cauchy's mean value theorem so $$\frac{R(x)}{W(x)}=\frac{R(x)-R(x_i)}{W(x)-W(x_i)}=\frac{R^{(1)}(x_i^{(1)})}{W^{(1)}(x_i^{(1)})}=\cdots$$but to proceed further we need $R^{(k)}(x_i^{(k)})=W^{(k)}(x_i^{(k)})=0,0\le i\le n-k$ for each $k\le n$, which is impossible. Hence we step back to look for instead $$\frac{R(x)}{W(x)}=\frac{R(x_i)}{W(x_i)}\quad\text{for some }x_i$$If enough such $x_i$ could be found then the above process could go on smoothly. At this point it's naturally to consider the $Y(t)$ defined above.
	- **Generalization**: the above proof yields a stronger result: any $f\in C^{(n+1)}$ with $n+1$ zeros $x_i$ can be written as $f(x)= \frac{f^{(n+1)}(\xi)}{(x+1)!}\prod(x-x_i)$ for some $\xi=\xi(x)$ lying between $x_i$.
	- **Intuition**: since $p^{(n+1)}\equiv0$ for all $\deg p\le n$, the difference between $f$ and $p$ is expected to be encapsulated by $f^{(n+1)}$. We take $W(x)=\prod(x-x_i)$ because $R(x_i)=0,\forall i$. This is spiritually the same as Taylor series. [[Differential Calculus on Euclidean Space]]
	- **Discussion on $\xi$**: it's obvious that $\xi=\xi(x,x_i)$ is a function of both the truncated point and interpolation points. Given two different interpolating points $x_i,\tilde x_i$ we have $$f(x)=p_1(x)+\frac{f^{(n+1)}(\xi_1(x))}{(n+1)!}W_1(x)=p_2(x)+\frac{f^{(n+1)}(\xi_2(x))}{(n+1)!}W_2(x)$$Hence it might be temping to rewrite the above equality as $$p_1(x)-p_2(x)=\frac{f^{(n+1)}(\xi_2(x))}{(n+1)!}W_2(x)-\frac{f^{(n+1)}(\xi_2(x))}{(n+1)!}W_1(x)$$and conclude by degree of polynomial that $f^{(n+1)}(\xi_1(x))=f^{(n+1)}(\xi_2(x))$ for all $x$. However this is invalid since $\xi_1,\xi_2$ are not even known to be continuous - the only thing we know it that it changes as $x$ changes. Yes, **it might not even be a function of $x$**! (although we still write $\xi=\xi(x)$ to make space for the footnote)
- **Runge's theorem**: when $n$ increases the remainder would increases rapidly, resulting in severe oscillation at the edge of the interval. To mitigate this a common method is to interpolate through **Chebyshev nodes** $x_k=\cos(\frac{2K+1}{2n}\pi)$ rather then evenly distributed ones, which minimizes that product term (i.e., $\|\cdot\|_\infty$ error). See detail in [[Chebyshev Polynomial]].
	- **Operator perspective**: denote $L_n:C([a,b])\to P_n([a,b])$ the Lagrange interpolation operator given interpolation points $x_i$, then Runge's theorem is basically saying that $L_n$ corresponding to $x_i=x_0+ih$ is unbounded. 
## Newton interpolation
**Intuition**: Newton's main idea is that, since any $k+1$ points uniquely determines a polynomial of degree $k$, while very obviously a linear interpolation between two points is exactly given by $$y=y_0+(x-x_0)\frac{y_1-y_0}{x_1-x_0}$$we could generalize the construction to arbitrarily many points progressively, ensuring that any newly added polynomial does not affect the value at previous points. Some calculation shows that this can be achieved by the Newton basis polynomial, with coefficient given by the forward divided difference.
With divided difference defined by $$[y_k]=y_k,\quad[y_k,\cdots,y_{k+j}]=\frac{[y_{k+1},\cdots,y_{k+j}]-[y_k,\cdots,y_{k+j-1}]}{x_{k+j}-x_k}$$we use **Newton basis polynomial** to define the Newton interpolation by $$N_n(x)=\sum_{j=0}^n[y_0,\cdots,y_j]n_j(x),\quad n_j(x)=\prod_{i=0}^{j-1}(x-x_i)$$with $n_0(x)=1$. If $x_i=x_0+ih,x=x_0+sh$, then $x-x_i=(s-i)h$, and we have $$\begin{array}{ll}\text{Forward interpolation}&N(x)=\sum_{i=0}^nC_s^i i!h^i[y_0,\cdots,y_i]=\sum_{i=0}^nC_s^i\Delta^{i}f(x_0)\\ \text{Backward interpolation}&N(x)=\sum_{i=0}^n(-1)^iC_{-s}^i i!h^i[y_n,\cdots,y_{n-i}]=\sum_{i=0}^n(-1)^iC_{-s}^i\nabla^{i}f(x_n)\end{array}$$since in this case we have $[y_j,\cdots,y_{j+n}]=\frac{1}{n!h^n}\Delta^{n}y_j,[y_j,\cdots,y_{j-n}]=\frac{1}{n!h^n}\nabla^{n}y_j$. [[Umbral Calculus]]
- **Relation with Newton's formula**: If we generalize the interpolation to infinite points, we'd get exactly the Newton series in [[Umbral Calculus]].
- **Calculation**: we can easily calculate the coefficients via $$\begin{array}{c}x&y&y^{(1)}&y^{(2)}&y^{(3)}&\cdots\\\hline x_0&\color{blue}{y_0}\\x_1&y_1&\color{blue}{[y_0,y_1]}\\x_2&y_2&[y_1,y_2]&\color{blue}{[y_0,y_1,y_2]}\\ x_3&y_3&[y_2,y_3]&[y_1,y_2,y_3]&\color{blue}{[y_0,y_1,y_2,y_3]}\\&&&\vdots\end{array}$$then the Newton polynomial can directly written out. When some $x_i$ coincides the given sampling data is place in the corresponding place directly.
- **Property of difference**
	- **Expression by data points**: by uniqueness of interpolation polynomial we can compare the coefficients to get $$f[x_0,\cdots,x_n]=\sum_{i=0}^n\frac{f(x_i)}{\prod_{j\neq i}(x_i-x_j)}$$thus divided difference is independent of the order of points.
	- **Relation with derivative**: we have that $$f[x_i,\cdots,x_{i+k}]=\frac{f^{(k)}(\xi)}{k!},\quad \xi\text{ lies between }x_j$$hence we have that $f[\underbrace{x_i,\cdots,x_i}_{k}]=\frac{f^{(k)}(x_i)}{k!}$.
- **Remainder**: if we view $x$ as a new sampling point then $$R(x)=[y_0,\cdots,y_n,f(x)]\prod_{i=0}^n(x-x_i)\approx N_{n+1}(x)-N(x)$$where $f(x)$ is replaced by suitable $y_{n+1}$ for estimation.
## Lagrange interpolation
We define the **Lagrange interpolation** as $$L_n(X,f)=\sum_{i=0}^nf(x_i)\ell_{i,n}(X)\quad\text{where}\quad \ell_{i,n}(X)(x)=\prod_{j\neq i}\frac{x-x_j}{x_i-x_j}$$Here $\ell_{i,n}(X)$ are referred to as **Lagrange basis** satisfying the property $$\ell_{i,n}(X)(x_k)=\delta_i^k,\quad\forall0\le i,k\le n$$Main problem with this is expensive computation and inability to reuse the lower order ones.
- **Interpretation**: finding interpolation amounts to inverting the Vandermonde matrix, which is computationally expensive. To avoid this we choose the Lagrange basis, which transforms the matrix into identity. This can be seen from the similarity in form between Lagrange basis and inverse of Vandermonde matrix. [[Determinant]]
- **Degeneracy**: if we're approximating some polynomial $f$ with $\deg f\le n$ then $L(x)$ is accurate and degenerate to $f$ itself. Using this we can find some identities like $\sum_{i=0}^n \ell_{i,n}x^k=x^k,k\le n$.
## Lebesgue constant
Given $X=\set{X_n:n\in\mathbb N}$ a node system on $[a,b]$ define the **Lebesgue function** as $$\lambda_n(X)(x)=\sup_{\|f\|\le1}|L_n(X,f)(x)|=\sum_{i=0}^n|\ell_{i,n}(X)(x)|,\quad x\in[a,b]$$and the **Lebesgue constant** as $$\Lambda_n(X)=\sup_{\|f\|\le 1}\|L_n(X,f)\|=\max_{x\in[-1,1]}\lambda_n(X)(x)$$It's mostly commonly assumed that $\|\cdot\|=\|\cdot\|_\infty$. [[Lp Space]]
- **Convergence**: by definition we have $$\Lambda_n(X)\to\infty\text{ as }n\to\infty,\quad\forall X=\set{x_0,\cdots,x_n}\subset[a,b]$$hence by UBP ([[Banach Space]]) there exists $f=f(X)\in C([a,b])$ s.t. $L_n(X,f)$ does not converge to $f$ uniformly. However when $f$ is well-behaved enough uniform convergence can be achieved, e.g., [[Chebyshev Polynomial]].
Below we provide a lis of results for several commonly used node systems.
- **Equally-spaced nodes**: set $$E=\set{E_n}\quad\text{where}\quad E_n=\Set{x_{i,n}=1-\frac{i}{n}:i=0,1,\cdots,n}$$by simple calculation we have $$\Lambda_n(E)\sim\frac{2^n}{en\ln n},\quad n\to\infty$$This result has be refined to a small extent by other authors.
- **Chebyshev nodes of the first kind**: by [[Chebyshev Polynomial]] define $$T=\set{T_n}\quad\text{where}\quad T_n=\Set{x_{i,n}=\cos\frac{i-1/2}{n+1}:i=1,2,\cdots,n+1}$$By direct computation we have that $$\Lambda_n(T)=\lambda_n(T)(\pm1)=\frac{1}{n}\sum_{i=1}^{n+1}\cot\frac{2i-1}{4(n+1)}$$With asymptotic result given by $$\Lambda_n(T)=\frac{2}{\pi}\ln n+\frac{2}{\pi}\left(\gamma+\ln\frac{8}{\pi}\right)+O\left(\frac{1}{n^2}\right),\quad n\to\infty$$here $\gamma$ is the Euler constant ([[Special Constant Gallery]]).