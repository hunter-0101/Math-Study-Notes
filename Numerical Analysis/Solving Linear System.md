In this note we consider solving linear equation of the form $$Ax=b$$ where $A\in\mathbb R^{n\times n}$. Below we provide some general discussion on solution of this system.
- $b=0$: in this case $x\in\ker A$, hence the solution space is a linear subspace of $\mathbb R^n$. Via [[Matrix]] we can show that $$\dim\ker A=n-\rank A$$Using the method of Gaussian elimination discussed below we can easily solve the system to get one basis for $\ker A$.
- $b\neq0$: in such case the solution space is an affine space ([[Euclidean Space]]) by superposition. In particular when $A\in\gl(n,\mathbb R)$ the particular solution can be obtained by the Cramer's rule $$x_i=\frac{\det A_i}{\det A}\quad\text{where}\quad A_i=A(I-E_{ii})+b e_i^T$$That is, $A_i$ is obtained by replacing its $i$-th column with $b$.
  **Proof**: denote $A=\begin{bmatrix}a_1&\cdots& a_n\end{bmatrix}$, then we get $\sum_{i=1}^na_ix_i=b$. By substituting this into $i$-th column of $A$ and utilize properties of [[Determinant]] we get $$\det A_i=\sum_{i=1}^n\det(\begin{bmatrix}a_1&\cdots&x_ia_i&\cdots&a_n\end{bmatrix})=x_i\det A$$which completes the proof.
## Gaussian elimination
Gaussian elimination is numerically stable iff the minors $\det A_{(1\cdots k),(1\cdots k)}\neq0$ for all $1\le k\le n$. In practice we usually strengthen the condition to one of the following: 
- $A$ is **positive definite**.
- $A$ is **strictly diagonally dominant**.
Since during the elimination it might be the case that some pivoting element is very small, which may enlarge the error of calculation significantly. To handle these cases we may utilize pivoting:
- **Partial pivoting**: generally, before the $k$-th step of elimination, we choose $\arg\max_p|a_{pk}^{(k-1)}|$ and perform the exchange $r_p\leftrightarrow r_k$. This ensures that the elimination coefficients are all less than $1$, increasing numerical stability.
- **Complete pivoting**: more extremely, before the $k$-th step of elimination we choose $\arg\max_{p,q}|a_{pq}^{(k-1)}|$ and perform the exchange $r_p\leftrightarrow r_k,c_q\leftrightarrow c_k$. 
## LU decomposition
Details can be found in [[Matrix Decomposition]]
Gaussian elimination has its matrix representation $A=LU$ when it's numerically stable (by partial pivoting we only need an extra exchange in rows $P$ to ensure its numerical stability, thus we always have $PA=LU$). 
- **Compact form of $LU$**: notice that the diagonal elements of $L$ are all $1$'s, thus we may represent the decomposition as $$\begin{pmatrix}u_{11}&u_{12}&u_{13}&\cdots&u_{1n}\\l_{21}&u_{22}&u_{23}&\cdots&u_{2n}\\l_{31}&l_{32}&u_{33}&\cdots&u_{3n}\\\vdots&\vdots&\vdots&\ddots&\vdots\\l_{n1}&l_{n2}&l_{n3}&\cdots&u_{nn}\end{pmatrix},\quad\begin{cases}u_{1j}=a_{1j},&j=1,\cdots,n\\l_{i1}=\frac{a_{i1}}{u_{11}},&i=2,\cdots,n\\u_{ij}=a_{ij}-\sum_{k=1}^{i-1}l_{ik}u_{kj},&i=2,\cdots,n;j=i,\cdots,n\\l_{ij}=\frac{a_{ij}-\sum_{k=1}^{j-1}l_{ik}u_{kj}}{u_{jj}},&j=2,\cdots,n-1;i=j+1,\cdots,n\end{cases}$$Note that in coding we usually denote $l_{ij}$ as $l_{ji}$ for convenience.
- **Solving equations**: now we have $LUx=b$, which is equivalent to solving $Ly=b,Ux=y$.
	- **Square root method**: for $A$ positive definite we have $A=LDL^T=GG^T$. Thus we may try to solve $$Gy=b,G^Tx=y\quad\text{or}\quad Ly=b,Dz=y,L^Tx=z$$The latter one (called **improved square root method**) is one of the most efficient and stable.
- **Approximation error and condition number**: see [[Approximation Error]]. The larger the condition number is, the more unstable the numerical algorithm for solving a corresponding linear equations is.
## QR decomposition
Details can be found in [[Matrix Decomposition]].
- Preference for QR over LU: for ill-conditioned matrices and very large matrices, QR has much higher stability, accuracy, and performance over LU.
## Iterative method
For $Ax=b$ where $A$ is sparse (only a small number of entries are non-zero) we may consider to obtain a approximate solution via iteration. In contrast with direct solution, iterative method is less expensive in calculation, and converges faster.
- **Jacobi method**: we rewrite the equation as $$x_i=\left(b_i-\sum_{j\neq i}a_{ij}x_{j}\right)\cdot a_{ii}^{-1}$$After picking an initial value $x=(x_1^{(0)},\cdots,x_n^{(0})$ we can start iteration via the formula $$x^{(k+1)}=Bx^{(k)}+g$$
- **Gauss-Seidel method**: in each iteration in the Jacobi method for calculating $x_j^{(k+1)}$ we may replace $x_1^{(k)},\cdots,x_{j-1}^{(k)}$ with $x_1^{(k+1)},\cdots,x_{j-1}^{(k+1)}$. This could help to faster convergence. Solving the modified iteration equation yields the matrix form.
**Convergence**: all iterative methods can be written as $$x^{(k+1)}=Bx^{(k)}+g$$if there exists one norm $\|\cdot\|$ s.t. $\|B\|<1$ then method would converge globally to $x^*$ with $$\|x^*-x^{(k)}\|\le\frac{\|B\|^k}{1-\|B\|}\|x^{(1)}-x^{(0)}\|$$the proof is essentially the same as [[Solving Non-linear System]]. Further we have that $$x^{(k+1)}=Bx^{(k)}+g\text{ converges}\quad\iff\quad\lim_{k\to\infty}B^k=0\quad\iff\quad\rho(B)<1\quad\Longleftarrow\quad\|B\|<1$$The first equivalence follows from $x^{(k+1)}-x^*=B^k(x^{0}-x^*)$, and the rest can be found in [[Matrix]] and [[Eigendecomposition]].
- **Choice of norm**: it's obvious that one single norm that makes $\|B\|<1$ is enough to establish convergence, while it might happen that $\|B\|_\alpha<1,\|B\|_\beta>1$. Note that existence of such $\|\cdot\|_\beta$ doesn't contradicts convergence, since it's taken as a supremum over all $\|x\|=1$ ([[Operator]]) and describes the **worst-case amplification**.