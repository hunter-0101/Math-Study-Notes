Refer to [[Hilbert Space]] for contents related to inner product space.
**Difference with [[Polynomial Interpolation]]**: for interpolation we're looking for $p$ that passes a specific set of points $\set{(x_i,y_i)}$, but we're not concerned about how $p$ behaves outside these points. For polynomial approximation we care more about a global property, that the $p$ does its best to "mimic" the behavior of the target function $f$, while the fact that the $p-f$ usually have some roots is just a by-product. This is partially illustrated by the Chebyshev polynomial below.
## Orthogonal polynomial
Some basic properties of a set of orthogonal polynomials $\set{g_n}_{n\in\mathbb N}$ where $\deg g_n=n$ on $[a,b]$: 
- **Zeros**: $g_n$ has $n$ distinct zeros in $[a,b]$, by induction.
- **Recurrence**: if $[x^n]g_n=1$ then we have that $$g_{n+1}=\left(x-\frac{\langle xg_n,g_n\rangle}{\langle g_n,g_n\rangle}\right)g_n-\frac{\langle g_n,g_n\rangle}{\langle g_{n-1},g_{n-1}\rangle} g_{n-1}$$**Proof**: for $xg_n=\sum_{k=0}^{n+1}b_kg_k$ it's obvious that $b_{n+1}=1,b_k=\frac{\langle xg_n,g_k\rangle}{\langle g_k,g_k\rangle},0\le k\le n$. Notice that $\deg g_n>\deg xg_k,\forall k\le n-2$, hence $\langle xg_n,g_k\rangle=\langle g_n,xg_k\rangle=0$. Similarly, by property of basis we have that $\langle xg_n,g_{n-1}\rangle=\langle g_n,xg_{n-1}\rangle=\langle g_n,g_n\rangle$. Simplifying this would yield the desired result.
	- **Intuition**: this is essentially making use of the expression of $xg_n$ under $g_k(1\le k\le n+1)$ to derive the coefficient. Unlike Gram-Schmidt process ([[Basis in Hilbert Space]]), space of polynomials allows us to use $g\mapsto xg$ for dimension climbing, and utilizing some other properties would yields a significantly shorter recurrence.
### Common orthogonal polynomials
- **Trigonometric polynomial**: see [[Fourier Series]].
- **Laguerre polynomial**: defined as $$L_k(x)=e^x\frac{d^k}{dx^k}(x^ke^{-x}),\quad k\in\mathbb N$$The recurrence is $L_{n+1}=(1+2k-x)L_k-k^2L_{k-1}$.
- **Hermite polynomial**: defined as $$H_k(x)=(-1)^ke^{x^2}\frac{d^k}{dx^k}e^{-x^2},\quad k\in\mathbb N$$The recurrence is $H_{k+1}=2xH_k-2k H_{k-1}$.
- [[Chebyshev Polynomial]].
## Least square approximation
Given $f$ defined on $[a,b]$ and a set of basis polynomial $\set{\phi_n}$, we call the solution to optimization problem $\arg\min_p\|p-f\|_2^2$ where $p=\sum c_n\phi_n$ the **least square approximation**.
- **Normal equation**: by linearity of inner product we can simplify the error to $$\|p-f\|_2^2=\langle p-f,p-f\rangle=\sum_{i,j}c_ic_j\langle\phi_i,\phi_j\rangle-2\sum_i c_i\langle\phi_i,f\rangle+\langle f,f\rangle$$since the solution should satisfy $\frac{\partial \|p-f\|_2^2}{\partial c_k}=0$, we get the **normal equations** $$\sum_{j=0}^n c_j\langle\phi_k,\phi_j\rangle=\langle\phi_k,f\rangle,\quad k=0,1,\cdots, n$$solving this equation yields the optimal coefficients for least square approximation.
	- **Discrete form**: for list function $\set{(x_i,y_i)}_{1\le i\le n}$ and basis $\set{\phi_j}_{1\le j\le m}$ let $G=[\phi_j(x_i)]_{m\times n}$, then the above equation can be written as $G^TGc=G^Ty$. Its analytic solution is discussed in [[Singular Value Decomposition]]. Notice that if we take $\phi_j=x^j$, then the above can be simplified into $$\begin{bmatrix}\sum_{i=1}^m1&\sum_{i=1}^mx_i&\cdots&\sum_{i=1}^mx_i^n\\\sum_{i=1}^mx_i&\sum_{i=1}^mx_i^2&\cdots&\sum_{i=1}^mx_i^{n+1}\\\vdots&\vdots&&\vdots\\\sum_{i=1}^mx_i^n&\sum_{i=1}^mx_i^{n+1}&\cdots&\sum_{i=1}^mx_i^{2n}\end{bmatrix}\begin{bmatrix}c_0\\c_1\\\vdots\\c_n\end{bmatrix}=\begin{bmatrix}\sum_{i=1}^my_i\\\sum_{i=1}^mx_iy_i\\\vdots\\\sum_{i=1}^mx_i^ny_i\end{bmatrix}$$In this case the least square approximation polynomial is exactly the [[Polynomial Interpolation]].
	- **Intuition**: given an over-determined system $Ax=b,A\in\mathbb C^{m\times n},m>n$ we try to find $x$ s.t. $\|Ax-b\|$ is minimal. This requires $\frac{d}{dx}\braket{Ax-b,Ax-b}=0$, which can be simplified to $A^TAx=A^Tb$. [[Differential Calculus on Euclidean Space]]
- **Projection**: if $\set{\phi_i}$ forms an orthogonal basis then the normal equation above reduces to $$c_k\braket{\phi_k,\phi_k}=\braket{\phi_k,f},\quad\forall k$$This is essentially the projection equation. The fact that it's a least square approximation is guaranteed by the Parseval's inequality. [[Basis in Hilbert Space]]
## Minimax approximation
The minimax approximation polynomial is the solution to the optimization problem $$p^*_n=\underset{\deg p=n}{\arg\min}\|f-p\|_\infty$$We call $p^*_n$ the minimax polynomial, and $E(x)=p(x)-f(x)$ the **error/oscillation** of $p$ at $x$. The following theorems establish some global descriptions of the approximation.
- **Chebyshev's equi-oscillation theorem**: for $f\in C([a,b])$ and $\deg p\le n$, $p=p^*_n$ iff there exists $n+2$ points $a\le x_0\le\cdots\le x_{n+1}\le b$ such that $$f(x_i)-p(x_i)=\sigma(-1)^i\|f-p\|_\infty$$where $\sigma=\pm1$. We call $\set{x_k}$ an **alternating set**.
  **Proof**: $\Leftarrow$ is obvious from the lemma below; for $\Rightarrow$ assume that $p$ has alternating set $\set{x_i}_1^k$ with $1\le k\le n+1,E(x_1)=-E$. Now pick the zeros of $E(x)$, i.e. $y_i\in[x_i,x_{i+1}]$ s.t. $$0<E(x)\le E,\forall x\in[y_{2i-1},y_{2i}];\quad0>E(x)\ge -E,\forall x\in[y_{2i},y_{2i+1}]$$and define $w(x)=(y_1-x)\cdots(y_{k-1}-x)$, then $p'=p+\varepsilon w$ for $\varepsilon>0$ small enough is a better approximation since $\sgn E$ and $\sgn w$ are always different.
	- **De La Vall√©e Poussin's lemma**: if $\deg p\le n$ has a non-uniform alternating set of length $n+2$ w.r.t $f$, denote the oscillations as $e_i=p(x_i)-f(x_i)$, then $\|f-p^*\|_\infty\ge\min|e_i|$.
	  **Proof**: assume that $\|f-p^*\|_\infty<\min|e_i|$. Consider $r=p^*-p=(f-p)-(f-p^*)$. By assumption $\sgn r(x_i)=\sgn(f(x_i)-p(x_i))$, hence $r$ changes sign $n+1$ times, i.e., has at least $n+1$ roots, but $\deg r\le n$, which is a contradiction.
	- **Intuition**: talking about alternating set of length $n+2$ for $\deg p\le n$ is not paradoxical because we're not requiring $p$ to interpolate these points, but rather requiring $p-f$ to reach a specific extrema at these points. This is the worst case that some arbitrary $f$ can deviate from $\deg p\le n$: such polynomial can change monotonicity at most $n+1$ times, but the number might be more than that for $f$. An intuitive example is that $y=0.5$ is the minimax polynomial of degree less than $2$ for $\sin x$ on $[0,\pi]$.
- **Uniqueness**: $p^*$ given $\deg p^*\le n$ is unique.
  **Proof**: assume that $p_1^*,p_2^*$ are both minimax with error $E$, then by triangle inequality we have $q=\frac{1}{2}(p_1^*+p_2^*)$ also satisfies $\|f-q\|\le E$. By equi-oscillation theorem there exists alternating set $\set{x_k}$ for $q$, and by the inequality at $\set{x_k}$ both $p_1^*,p_2^*$ should also reaches their local extremum, i.e., $\set{x_k}$ is also the alternating set for $p_1^*,p_2^*$. Since $\deg p\le n$ and $\set{x_k}$ determines $n+2$ points, we must have $p_1^*=p_2^*$.
- **Calculation**: given that $f\in C^{n+1},f^{(n+1)}$ doesn't change sign, then pick $x_0=a,x_{n+1}=b$, and the following equations: $$\begin{cases}f(x_i)-p^*(x_i)=(-1)^i\mu&i=0,1,\cdots,n+1\\\ f'(x_i)-p'(x_i)=0&i=1,2,\cdots,n\end{cases}$$There are $n+2$ equations, corresponding to $n+2$ unknowns ($\set{c_i}_{i=0}^n,\set{x_i}_{i=1}^n,\mu$), hence solvable. However it's usually exceptionally expensive in computation to solve the system ([[Solving Non-linear System]]), hence we usually consider the following alternative.
	- **Intuition**: as described above, the fact that $f$ might change monotonicity more than $n+1$ times prevents $p$ from perfectly fitting $f$, but given that $f^{(n+1)}$ doesn't change sign we ensure that the number is exactly $n+1$. Such well-behaved-ness enables us to calculate the coefficients in an intuitive way. For an intuitive example given $x^2$ on $[0,1]$ it's obvious that the best linear regression takes maximal error at ends points and a middle point, but for $x^3$ we'll have to be careful about the actually shape and interval.
- **[[Chebyshev Polynomial]]**: in practice we usually use Chebyshev interpolation as a quasi-minimax polynomial. More can be found in the linked note.