For a probability space $(\Omega,\Sigma,P)$, the expectation of $X$ is defined as the [[Lebesgue Integral]] $$EX=\int_{\Omega_X} XdP_X$$where the footnotes are used to emphasize that both should be taken w.r.t the $X$ to be summed toward (see discussion below). 
- **Discussion**: instead of a norm (as in [[Lp Space]]) expectation is essentially a functional ([[Topological Dual Space]]) on the space of random variables. It's a statistical characteristic that describes the overall value distribution of $X$, and nothing more.
	- **A critical point on integral domain**: it might be temping to think that $E(XY)=\int_\Omega XYdP$, but it's obviously not true, e.g., $X=Y\sim N(0,1)$. The key is that, expectation sums over the **probability space corresponding to the RV**, rather than $\Omega$ itself. The correct way to interpret $E(XY)$ is to first let $Z=XY$, and write $$E(XY)=EZ=\int_{\Omega_Z}ZdP_Z=\int_{\Omega^2}ZdP_Z$$To summarize, when dealing with integral of RVs we must first figure out what is its $\sigma$-algebra of definition.
- **Linearity**: by linearity of [[Lebesgue Integral]] we have that $$E\left(\sum a_iX_i\right)=\sum a_iEX_i$$this identity holds for whenever $X_i$ are independent or not.
	- If $X_n\ge0$ then by MCT we get $E\Big[\sum_{i=1}^\infty X_i\Big]=\sum_{i=1}^\infty E(X_i)$. However this is generally not true.
- **Expectation of function of RV**: given $X$ an RV and $g$ any function, we usually write $$Eg(X)=\int_{\Omega}g(x)f(x)dx$$However there should be a **warning** that strictly speaking, we have to treat $g(X)$ itself as an RV and first calculate its PDF, and then its expectation from definition. While the above equation is true for many common cases (e.g., when $g$ is monotonic & differentiable), it might break down in general case, such as for complex-valued RV for random vectors.
	- **Proof for monotonic $g$**: this is a simple application of the change of variable formula ([[Lebesgue Integral]]).
- **Expectation of positive RV**: an important identity is $$E(X)=\int_0^\infty P(X>t)dt$$This is usually used to simplify an integral involving expectation.
- **Maximum-Minimum identity**: for arbitrary numbers $x_i,i=1,\dots,n$, we have $$\max_i x_i=\sum_ix_i-\sum_{i<j}\min(x_i,x_j)+\sum_{i<j<k}\min(x_i,x_j,x_k)+\dots+(-1)^{n+1}\min(x_1,\dots,x_n)$$This can be proven probabilistically using inclusion-exclusion identity. Taking expectation of both sides yields$$E\Big[\max_iX_i\Big]=\sum_iE[X_i]-\sum_{i<j}E[\min(X_i,X_j)]+\dots+(-1)^{n+1}E[\min(X_1,\dots,X_n)]$$
	- When $X_i$ are i.i.d. random variables we can also use $E(\max)=\int P(\max>t)dt$. The same applies to minimum.
- **Number of events that occur**: for a series of events $X_i$, let $X$ denotes the number of events that occur, then $X=\sum I_i$ in which $I_i$ is the **indicator variable**. Thus $$E(X)=\sum E(I_i)=\sum P(A_i)$$When considering the number of pair of events that occur, denoted $X^{(2)}$, we know that $$E(X^{(2)})=E(C_X^2)=\sum_{i<j}E(I_iI_j)=\sum_{i<j}P(A_iA_j)$$The same applies to cases of k events.
## Expectation inequality
Generally inequalities involving expectation is obtained by first establish $f(x)\le g(x)$ and then directly take $Ef\le Eg$.
- For a essentially bounded $X$ with $EX=0$ show that $Ee^X\le\cosh\|X\|_\infty$.
  **Proof 1**: as $e^x$ is convex consider the line connecting $(-M,e^{-M}),(M,e^M)$ we have $$e^x\le\cosh M+\frac{\sinh M}{M}x$$Now let $M=\|X\|_\infty$ and take expectation for both sides would yields the result.
  **Proof 2**: now that the expectation exists, we can make use of integration by parts to get $$\int_{-M}^MF(x)dx=M,\quad F(-M)=0,F(M)=1$$where $F(x)$ is the CDF. The desired inequality can be transformed into $$\int_{-M}^Me^xF(x)dx\ge\frac{e^M-e^{-M}}{2}$$which obviously holds with identity at $F(-M)=0,F(M)=1,F(x)=\frac{1}{2},x\in(-M,M)$. By taking a linear combination of $F(x)$ and the identity condition we can establish the inequality.
- **Chebyshev inequality**: For monotonic $g,h:\mathbb R\to\mathbb R$ of the same monotonicity we have $$\cov(g(X),h(X))=E(g(X)h(X))-Eg(X)\cdot Eh(X)\ge0$$with identity iff at least one of $g,h$ is $\ae$ constant. For $g,h$ with opposite monotonicity the inequality is also reverted. To proof this we **take a copy** $Y\sim X$ and notice that $$\cov(g(X),h(X))=\frac{1}{2}E(g(X)-g(Y))(h(X)-h(Y))$$by monotonicity the result is obvious. This proof mimics the trick for proofing the integral inequality.
	- Another proof is that, as the inequality is invariant under vertical translation of $g,h$, we could assume that $Eg(X)=0$ and $g,h$ share the same zero. Now we can directly get the result from above expression of covariance. The newly introduced $Y$ essentially take expectation across all possible translations.
## Covariance & correlations
The **covariance** of $X,Y$ is a bilinear form defined by $$\cov(X,Y)=E[(X-EX)(Y-EY)]$$Based on this we can further deduce that $\cov(X,Y)=E(XY)-EX\cdot EY$. We defined the variance as $\cov(X,X)=\var(X)$.
- **Intuition**: covariance measures the **linear relations** between two RVs. Consider $\set{X_i}$ a set of independent RVs, then by linearity (below) $$\cov\left(\sum_{i=1}^n a_iX_i,\sum_{i=1}^n b_jX_j\right)=\sum_{i,j}a_ib_j\cov(X_i,X_j)=\sum_{i=1}^na_ib_i\var(X_i)$$Intuitively, only linear correlations are "filtered" out from the operator, while other information (such as quadratic correlations) are directly abandoned.
- **Relation with independent**: given $X,Y$ independent we have $\cov(X,Y)=0$ by property of expectation above, but the converse is not true. This is natural since covariance is just **one number** calculated from two RVs, while independence requires a huge amount of equations to be satisfied.
- **Arithmetic properties**
	- **Symmetry**: $\cov(X,Y)=\cov(Y,X)$.
	- **Bilinearity**: for given any linear combinations of $X_i,Y_i$ we have $$\cov\left(\sum_{i=1}^n a_iX_i,\sum_{j=1}^m b_jY_j\right)=\sum_{i=1}^n\sum_{j=1}^ma_ib_j\cov(X_i,Y_j)$$which can be verified directly from definition, which can be used to deduce the formula for $\var\left(\sum_{i=1}^nX_i\right)$.
- **Correlation**: for $X,Y$, as long as $\var(X), \var(Y)>0$, we define $$\rho(X,Y)=\frac{\cov(X,Y)}{\sqrt{\var(X)\var(Y)}}=\frac{\cov(X,Y)}{\sigma_X\sigma_Y}=\cov\left(\frac{X}{\sigma_X},\frac{Y}{\sigma_Y}\right)$$That is, correlation is a standardized (i.e., scale-invariant) covariance that only extracts the information on relative directions.
	- **Intuition**: for $Y=aX+\varepsilon$, we can calculate that $$\rho(X,Y)=\frac{a\sigma_X}{\sqrt{a^2\sigma_X^2+\sigma_\varepsilon^2}}$$so for $\sigma_\varepsilon=0$ the error term is a constant, making $X,Y$ purely linearly related, while large $\varepsilon$ will "blur" the linear relations.
	- $-1\le\rho(X,Y)\le1$, proven **algebraically usingÂ Cauchy-Schwarz inequality**, or use $$\var\left(\frac{X}{\sigma_X}+\frac{Y}{\sigma_Y}\right)=2(1+\rho(X,Y)),\quad \var\left(\frac{X}{\sigma_X}-\frac{Y}{\sigma_Y}\right)=2(1-\rho(X,Y))$$