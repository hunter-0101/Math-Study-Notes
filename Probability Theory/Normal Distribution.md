The PDF of **Normal (Gaussian) distribution** with expectation $\mu$ and standard deviation $\sigma$ is $$f(x)=\frac{1}{\sigma\sqrt{2\pi}}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)$$For $\mu=0,\sigma=1$ we have the **standard normal distribution**: $$\Phi(x)=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^ae^{-\frac{y^2}{2}}dy$$Any normal distribution with parameters $\mu$ and $\sigma^2$ can be written as $$F_X(a)=\Phi\left(\frac{a-\mu}{\sigma}\right)$$which is usually denoted as $N(\mu,\sigma^2)$. For normal random vectors, we write $$X\sim N(\mu,\Sigma)\quad\iff\quad X_i\sim N(\mu_i,\Sigma_{ii}),\quad\cov(X_i,X_j)=\Sigma_{ij}$$
- **MGF & CF**: the moment generating function of $X\sim N(\mu,\sigma^2)$ is given by $$M_X(t)=\exp\left(\mu t+\frac{\sigma^2t^2}{2}\right)$$and the characteristic function is given by $$\varphi_X(t)=\exp\left(i\mu t-\frac{\sigma^2t^2}{2}\right)$$This is can derived directly from basic calculation.
- **Sum of independent normal RV**: for pairwise independent RVs $X_k\sim N(\mu_k,\sigma_k^2)$ we have $$Z=\sum_{k=1}^n X_k\sim N\left(\sum_{k=1}^n\mu_i,\sum_{k=1}^n\sigma_i^2\right)$$**Proof**: by [[Distribution Function]] the PDF of $Z$ is given by $$f_Z(z)=(f_{X_1}*\cdots* f_{X_n})(z)=\frac{1}{\sqrt{\sum_{k=1}^n\sigma_k^2}\cdot\sqrt{2\pi}}\exp\left(\frac{x-\sum_{k=1}^n\mu_k}{2\sum_{k=1}^n\sigma_k^2}\right)$$hence $Z\sim N\left(\sum_{k=1}^n\mu_i,\sum_{k=1}^n\sigma_i^2\right)$ by definition. 
	- **Proof via characteristic function**: by [[FT on L1]] it suffices to verify that $Z$ has the same characteristic function as a normal variable with above mean and variance. By property of CF ([[Random Variable]]) we have $$\varphi_Z(t)=\prod_{k=1}^n\varphi_{X_k}(t)=\exp\left(it\sum_{k=1}^n\mu_k-\frac{t^2}{2}\sum_{k=1}^n\sigma_k^2\right)$$which completes the proof. Actually we can also go with MGF.
	- **Proof via FT**: this is basically a combination with the above two proofs. We still start with convolution, but simplify the calculation via the convolution theorem ([[FT on L1]]).
	- **Intuition**: it's obvious that distribution of $X+Y$ is rotationally invariant. By the fact that multivariate normal RV is the only RV having this property, in instantly know that their sum is still a normal RV.
- **Demoivre-Laplace theorem**: if $S_n$ denotes the number of successes that occur in $B(n,p)$, then $$P\left(a\le\frac{S_n-np}{\sqrt{np(q-p)}}\le b\right)\to\Phi(b)-\Phi(a),\quad n\to\infty$$This is a special case of the [[Central Limit Theorem]]. To proof it for cases where $g(x)\ge0$, we could make use of another important equality that when $Y\ge0$, we have $$EY=\int_0^\infty P(Y>y)dy $$See [[Expectation]].
- real-life example: flip 8 coins of $0$ and $1$ and calculate their sum. The distribution of the sum tends to normal distribution.
- **log-normal distribution**: if random variable $X$ is log-normally distributed, then $Y=\ln(X)$ is normally distributed.
	- real-life example: neuron firing frequency distribution in human brain.
- **Bivariate normal distribution**: for $X,Y$, their bivariate normal distribution is $$f(x,y)=\frac{1}{2\pi\sigma_x\sigma_y\sqrt{1-\rho^2}}\exp\Big\{-\frac{1}{2(1-\rho^2)}\Big[\big(\frac{x-\mu_x}{\sigma_x}\big)^2+\big(\frac{y-\mu_y}{\sigma_y}\big)^2-2\rho\frac{(x-\mu_x)(y-\mu_y)}{\sigma_x\sigma_y}\Big]\Big\}$$We denote this distribution as $N(\mu_x,\mu_y,\sigma_x^2,\sigma_y^2,\rho)$.
	- Here $\rho=\cov(x,y)=\frac{V_{xy}}{\sigma_x\sigma_y}$ is the covariance. [[Expectation]]
	- **Marginal distribution**: it can be easily shown that the $N_X=N(\mu_x,\sigma_x^2)$.
	- Assume that $X=(X_1,\cdots,X_m)^T,Z=(Z_1,\cdots,Z_n)^T,\mu=(\mu_1,\cdots,\mu_m)^T$, where $Z$ are standard normal random variables, and $X=AZ+\mu$ for some constants $A=(a_{ij})_{mn}$, then $X$ are said to have a **multivariate normal distribution**.
		- In this case we have $E(X_i)=\mu_i,Var(X_i)=\sum_{j=1}^na_{ij}^2$.
## $\chi^2$ distribution
An RV that follows chi-squared distribution is defined as $$X\sim\chi^2_k\quad\iff\quad X=\sum_{i=1}^kZ_i^2\quad\text{where}\quad Z_i\sim N(0,1)$$Here $k$ is called the **degree of freedom**. By direct computation Its CDF is given by $$F(x;k)=\frac{\gamma(k/2,x/2)}{\Gamma(k/2)}$$where $\gamma$ is the lower incomplete [[Gamma Function]], and by differentiation we get its PDF $$f(x;k)=\frac{x^{k/2-1}e^{-x/2}}{2^{k/2}\Gamma(k/2)},\quad x>0$$
- **Motivation**: by [[Estimator]] the unbiased estimator of sample variance $x_i\sim N(\mu,\sigma^2)$ is $$\hat\sigma^2=\frac{1}{n-1}\sum_{i=1}^n(x_i-\overline x)^2\quad\Longrightarrow\quad\frac{(n-1)\hat\sigma^2}{\sigma^2}\sim\chi_{n-1}^2$$so $\chi^2$ law is essentially the law of how variance itself fluctuates. It measures the discrepancy between observed and expected counts as squared deviations, and then ask, "is this discrepancy too large to be explained by chance?" The benchmark is $\chi^2$.
- **Expectation & variance**: since $Z_i\sim N(0,1)$, we have $$EX=\sum_{i=1}^kEZ_i^2=k,\quad \var X=2k$$
- **Representation by Gamma distribution**: by directly compare the PDF we can notice that $$\chi_k^2\sim\Gamma\left(\frac{k}{2},\frac{1}{2}\right)$$hence properties of Gamma distribution like **additivity** also applies here. 
## $F$-distribution
The $F$-distribution (Fisherâ€“Snedecor distribution) is defined as $$X\sim F(d_1,d_2)\quad\iff\quad X=\frac{U_1/d_1}{U_2/d_2}\quad\text{where}\quad U_1\sim\chi^2_{d_1},U_2\sim \chi_{d_2}^2$$with $d_1,d_2$ the degree of freedom. By direct computation Its CDF is given by $$F(x)=I_{\frac{d_1x}{d_1x+d_2}}\left(\frac{d_1}{2},\frac{d_2}{2}\right)$$where $I(\cdot)$ is the regularized incomplete beta function ([[Gamma Function]]). Its PDF is $$f(x)=\frac{\Gamma\left(\dfrac{m+n}{2}\right)\left(\dfrac{m}{n}\right)^{m/2}}{\Gamma\left(\dfrac{m}{2}\right)\Gamma\left(\dfrac{n}{2}\right)}x^{\frac{m}{2}-1}\left(1+\frac{m}{n}x\right)^{-\frac{m+n}{2}}$$
- **Motivation**: often, we want to compare **two sources to variability**, e.g., 
  1. Example: are two population variances equal?
  2. Example: in regression, is the explained variance (by model) large relative to the unexplained variance (residuals)? 
  Essentially, $F$ measures **relative strength of two variances**. It's the language of comparing models or groups.
## $t$-distribution
The $t$-distribution is defined as $$X\sim t_\nu\quad\iff\quad X=\frac{Z}{\sqrt{U/\nu}}\quad\text{where}\quad Z\sim N(0,1), U\sim\chi_\nu^2$$with $\nu$ the degree of freedom. By direct computation Its PDF is given by $$f(x)=\frac{\Gamma\left(\frac{\nu+1}{2}\right)}{\sqrt{\pi\nu}\Gamma\left(\frac{\nu}{2}\right)}\left(1+\frac{t^2}{\nu}\right)^{-(\nu+1)/2}=\frac{1}{\sqrt\nu B\left(\frac{1}{2},\frac{\nu}{2}\right)}\left(1+\frac{t^2}{\nu}\right)^{-(\nu+1)/2}$$Its CDF is quite complicated and not commonly used, hence we neglect it here.
- **Motivation**: when we're testing whether a mean $\mu_2$ is equal to some hypothesized value, for $X_i\sim N(\mu,\sigma^2),1\le i\le n$ we'd naturally form $$\frac{\overline x-\mu_0}{\sigma/\sqrt n}$$when $\sigma$ is known the above is just $N(0,1)$, but in practice we must replace it with $\hat\sigma$, which introduces extra randomness, whose distribution is governed by $\chi^2$. Intuitively, it measures **how far a sample mean is from a hypothesized mean, relative to its estimated noise**.
 