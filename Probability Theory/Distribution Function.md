For specific distributions, see [[Distribution Function Gallery]]. The distribution functions below are specified by [[Random Variable]] except for those explicitly stated.
# DF of a single RV
A **(cumulative) distribution function (CDF)** of on a probability space $(\Omega,\Sigma,P)$ induced by RV $X:\Omega\to\mathbb R$ is a function $F:\overline{\mathbb R}\to[0,1]$ defined by $$F_X(x)=P(\set{\omega\in\Omega:X(\omega)\le x})$$Any $F_X$ defined in this way satisfies the following three characteristics:
1. **Monotonicity**: $F$ is non-decreasing.
2. **Right-continuity**: $F(x^+)=F(x)$.
	- If we modify $\le$ to $<$ in definition then $F$ is left continuous. This depends on your preference.
3. **Regularity**: $\lim\limits_{b\to\infty}F(b)=1,\lim\limits_{b\to-\infty}F(b)=0$. 
Conversely, any function $F(x)$ satisfying them also uniquely determines a random variables by $X=F^{-1}(Y)$ where $Y\sim U(0,1)$ up to $\ae$ equality. [[Measure]]
- **Decomposition of CDF**: by integrating both sides in refined Lebesgue decomposition theorem we have $$F(x)=F_d(x)+F_{ac}(x)+F_s(x)$$as a decomposition into its discrete, absolutely continuous, and singular parts. 
- **PDF**: the PDF w.r.t $\mu$ is defined as the R-N derivative [[Signed Measure]] $$f=\frac{dX_*P}{d\mu}\quad\iff\quad P(X\in A)=\int_{X^{-1}A}dP=\int_Afd\mu$$Mostly we'll take $\mu=\lambda$.
## Distribution of function of RV
Below we list some useful formulas concerning the distribution of common functions of RV.
- **Distribution of function of RV**: for continuous $X$ and strictly monotonic, differentiable $f_x,g(x)$, let $Y=g(X)$, then $$f_Y(y)=\begin{cases}f_X(g^{-1}(y))\left|\frac{d}{dy}g^{-1}(y)\right|,&y=g(x)\text{ for some }x\\0,&y\neq g(x)\text{ for all }x \end{cases}$$This is derived via chain rule ([[Differential Calculus on Euclidean Space]]), by differentiating both sides of the expression $F_Y(y)=F_X(g^{-1}(y))$.
	- **Distribution of inverse of RV**: as an corollary, for additive inverse we have $$F_{-X}(x)=P(X\ge -x)=1-F_X(-x),\quad f_{-X}(x)=f_X(-x),$$and given that $X>0$, for multiplicative inverse we have $$F_{X^{-1}}(x)=P(X\ge x^{-1})=1-F_X(x^{-1}),\quad f_{-X}(x)=x^{-2}f_X(x^{-1})$$Utilizing these two results we can calculate the distribution of most of the commonly used RVs constructed via basic arithmetics.
	- **Non-bijective $g$**: when the function $g$ is not bijective then more care is required. For example, if we have $Y=X^2$, then $$F_Y(y)=P(Y\le y)=P(-\sqrt y\le X\le\sqrt y)=F_X(\sqrt y)-F_X(-\sqrt y)$$hence upon differentiation we get the corresponding expression with an extra term.
- **Distribution of sum of RVs**: given $X,Y$ independent, denote $Z=X+Y$, then $$F_Z(z)=P(X+Y\le z)=\int_\mathbb Rdy\int_{-\infty}^{a-y}f_X(x)f_Y(y)dx=(F_X*f_Y)(a)=(f_X*F_Y)(a)$$and by differentiating both sides and utilizing [[Convolution]] we have $$f_Z(z)=(f_X*f_Y)(z)$$If we're given $Z=\sum_{i=1}^nX_i$ where $X_i$ are pairwise independent, then by induction we have $$F_Z(z)=(f_{X_1}*\cdots*f_{X_{i-1}}*F_{X_i}*f_{X_{i+1}}*\cdots*f_{X_n})(z),\quad f_Z(z)=(f_{X_1}*\cdots*f_{X_n})(z)$$this is well-defined due to associativity and commutativity of convolution.
- **Distribution of product of RVs**: given $X,Y$ independent, denote $Z=XY$, then $$\begin{align}F_Z(z)&=P(XY\le z,X\ge0)+P(XY\le z,X\le0)\\&=P(Y\le z/X,X\ge0)+P(Y\ge z/X,X\le0)\\&=\int_0^\infty f_X(x)\int_{-\infty}^{z/x}f_Y(y)dydx+\int_{-\infty}^0f_X(x)\int_{z/x}^\infty f_Y(y)dydx\end{align}$$by differentiation both sides and using [[Differentiation Under the Integral Sign]] we have $$f_Z(z)=\int_\mathbb R f_X(x)f_Y(z/x)\frac{1}{|x|}dx$$Unfortunately there is not simple, explicit expression for product of $n$ independent RVs.
The above several results, combined together, basically yields most of the commonly used constructions from basic RVs. For special functions like exponential or logarithm of RV, we only need to directly apply the definition and utilize the above results accordingly.

---
## DF of multivariate RV
For $X=(X_1,\cdots,X_n)$ where each $X_k$ is an RV, define its **joint CDF** as $$F_X(x)=P(X_1\le x_1,\cdots, X_n\le x_n),\quad\forall x\in\mathbb R^n$$and the corresponding **joint PDF** as $$f_{X}(x)=\frac{\partial^n}{\partial x_1\cdots\partial x_n}F_X(x)$$Most of the time [[Differential Calculus on Euclidean Space]] under [[Riemann Integral]] suffices for the above calculation.
- **Limit behavior**: obviously $F_X$ is non-decreasing and right-continuous in each variable, and $$\lim_{x\to-\infty}F_X(x)=0,\quad\lim_{x\to\infty}F_X(x)=1$$Note that when some coordinates of $x$ tends to $\infty$ while others fixed at some finite value, then the limit may not be $1$, since $f_X$ might have some support on the other half of the space.

- **Qualification for CDF**: for $F:\Omega^n\to\mathbb R$ to be the CDF we additionally require $$\Delta_a^bF=\sum\sgn(x)F(x)=P(X\in(a,b])\ge0,\quad\forall a,b\in\Omega^n$$where $\sgn(x)$ takes $+1$ when $2\mid\#\set{i:x_i=a_i}$ and $-1$ otherwise.
- **Marginal distribution**: given an index set $I\subset[n]$, the marginal distribution is defined as $$F_{X_I}(x_I)=P(X_I\le x_I)=F_X(x_I,\infty_{[n]\backslash I})$$Obviously marginal CDF is also CDF.
	- **Uniqueness**: the marginal distribution given $I$ is uniquely determined by $F_X$. However the converse is not true, say in [[Normal Distribution]] the multivariate normal distribution is affected by $r$ although all its marginal distributions are $N(0,1)$.
- **Conditional distribution**: by definition ([[Conditional Probability]]) we have $$P(X\le x|Y\le y)=\frac{F_{X,Y}(x,y)}{F_Y(y)}$$Similarly we can define other kinds of conditional distributions.
	- **Conditional PDF**: given $f_Y>0$ we have $$f_{X|Y}(x|y)=\frac{f_{X,Y}(x,y)}{f_Y(y)}$$We can show this by taking $\Delta y\to0$ in conditional CDF.
- **Jointly continuous**: $\exists f,\forall C\subset\mathbb{R}^2$, $$P((X,Y)\in C)=\iint\limits_{(x,y)\in C}f(x,y)dxdy,\quad f(a,b)=\frac{\partial^2}{\partial a\partial b}F(a,b)$$The above identity can be written in the integral form:$$f_X(x)=\int_{-\infty}^\infty f(x,y)dy,\quad f_Y(y)=\int_{-\infty}^\infty f(x,y)dx$$
- **Maxwell's theorem**: if the PDF of random vector $X:\mathbb R^n\to\mathbb R^n$ is invariant under rotation, i.e., $f_X=f_{GX}$ for any orthonormal $G\in \mathbb R^{n\times n}$, and each components are independent, then each $X_i$ are normally distributed with zero mean and the same variance.
  **Proof**: WLOG assume that $n=2$. Since rotation by $90\degree$ preserves the PDF, all $X_i$ have the same probability measure $\mu$. By Lebesgue decomposition theorem ([[Signed Measure]]) we have $\mu=\mu_r+\mu_s$, its regular and atomic parts. 
  Obviously $\mu_s=0$ otherwise by rotational invariance $X$ contains an infinite set of disjoint sets of constant measure, which adds up to infinite, a contradiction.
  Thus let $\rho=dX_*\mu/dx$ be the PDF induced by $\mu$, it remains to solve the function equation $$\rho(x)\rho(y)=\rho(x\cos\theta+y\sin\theta)\rho(x\sin\theta-y\cos\theta),\quad\forall x,y,\theta$$$\rho$ cannot have any zero, thus it's same to assume $\rho=e^{\omega(x)}$. Taking logarithm for both sides and with method of difference we can show that $\omega(x)=-Cx^2$ for some positive $C$.
## Distribution of function of multivariate RV
Given $X=(X_1,\cdots,X_2)$, denote $Y=(Y_1,\cdots,Y_n)$ defined by $$Y=G(X):\quad Y_j=g_j(X_1,\cdots,X_n),\quad1\le k\le n$$assume that the inverse functions $H=G^{-1}$ exists, expressed by $$X=H(Y):\quad X_i=h_i(Y_1,\cdots,Y_n),\quad 1\le i\le n$$then the distribution of $Y$ can be expressed with that of $X$ by $$f_Y(y)=\left|\det J_H\right|f_X(H(y))$$**Proof**: for any $B\subset\Omega_Y$ by definition we have $$P(Y\in B)=\int_Bf_Y(y)dy$$By invertibility we have the equivalence $$\set{Y\in B}\quad\iff\quad\set{G(X)\in B}\quad\iff\quad\set{X\in H(B)}$$hence by change of variable formula ([[Integral Calculus on Euclidean Space]]) we have $$\int_Bf_Y(y)dy=\int_{H(B)}f_X(x)dx=\int_B\left|\det J_H\right|f_X(H(y))dy$$by arbitrariness of $B$ we know that ([[Lebesgue Integral]]) $$f_Y(y)=\left|\det J_H\right|f_X(H(y))$$This equality holds for all $y\in\Omega_Y$, hence completing the proof.
- **Linear transformation**: if $G\in\gl_n(\mathbb R)$ with $H=G^{-1}$ then we have $$f_Y(y)=\left|\det H\right|f_X(H(y))$$Notice that when $X$ has pairwise independent components, by [[Random Variable]] we have $$f_Y(y)=\left|\det H\right|f_X(H(y))=\left|\det H\right|\prod_{i=1}^nf_{X_i}(y_i)$$hence by properly decomposing $\det H$ we know that $Y$ also has pairwise independent components, with exactly the same marginal distribution up to a constant multiplier.

---
## Tail behavior
A distribution $f$ of an RV $X$ is said to have a **heavy tail** if $M_X(t)=\infty,\forall t>0$. Intuitively it's essentially saying that $f$ decays slower than exponential function. 
- **Long tail**: a distribution is said to have a long tail if $$\lim_{x\to\infty}P(X>x+t|X>t)=1,\quad\forall t>0$$it can be easily verified that all long tailed distributions are heavy tailed, but not vise versa.