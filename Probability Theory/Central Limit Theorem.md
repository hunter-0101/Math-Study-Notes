- **Markov's inequality**: for RV $X\ge0$ and any $a>0$, we have $$P(X\ge a)\le\frac{E(X)}{a}$$which relies on a simple estimation. 
- **Chebyshev's inequality**: for RV $X$ with $\mu,\sigma^2<\infty$ we have $$P(|X-\mu|\ge k)\le\frac{\sigma^2}{k^2},\quad\forall k>0$$This is proven by apply Markov's inequality on $(X-\mu)^2$. 
	- **General version**: given $g$ non-decreasing and non-negative on $[0,\infty)$ we have $$P(|X|\ge k)\le\frac{Eg(|X|)}{g(k)}$$which follows from a simple estimation similar to that in Markov's inequality.
	- **One-sided Chebyshev inequality**: for RV $X$ with mean $0$ and variance $\sigma^2$, given $a>0$,$$P(X\ge a)\le\frac{\sigma^2}{\sigma^2+a^2}$$which is proven by Markov inequality, applied on $(X+b)^2\ge(a+b)^2$ and let $b=\sigma^2/a$.
- **Chernoff bound**: apply Markov's inequality to $e^{tX}(t>0)$ we get $$P(X\ge a)\le M(t)e^{-ta}$$as the inequality holds for all $t>0$ we may take an infimum $P(X\ge a)\le\inf_{t>0}M(t)e^{-ta}$. Similarly for $t<0$ we get a bound on left tail $P(X\le a)\le M(t)e^{-ta}$.
## Law of large number
- **Weak LLN**: let $X_1,X_2,\cdots$ be a sequence of i.i.d. random variables with $E(X_i)=\mu$. For any $\varepsilon>0$ we have the following $P$-convergence $$\frac{X_1+\cdots+X_n}{n}\xto{P}\mu\quad\text{as}\quad n\to\infty$$As a direct result of Chebyshev's inequality, as $\var(\overline X_n)=\frac{1}{n^2}\var(\sum X_n)=\frac{\sigma^2}{n}\to0$. We can also make use of characteristic function with Lévy continuity theorem. [[Random Variable]]
	- **Weaker conditions**: since application of Chebyshev's inequality only required finite variance, we can weaken the condition to $(\var S_n)/n^2\to0$.
- **Strong LLN**: Let $X_i$ be a sequence of i.i.d. random variables with $E(X_i)=\mu$, then $$\frac{X_1+\cdots+X_n}{n}\xto{\text{a.s.}}\mu\quad\text{as}\quad n\to\infty$$**Proof**: WLOG assume $\mu=0$, and denote $S_n^4=(X_1+\cdots+X_n)^4$. By independence we have $$E(S_n^4)=nE(X_i^4)+6C_4^2E(X_i^2X_j^2)$$Assuming $E(X_i^4)=K$, since $[E(X_i^2)]^2\le E(X_i^4)$ we have $E(S_n^4)\le nK+3n(n-1)K$, which implies that $$E\left(\frac{S_n^4}{n^4}\right)\le\frac{K}{n^3}+\frac{3K}{n^2}\Rightarrow E\left(\sum_{n=1}^\infty\frac{S_n^4}{n^4}\right)=\sum_{n=1}^\infty E\left(\frac{S_n^4}{n^4}\right)<\infty$$Thus with probability $1$, $\frac{S_n^4}{n^4}\to0$. The monotonicity of $y=x^4$ completes the proof.
> [!note] Difference between weak/strong law of large numbers
> The weak law of large numbers only state that the sample mean is likely to be near mean for any specific large $n^*$, but it says nothing about the bound for any $n>n^*$. This leaves open the possibility that some large value of $|(\sum X_i)/n-\mu|$ can occur infinitely often, which is denied by the strong law of large numbers.
- **Kolmogorov LLN**: let $X_i$ be a sequence of independent random variables with $EX_n=0$, then $$\sum_{n=1}^\infty\frac{\var X_n}{n^2}<\infty\quad\Longrightarrow\quad\frac{X_1+\cdots+X_n}{n}\xto\ae0$$**Proof**: denote $A_\nu$ the event that there exists at least one $2^{\nu-1}<n\le 2^{\nu}$ with $$\left|\frac{X_1+\cdots+X_n}{n}\right|>\varepsilon$$It suffices to show $\sum_{\nu\in\mathbb N}P(A_\nu)<\infty$ by characterization of $\ae$ convergence ([[Measurable Function]]). By definition $A_\nu$ implies $|X_1+\cdots+X_n|\ge2^{\nu-1}\varepsilon$, hence by Kolmogorov's inequality we have $P(A_\nu)\le 4\varepsilon^{-2}s_{2^\nu}^22^{-2\nu}$, hence $$\sum_{\nu\in\mathbb N}P(A_\nu)\le 4\varepsilon^{-2}\sum_{\nu\in\mathbb N}2^{-2\nu}\sum_{k=1}^{2^\nu}\var X_k\le8\varepsilon^{-2}\sum_{k=1}^\infty\frac{\var X_k}{k^2}<\infty$$ which completes the proof.
## Central limit theorem
**Central limit theorem**: let $X_1,X_2,\cdots$ be a sequence of **i.i.d.** RV having mean $\mu$ and variance $\sigma^2$, and denote $S_n=\sum_{k=1}^n X_n$, then we have $$\frac{S_n-n\mu}{\sqrt n\sigma}\xto{d}N(0,1)$$Note that the convergence is actually uniform.
**Proof via CF**: WLOG assume that $\mu=0,\sigma=1$. Denote $Z_n=(X_1+\cdots+X_n)/\sqrt n$, then $$\varphi_{Z_n}(t)=\varphi_{(X_1+\cdots+X_n)/\sqrt n}=\prod_{k=1}^n\varphi_{X_k}\left(\frac{t}{\sqrt n}\right)=\varphi_{X_1}^n\left(\frac{t}{\sqrt n}\right)$$Apply Taylor's theorem ([[Differential Calculus on Euclidean Space]]) with coefficients obtained via first and second moments, we get $$\varphi_{X_1}\left(\frac{t}{\sqrt n}\right)=1-\frac{t^2}{2n}+o\left(\frac{t^2}{n}\right)\quad\Longrightarrow\quad\varphi_{Z_n}(t)\to e^{-\frac{1}{2}t^2},\quad n\to\infty$$Hence by Lévy's continuity theorem ([[Random Variable]]) we have $Z\sim N(0,1)$. 
**Proof via MGF**: since the MGF of $X_i/\sqrt{n}$ is $$E\left(\exp\frac{tX_i}{\sqrt{n}}\right)=M\left(\frac{t}{\sqrt{n}}\right)$$by taking $L(t)=\log M(t)$ we can show that the mean and variance of the mean of variables are $\mu,\sigma^2$, thus $M_{Z_n}(t)\to M_{Z}(t)\Rightarrow F_{Z_n}(t)\to F_Z(t)$ for all $t$ at which $F_Z(t)$ is continuous. Now we only have to prove that $[M(t/\sqrt{n})]^n\to e^{t^2/2}$, which is equivalent to $nL(t/\sqrt{n})\to t^2/2$ by the lemma. The last equation can be proven by L'Hôpital's rule, thus completing the proof.
- **Intuition**: by [[Random Variable]] the PDF of sum of independent RVs is the convolution of PDF of each. If we look at the space of distributions of $\mu=0,\sigma=1$ equipped with convolution, the key for CLT is that is normal distribution is the attractive fixed point of each orbit (discussed in [[Convolution]]). 
- **Lyapunov CLT**: Let $X_i$ be a sequence of i.d. RV having $E(X_i)=\mu_i,\var(X_i)=\sigma_i^2$. If $X_i$ are uniformly bounded, and $\sum\sigma_i^2=\infty$, then$$P\left(\frac{\sum(X_i-\mu_i)}{\sqrt{\sum\sigma_i^2}}\le a\right)\to\Phi(a),\quad n\to\infty$$
## Additional properties
- Let $Z_n,n\ge1$ be a sequence of random variables and $c$ a constant such that, for each $\varepsilon>0, P(|Z_n-c|>\varepsilon)\to0$ as $n\to\infty$. Then for any bounded continuous function $g$, $$E[g(Z_n)]\to g(c)\quad\text{as}\quad n\to\infty$$
	- **Proof**: for $\varepsilon>0$, let $\delta>0$ be such that $|g(x)-g(c)|<\varepsilon$ whenever $|x-c|\le\delta$. Also, let $B$ be such that $|g(x)|<B$. Then $$\begin{align}E[g(X_n)]&=\int_{|x-c|\le\delta}g(x)dF_n(x)+\int_{|x-c|>\delta}g(x)dF_n(x)\\&\le(\varepsilon+g(c))P(|Z_n-c|\le\delta)+BP(|Z_n-c|>\delta)\end{align}$$by letting $n\to\infty$ we get $\lim\sup E[g(x)]\le g(c)+\varepsilon$. Similarly we could get $\lim\inf E[g(x)]\ge g(c)-\varepsilon$, the result follows since $\varepsilon$ is arbitrary.