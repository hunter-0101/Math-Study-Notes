Given the **probability space** $(\Omega, \Sigma,P)$ and some measurable space $(E,\mathcal E,\mu)$ (the state space), a **$(E,\mathcal E,\mu)$-valued random variable** is a measurable function $$X:(\Omega,\Sigma,P)\to(E,\mathcal E,\mu)$$We'll assume $(E,\mathcal E,\mu)=(\mathbb R,\mathcal B(\mathbb R),\lambda)$. A **multivariate RV** is a vector $X=(X_i)^T$ where each $X_k$ is an RV.
- **Intuition**: RV is not random and not a variable in the sense that, as a measurable $$X:\Omega\to E,\quad\omega\mapsto X(\omega)$$its output given a fixed $\omega$ is invariant, while the input $\omega$ has its randomness according to $P$. For $f=X_*P$, by definition ([[Measurable Function]]) it packs all $\omega$ that returns the same output under $X$ into one place, and assigns the total probability over them. Hence instead of the probability space, it illustrates the probability distribution over output space of $X$. 
  <p align="center"><img align="center" src="https://i.upmath.me/svg/%0A%5Cbegin%7Btikzcd%7D%0A%7B(%5COmega%2C%5CSigma)%7D%20%5Carrow%5Bd%2C%20%22P%22'%5D%20%5Carrow%5Br%2C%20%22X%22%5D%20%26%20%7B(E%2C%5Cmathcal%20E)%7D%20%5Carrow%5Bd%2C%20%22%5Cmu%22%5D%20%5Carrow%5Bld%2C%20%22%5Cmu_X%22%5D%20%5C%5C%0A%7B(%5Cmathbb%20R%2C%5Cmathcal%20B(%5Cmathbb%20R))%7D%20%20%20%20%20%20%20%20%20%20%20%20%20%20%26%20%7B(%5Cmathbb%20R%2C%5Cmathcal%20B(%5Cmathbb%20R))%7D%20%5Carrow%5Bl%2C%20%22f_X%22%5D%20%0A%5Cend%7Btikzcd%7D%0A"/></p>
- **PDF**: the PDF of an RV $X$ is formally R-N derivative ([[Signed Measure]]) defined as $$f_X=\frac{dX_*P}{d\mu}=\frac{d\mu_X}{d\mu}$$since $dX_*P\ll\mu$. General discussion can be found in [[Distribution Function]].
	- **PDF of sum of RVs**: for $X,Y$ independent we have that $$f_{X+Y}=f_X*f_Y$$which follows directly from the definition. [[Convolution]]
## Independence
we progressively define independence for objects of different level:
- **Sets**: for $A,B\in\Sigma$ we denote $A\perp B$ if $P(A\cap B)=P(A)P(B)$. This essentially says that occurrence of $A$ gives no information about $B$, since $P(B|A)=\frac{P(A\cap B)}{P(A)}=P(B)$ in this case. If otherwise $$P(AB)>P(A)P(B)\quad\iff\quad P(B|A)=\frac{P(A\cap B)}{P(A)}>P(B)$$then we know that $A,B$ shows a tendency of co-occur. For $<$ similarly $A,B$ appears to repel each other.
- **Sub-$\sigma$-algebras**: $\sigma_1\perp\sigma_2$ if $A\perp B,\forall A\in\sigma_1,B\in\sigma_2$. Following above, it's essentially saying that the measure on $\sigma_1\otimes\sigma_2$ coincides with the product $P|_{\sigma_1}\times P|_{\sigma_2}$, i.e., anything $A\in\sigma_1$ leaks no information about any $B\in\sigma_2$.
- **RVs**: $X\perp Y$ if $\sigma(X)\perp\sigma(Y)$ where $\sigma(X)$ is the $\sigma$-algebra generated by $X$ ([[Measurable Function]]). Since RV is intuitively a measurable function that returns an output for each event, $X\perp Y$ means that given $X(\omega_1)$ we cannot infer what is to be returned by $Y(\omega_2)$. In a [[Measure]]-theoretic perspective it's basically saying that the joint probability measure $P_{\Omega^2}$ is exactly the product measure $P\times P$.
The above definition is **equivalent to the following statements**: 
1. **Probability measure characterization**: for $Z=(X,Y)$, the measure $\mu_Z=\mu_X\otimes\mu_Y$, as we've already discussed above.
2. **Expectation characterization**: $E(g(X)h(Y))=Eg(X)\cdot Eh(y)$ for any Borel measurable $g,h$.
   **Proof**: for $\Rightarrow$ we have $$E(g(X)h(Y))=\int_{\Omega^2}g(X)h(Y)dP_{(X,Y)}=\int_\Omega\int_\Omega g(X)h(Y) dPdP=Eg(X)Eh(Y)$$by Fubini's theorem ([[Measure]]) since $X\perp Y$ ensures that $\Omega^2$ is equipped with the product measure. For $\Leftarrow$ we only have to take $g=\chi_A,h=\chi_B$ for all $(A,B)\in\sigma(X)\times\sigma(Y)$.
	- **Generalization**: in general, given a $\sigma$-finite measure space $(\Omega,\Sigma,\mu)$ and $X,Y:\Omega\to\mathbb R$ measurable functions let $\ell$ be a functional from the space of measurable functions to $\mathbb R$, then the measure on the product space $\mu_{(X,Y)}=\mu_X\times \mu_Y$ iff $$\ell(g(X)h(Y))=\ell(g(X))\ell(h(Y)),\quad\forall g,h\text{ measurable}$$Of course this is not a rigorous statement since there is no canonical norm defined on the space of measurable functions.
3. **Decomposition of CDF**: $F_{X,Y}(x,y)=F_{X}(x)F_Y(y)$. This is exactly the characterization of independence of events.
4. **Decomposition of PDF**: $f_{X,Y}(x,y)=f_X(x)f_Y(y)$, where $f_X,f_Y$ are marginal distributions.
   **Proof**: $4\Rightarrow 3$ is obvious by integration, and $3\Rightarrow4$ can be shown by the fact that $$\int_{[0,x]}f=0,\forall x\quad\iff\quad f\equiv0$$See [[Lebesgue Integral]] for proof of this equivalence.
	- **Discussion on coefficients**: obviously LHS integrates to $1$ over $\Omega_X\times\Omega_Y$, hence so is RHS. When applying Fubini's theorem ([[Lebesgue Integral]]) we need both the marginal distributions normalized, i.e., $\|f_X\|_1=\|f_Y\|_1=1$, this can always be achieved via proper adjustment of coefficients.
Below we list some related facts.
- **Independence of several RVs**: given $\set{X_i}$ they're said to be **pairwise independent** if any two RVs are independent; they're said to be **mutually independent** if 1 or 2 only applies to the total set.
- **Probability space for iid RV**: for $X_1,X_2\sim X:\Omega\to\mathbb R$ defined on $\Omega$, we have $$Z=X_1+X_2:\Omega_1\times \Omega_2\to\mathbb R$$That is, the probability space for the sum of $Z$ is the product $\sigma$-algebra rather than $\Omega$ itself. Essentially, independence requires the probability space of definition to be treated separately.
## Convergence of RV
For discussion of convergence we're essentially treating RV as a [[Measurable Function]], and operations like $X-Y$ will be treated as $(X-Y)(\omega)$ on $\Omega$ rather than $X(\omega_Y)-Y(\omega_Y)$ on $\Omega_X\times\Omega_Y$.
We have the following relation: $$\text{sure conv.}\Longrightarrow\text{a.s. conv.}\Longrightarrow\text{conv. in probability}\Longrightarrow\text{conv. in distribution}$$Note that convergence in probability $\iff$ convergence in measure under probability measure.
- **Convergence in distribution**: for a sequence of RV $X_n$ with CDF $F_n$, it's said to converge in distribution (converge weakly) to $X$ with CDF $F$ if $$X_n\xrightarrow{d}X\quad\iff\quad F_n\xto{\ae}F$$It's weakest mainly because it says nothing about behavior of RV on null sets and atomic points (it may even occur that $X_n$ and $X$ are defined on different probability spaces).
	- **Relation with PDF convergence**:  $f_n\xto{\ae}f\Rightarrow F_n\xto{\ae}F$ given that $f$ is also a PDF, where the convergence is uniform over all Borel sets, since $$\delta_n=f_n-f\quad\Longrightarrow\quad\delta_n^-\ge-f$$hence DCT can be applied to get $\int\delta_n^-\to0$, and by $\int_{\mathbb R}\delta_n=0$ we get $\int\delta_n^+\to0$, which is enough to guarantee uniform convergence of $F_n$ over $\mathcal B(\mathbb R)$. $\Leftarrow$ is generally not true even if $F_n\to F$ uniformly and $F$ is CDF, e.g., $f_n=(1+\cos2\pi nx)\chi_{[0,1]}$, but extra conditions like uniform integrability of $\set{f_n}$ or total variation convergence of $F_n$ could establish $f_n\xto{L^1}f$.
	- **Why CDF not PDF**: the key is that, as discussed in [[Distribution Function]], every random variable has a CDF, but for discrete RV PDF does no exist without introducing Delta function ([[Special Function Gallery]]), which contradicts the R-N derivative definition above since Delta is not AC w.r.t $\lambda$. Of course if we're using other measure (e.g., counting measure) then it depends.
- **Convergence in probability**: given $P$ we write $$X_n\xrightarrow{P}X\quad\iff\quad\lim_{n\to\infty}P(|X_n-X|>\varepsilon)=0,\quad\forall\varepsilon>0$$This is the probability version of convergence in measure ([[Measurable Function]]). 
	- **Intuition**: for this condition to be satisfied it's not possible that $X_n,X$ are independent for all $n$, since it's a restriction on the joint CDF (unless $X$ is deterministic, as in LLN).
- **Convergence in mean**: via the $r$-moment we define $$X_n\xto{L^r} X\quad\iff\quad\lim_{n\to\infty} E|X_n-X|^r=0$$This implies $X_n\xto{P}X$ by (general) Chebyshev's inequality ([[Central Limit Theorem]]), but there is no obvious relation with $\text{a.s.}$ convergence.
	- **Relation with [[Lp Space]] convergence**: by definition of [[Expectation]] this convergence is exactly the probability version of $L^p$ norm convergence, treating $X$ as a measurable function. Hence related properties could be directly inherited.
- **Almost sure convergence**: we write $$X_n\xto{\text{a.s.}}X\quad\iff\quad P\left(\lim_{n\to\infty}X_n=X\right)=1$$It's the probability version of $\ae$ convergence. [[Measurable Function]]
- **Sure convergence**: $X_n$ converge surely (everywhere/pointwise) towards $X$ if$$\lim_{n\to\infty}X_n(\omega)=X(\omega),\quad\forall\omega\in \Omega$$This is the strongest of all because the behavior of RV on ALL possible $\omega$ are restricted, rather than allowing a null set to diverge.
- **Continuous mapping theorem**: given RVs $X_n\to X$ on $\Omega$ a metric space, for any $g:S\to S'$ continuous we have that $g(X_n)\to g(X)$ where the mode of convergence is the same as $X_n$. Each of the statements relies on some simple estimation.
- **Slutsky's theorem**: if $X_n\xto{d} X,Y_n\xto{P}c$, then $$X_n+Y_n\xto{d}X+c,\quad X_nY_n\xto{d}Xc,\quad X_n/Y_n\xto{d} X/c$$This follows directly from continuous mapping theorem above since $g(x,y)=x+y,xy,xy^{-1}$ are all continuous.
## Moment
Moments of a function measures the **shape of the function's graph**. The n-th moment of an $\mathbb R$-valued RV about a value $c$ is defined by the [[Expectation]] $$\mu_n(c)=E[(X-c)^n]=\int_\Omega (X-c)^ndP$$using the [[Lebesgue Integral]] framework. Note that $\mu_n(b)=\sum_{i=0}^nC_n^i\mu_i(a)(a-b)^{n-i}$ for any $a,b\in\mathbb R$. $\mu_n/\sigma^n$ is the n-th **standardized moment**. 
- **Hausdorff moment theorem**: for $f:[a,b]\to\mathbb C$ be continuous, then $$\int_a^bx^kf(x)dx=0,\forall k\in\mathbb N\quad\Longrightarrow\quad f\equiv0$$This can be shown easily by the Weierstrass approximation theorem ([[Functional Approximation]]). 
	- **Related problems**: **Hamburger moment problem** focuses on $\mathbb R$, while **Stieltjes moment problem** focuses on $[0,\infty)$. These problems are not fully solved yet, say, the wavelet $h(x)=\exp(-x^{1/4})\sin x^{1/4}$ ([[FT on L1]]) satisfies $\int x^kh(x)=0,\forall k\in\mathbb N$.
- **Characterization of distribution**: as characteristic function can be expanded as $$\varphi_X(t)=\sum_{n=0}^\infty\frac{i^n\mu_n}{n!}t^n,\quad\forall t<\rho\text{ the radius of convergence}$$Thus as long as we have infinite radius of convergence, i.e., $$\limsup_{n\to\infty}\left|\frac{\mu_n}{n!}\right|^{\frac{1}{n}}=0\iff\limsup_{n\to\infty}\frac{|\mu_n|^{\frac{1}{n}}}{n}=0$$then the set of moments will uniquely determine the distribution.
	- **Convergence of moments implies convergence in distribution**: if $X$ is fully determined by its set of moments, then for any sequence $\set{X_n}$, $$\lim_{n\to\infty}EX_n^k=EX^k,\forall k\in\mathbb N\quad\Longrightarrow\quad X_n\xto{d} X$$An example is that normal distribution is fully characterized by its moments, and any RV whose moments converge to those of normal also converges to normal.
- **Moment generating function**: for $t\in\mathbb R$ defined $$M(t)=E(e^{tX})=\int_\mathbb R e^{tx}f(x)dx$$The moment generating function gets its name from the fact that $$M^{(n)}(t)=\frac{d^n}{dt^n}E(e^{tX})=E(X^ne^{tX})\quad\Longrightarrow\quad M^{(n)}(0)=EX^n=\mu_n(0)$$where we have assume that **differentiation and expectation are interchangeable**. 
	- **MGF of sum of independent RV**: $M_{X+Y}(t)=M_X(t)M_Y(t)$ for independent $X,Y$. 
	- $M_X(t)=\sum_{n=0}^\infty\frac{t^n}{n!}E(X^n)$.
	- Moment generating function **uniquely determines the distribution function**. We usually inverse this procedure by "recognizing the MGF" ([[Distribution Function Gallery]]). More practical methods include inverse Laplace transform or the method of moments.
	- **Joint moment generating functions**: for $n$ random variables $X_i(1\le i\le n)$, we define the joint moment generating function $$M(t_1,\cdots,t_n)=E(e^{t_1X_1+\cdots+t_nX_n})$$We have that $X_i$ are independent $\Leftrightarrow M(t_1,\cdots,t_n)=M_{X_1}(t_1)\cdots M_{X_n}(t_n)$.
## Cumulant generating function
**Cumulant generating function (CGF)** is an alternative characterization for a distribution in align with MGF, and is defined by $$K_X(t)=\ln M_X(t)=\ln Ee^{tX}=\sum_{n=1}^\infty\kappa_n\frac{t^n}{n!}$$where $\kappa_n$ is called the $n$-th **cumulant**.
- **Property of cumulant**: cumulant has many properties that are a bit different with MGF, which makes it more useful in some cases.
	- **Translational invariance**: $\kappa_n(X+c)=\kappa_n(X),\forall n\ge2$ and $\kappa_1(X+c)=\kappa(X)+c$.
	- **Homogeneity**: $\kappa_n(cX)=c^n\kappa_n(X),\forall n\in\mathbb Z_+$.
	- **Cumulative property**: given $X_1,\cdots,X_k$ independent, then $$\kappa_n\left(\sum_{i=1}^kX_i\right)=\sum_{i=1}^k\kappa_n(X_i),\quad\forall n\in\mathbb Z_{\ge1}$$That is, the CGF itself is cumulative: $$K_{\sum_{i=1}^kX_i}(t)=\sum_{i=1}^kK_{X_i}(t)$$this is how it gets its name.
- **Relation with moments**: by direct differentiation we have $$\kappa_1=EX,\quad\kappa_2=\var X,\quad\kappa_3=\mu_3$$here $\mu_n$ are central moments. 
## Characteristic function
For an RV $X$ its characteristic function is defined as $$\varphi_X(t)=E(e^{itX})=\int_\mathbb R e^{itx}dF(x)$$which is actually the inverse Fourier transform of its PDF.
- **Why inverse**: they're essentially equivalent since $\mathcal F^2=\mathcal R$ where $\mathcal Rf(x)=f(-x)$. There are two main factors for this choice:
  1. **Conventions in engineering**: it's more common to use $e^{itx}$ rather than $e^{-itx}$ in engineering due to conventional issues. This is adapted in classical probability theory, hence also the measure-theoretic probability theory.
  2. **Connection with moments**: in MGF we're using $Ee^{tX}$, hence choosing $Ee^{itX}$ fits more. Also the expression is more natural compared to $Ee^{-itX}$ from probabilistic perspective.
- **Relation with MGF**: we have $\varphi_X(-it)=M_X(t)$. Note that the characteristic function is **always well-defined** ([[FT on L1]]), while the MGF may not exists due to convergence issue.
- **Continuity**: since $e^{-itx}$ is bounded CF is continuous by the DCT. [[Lebesgue Integral]]
	- Since $|\varphi(t+h)-\varphi(t)|\le E|e^{ihX}-1|$ we know that $\varphi$ is also **uniformly continuous**.
- **Smoothness**: $\varphi^{(k)}(0)=i^kEX^k$ when the $k$-th moment exists. Conversely if $\varphi^{(k)}(0)$ exists then $EX^i$ exists up to $k$ if $k$ even, but only to $k-1$ if $k$ odd.
- **Basic properties**
	- $|\varphi(t)|\le1$ by triangle inequality
	- $\varphi(-t)=\overline{\varphi(t)}$, which is obvious from the definition. Further if $\varphi(x)\in\mathbb R$ then it's even.
	- $\varphi_{\sum a_iX_i}(t)=\prod\varphi_{X_i}(a_it)$ if $X_i$ are independent, by convolution theorem ([[FT on L1]]).
- **Lévy's continuity theorem**: for $X_n(n\in\mathbb N)$ with $\varphi_{X_n}$ if $\varphi_{X_n}\to\varphi$ pointwise then $$X_n\xto{d} X\quad\iff\quad \varphi=\varphi_X\quad\iff\quad\varphi\in C(\mathbb R)\quad\iff\quad \lim_{t\to0}\varphi(t)=\varphi(0)$$This follows directly from the uniqueness of [[FT on L1]]. Notice that pointwise limit of CF is not necessarily a CF. 
- **Criteria for characteristic function**: the set of characteristic functions is closed under **convex linear combination (finite or countable)** and **finite product**. Further, for $\varphi$ a characteristic function, $\overline\varphi,\Re\varphi,|\varphi|^2,\varphi(\alpha t)$ are all characteristic functions.
	- **Pólya's theorem**: if $\varphi$ is a **real-valued**, **even**, **continuous** function such that $\varphi$ is convex for $t>0$ and $\varphi(0)=1,\varphi(\infty)=0$, then it's a characteristic function.