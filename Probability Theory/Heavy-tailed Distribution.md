The [[Distribution Function]] $F$ of [[Random Variable]] $X$ is **(right) heavy-tailed** if $$M_X(t)=\infty,\quad\forall t>0\quad\iff\quad Ee^{tX}=\int_\mathbb R e^{tx}dF(x)=\infty,\quad\forall t>0$$We can also write it with the tail distribution function $\overline F(x)=1-F(x)$ as $$\lim_{x\to\infty}e^{tx}\overline F(x)=\infty,\quad\forall t>0$$Such distributions are also said to have **heavy (right) tail**.
- **Common heavy-tailed distributions**: common examples include Cauchy distribution ([[Distribution Function Gallery]]), log-normal distribution ([[Normal Distribution]]), and $t$-distribution ([[Normal Distribution]]).
- **Long-tailed distribution**: a heavy-tailed distribution $F$ is **(right) long-tailed** if $$\lim_{x\to\infty}\frac{\overline F(x+t)}{\overline F(x)}=1,\quad\forall t>0$$Equivalently, in terms of asymptotic analysis, we have $\overline F(x+t)\sim\overline F(x),x\to\infty$ for any $t>0$. 
	- **Examples**: log-normal, log-Gamma, and the distribution generated by power law, as defined below. 
## Pareto distribution
A continuous RV $X$ is **Pareto-distributed** if its survival function is given by $$\overline F(x)=P(X>x)=\begin{cases}\left(\dfrac{x_\min}{x}\right)^\alpha,&x\ge x_\min\\1,&x<x_\min\end{cases}$$where the **scale parameter** $x_\min>0$ is the minimum possible value of $X$, and $\alpha>0$  is the **shape parameter (tail index)**. More generally, $X$ is said to follow a **power law distribution** if $$\overline F(x)\propto x^{-\alpha},\quad x\ge x_\min$$It's obvious that such distribution is **long-tailed**, hence **heavy-tailed**.
- **Relation with [[Pareto Principle]]**: the heavy tail of Pareto distribution formalize the general idea of Pareto principle that only a small proportion of the effort contributes to a large proportion of the outcome, and the 80/20 principle is realized by $\alpha=\log_45\approx1.16$. Empirical observation shows that this distribution fits a wide range of cases, including natural phenomenon and human activities.
- **PDF & CDF**: by definition we can directly deduce that $$F_X(x)=\begin{cases}1-\left(\dfrac{x_\min}{x}\right)^\alpha,&x\ge x_\min\\0,&x<x_\min\end{cases}\quad\text{and}\quad f_X(x)=\begin{cases}\dfrac{\alpha x_\min^\alpha}{x^{\alpha+1}},&x\ge x_\min\\0,&x<x_\min\end{cases}$$by taking logarithm on both sides of PDF we get $$\ln f_X(x)=-(\alpha+1)\ln x+\ln(\alpha x_\min^\alpha)$$therefore it's plotted as a straight line on the log-log plot. 
- **Finite moment condition**: by simple [[Integral Calculus on Euclidean Space]] we have $$EX^k<\infty\quad\iff\quad\alpha>k$$it's a defining feature of heavy-tailed distributions and leads to the counter-intuitive results often observed in real-world systems governed by power laws. 
	- **Sum of Pareto-distributed RVs**: when $\alpha>2$ we have finite mean and variance, hence the classic [[Central Limit Theorem]] applies to yield that $$\frac{S_n-n\mu}{\sqrt n\sigma}\xto{d}N(0,1)$$If $\alpha<2$, on the other hand, we need to invoke the generalized CLT to conclude that it converges in distribution to Lévy distribution ([[Stable Distribution]]). 
- **Universality**: the equivalence of power laws with a particular scaling exponent can have a deeper origin in the dynamical processes that generate the power-law relation. Diverse systems with the same critical exponents - that is, which display identical scaling behavior as they approach [[Criticality]] can be shown, via renormalization group theory, to share the same fundamental dynamics. Formally, this sharing of dynamics is referred to as **universality**, and and systems with precisely the same critical exponents are said to belong to the same **universality class**. 
### Case study: the coin-flipping gamble 
#### Example 1: single-player coin-flipping with exponential reward
Denote the overall reward as $S$ and a temporary reward variable as $X$, initiated as $X=1$. Now imagine flipping a fair coin with the following rules in each round:
1. If it's head, you get the reward $S\gets S+X$, and $X$ is reinitialized as $X\gets1$.
2. If it's tail, then the temporary reward is doubled: $X\gets 2X$, and you start the next round.
Obviously if you're only allowed to claim your reward once, then the expectation of this game is $$ES=\sum_{n=1}^\infty 2^n\times\frac{1}{2^n}=\infty$$hence it's extremely appealing, but you're very likely to end the game with relatively small amount of points.
## Comparison with normal distribution
[[Normal Distribution]] and power law distribution are two extremely common types of distributions in real world, and they're fundamental, structural different, illustrating the general comparison between light-tailed and heavy-tailed distributions.
- **Normal distribution**: as discussed above (or seen from the proof), normal distribution arises as **sum** of a sequence of events exhibiting properties including:
	- **Additivity/Locality**: events are additive, and a large event is merely the sum of many small, local contributions.
	- **Independence**: the variables contributing to the outcome are independent, or at most weakly correlated. The state of one component does not significantly influence the state of another.
	- **Finite variance**: the variance of the distribution is finite, hence extreme cases are rare.
  These characteristics makes the outcome totally predictable based on mean and standard deviation. Hence normal distribution rewards **consistency**. 
- **Power law distribution**: power law distribution arises as **product** of a sequence of events exhibiting properties including: 
	- **Multiplicativity/Global**: outcomes are often generated by a process where the growth/change at time $t$ is proportional to the size at $t-1$. (e.g., compound interest, network growth where "rich get richer"). 
	- **Interdependence & feedback**: the state of one element strongly influences the state of others, often via positive feedback loops. The system is a complex network where local perturbations can cascade globally. This is the essence of **scale invariance** and long-range correlations seen in [[Criticality]].
	- **Infinite variance**: given $\alpha\le2$ the variance (and for $\alpha\le1$, the mean) is theoretically infinite. This means the standard descriptive statistics like $\mu,\sigma$ are inadequate or misleading. 
  These heavy tailed ([[Distribution Function]]) characteristics makes the outcome highly unpredictable. In such cases instead of consistency, it would get much more to bet on occurrence of extreme cases via **persistency**. 