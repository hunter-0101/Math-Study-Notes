The philosophical motivation behind the definition of conditional probability can be found in [[Philosophy of Probability & Statistics]]. 

**Cox's theorem**: assume that there is a **measure of plausibility** satisfying:
1. **Divisibility & comparability**: plausibility for a proposition is always a real number ([[Real Number System]]) and is dependent on the information we have related to the proposition.
2. **Common sense**: plausibilities should vary sensibly with the assessment of plausibilities in the model.
3. **Consistency**: if the plausibility can be derived in many ways, all the results must be equal. 
Then we can derive most of the axioms of probability. This is used as a justification for the use of Bayesian probability theory, and many philosophers suggest, based on this theorem, that probability theory should be used as a normative model for reasoning. 

**Remark on notation**: we adapt the definitions and assumptions in [[Random Variable]].

---
## Conditional expectation
Given a [[Probability Space]] $(\Omega,\Sigma,P)$ and a [[Random Variable]] $X$, consider a sub-$\sigma$-field $\mathcal C\subset\Sigma$. **A version of conditional expectation** of $X$ given $\mathcal C$, denoted $E(X|\mathcal C)$, is any function $h$ that is $(\mathcal C,\mathcal B(\mathbb R))$-measurable and $$\int_ChdP=\int_CXdP,\quad\forall C\in\mathcal C$$Existence of such function is obvious via domain projection of $X$ onto $\mathcal C$. The formal proof relies on R-N derivative ([[Signed Measure]]) by observing that the measure $\mu(C)=\int_CXdP$ satisfies $\mu\ll P$. For two RVs $X,Y$ we define $E(X|Y)=E(X|\sigma(Y))$.
- **Intuition**: $E(X|\mathcal C)$ is a "**restricted**" version of $X$ whose "observable randomness" depends only on $\mathcal C$. This is done by **averaging $X$ out** on sets that are "finer" than $\mathcal C$, and discarding sets that do not intersect with $\mathcal C$. 
	- **[[Hilbert Space]] analogy**: the domain projection analogy is rigorous if we consider $$\mathcal H=\set{X\in L^2}\quad\text{with}\quad\braket{X,Y}=E(XY),\quad\mathcal H_0=\set{X\in\mathcal H:EX=0}$$in which case we'll have an intuitive correspondence $$\cos(X,Y)=\rho(X,Y),\quad X\perp_{\mathcal H} Y\iff E(XY)=0,\quad\forall X,Y\in\mathcal H_0$$it can be verified in this case that $\operatorname{Proj}_{\mathcal H_\mathcal C}(X)=E(X|\mathcal C)$ where $\mathcal H_\mathcal C$ is the subspace restricted to RVs that are $\mathcal C$-measurable.
	- **Why conditional expectation first**: conditional expectation is more foundational and better-behaved analytically in the framework of measure theory, and it's also natural and easier to define $P(A|\mathcal C)=E(\chi_A|\mathcal C)$. Another thing to notice is that conditional probability itself requires some restriction on the probability space to be regular (e.g.,, $\Omega$ being a Polish space).
	- **Why use version**: similar to [[Lp Space]], we're using equivalent classes under $\ae$ equality.
- **Conditional variance**: we define $$\var(X|Y)=E(X-E(X|Y))^2|Y)$$and similarly **conditional covariance** as $$\cov(X,Y|Z)=E\big((X-E(X|Z))(Y-E(Y|Z))|Z\big)$$since conditional probability is also a probability measure, these two concepts inherits all properties from [[Random Variable]].
	- **Law of total variance**: $\var(X)=E(\var(X|Y))+\var(E(X|Y))$. This can be seen from $$\cov(E(X|Y),X-E(X|Y))=0\Longrightarrow\var(X-E(X|Y))=E(X-E(X|Y))^2=E(\var(X|Y))$$Intuitively it's a generalization of Pythagorean's law where perpendicularity is measured by $\cov$. Similarly we have law of total covariance $$\cov(X,Y)=E\big(\cov(X,Y|Z)\big)+\cov\big(E(X|Z),E(Y|Z)\big)$$via similar proof.
- **Properties**
	- **William's tower property**: for $\mathcal C_1\subset\mathcal C_2\subset\mathcal F$, $E(X|\mathcal C_1)=E(E(X|\mathcal C_2)|\mathcal C_1)$. This is intuitively obvious since the tower is just a nested "coarsening" process.
		- **Remark on application**: sometimes multiple RVs, e.g. $X,Y$ are involved in $E$, with property that $X$ is totally determined by some $\mathcal C\subset\Sigma$. In such case $$E(E(XY|\mathcal C))=E(YE(X|\mathcal C))$$one application of this is the concentration bound for MDS in [[Martingale Method]].
	- **Linearity**: $E(X|\mathcal C)+E(Y|\mathcal C)=E(X+Y|\mathcal C)$ by linearity of [[Lebesgue Integral]].
	- $E(XY|\mathcal C)=XE(Y|\mathcal C)$. This can be proven following the standard machinery.
## Conditional probability
For some $\mathcal A\subset\Sigma$ define the conditional probability of $A\in\mathcal A$ given $\mathcal C$ as $$P(A|\mathcal C)=E(\chi_A|\mathcal C),\quad\forall A\in\mathcal A$$and the collection of versions $\set{P(A|\mathcal C):A\in\mathcal A}$ is a **regular conditional probability** if $P(\cdot|\mathcal C)(\omega)$ is a probability measure on $(\Omega,\mathcal A)$ for each $\omega\in \Omega$.
- **Conditional distribution**: given RV $X$, for each $B\in\mathcal B(\mathbb R)$ define $$\mu_{X|\mathcal C}(B)(\omega)=P(X^{-1}(B)|\mathcal C)(\omega)$$A collection of versions $\set{\mu_{X|\mathcal C}(B)(\cdot):B\in\mathcal B}$ is called a conditional distribution of $X$ given $\mathcal C$. It's called a **regular conditional distribution** if $\mu_{X|\mathcal C}(\cdot)(\omega)$ is a probability measure for each $\omega$.
- **Conditional distribution given random variable**: for another RV $Y$ define the conditional distribution of $X$ given $Y$ as the collection $$\mu_{X|Y}=\set{\mu_{X|Y}(B|y)=\mu_{X|\sigma(Y)}(B)(\omega):\omega\in Y^{-1}(y),y\in\mathbb R}$$Notice that in above expression we assume $(E_Y,\mathcal E_Y)=(\mathbb R,\mathcal B(\mathbb R))$. In general we do not require $Y$ to have the same state space with $X$.
## others
- **Conditional independence**: we say that $E_1,E_2$ are independent conditioning $F$ if  $$P(E_1E_2|F)=P(E_1|F)P(E_2|F)$$
- **Bayesâ€˜ theorem**: for an event $E$ and a partition of the sample space $\{F_i\}$, we have$$P(F_j|E)=\frac{P(E|F_j)P(F_j)}{\sum\limits_iP(E|F_i)P(F_i)}$$this follows directly from the definition of conditional probability. 
- The **conditional PDF** is$$f_{X|Y}(x|y)=\frac{f(x,y)}{f_Y(y)}=P(x\le X\le x+dx|y\le Y\le y+dy)$$which can be verified by a limit argument from conditional probability. The CDF is just the integral of PDF. 