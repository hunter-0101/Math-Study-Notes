### Poisson process
- Suppose that "events" are occurring at random points at time, and $N(t)$ denotes the number of events that occur in the time interval $[0,t]$. $\{N(t),t\ge0\}$ is said to be a **Poisson process** having rate $\lambda,\lambda>0$, if:
	- $N(0)=0$.
	- The numbers of events that occur in disjoint time intervals are independent. (the *independent increment assumption*)
	- The distribution of the number of events that occur in a given interval depends only on the length of that interval and not on its location. (the *stationary increment assumption*)
	- $P(N(h)=1)=\lambda h+o(h)$.
	- $P(N(h)\ge2)=o(h)$.
- The foregoing conditions imply that $N(t)$ has a Poisson distribution with mean $\lambda t$, i.e., $P(N(t)=0)=e^{-\lambda t}$.
	- $P(N(t)=n)=\frac{e^{-\lambda t}(\lambda t)^n}{n!}$.
- Define the time elapsed between $(n-1)$th and $n$th event as $T_n$, the the sequence $\{T_n,n=1,2,\cdots\}$ is called the **sequence of interarrival times**. It can be shown that $T_1,T_2,\cdots$ are independent exponential random variables with mean $1/\lambda$.
### Markov chain
Consider sequence of random variables $X_0,X_1,\cdots$ taking values from $\{1,2,\cdots,M\}$. They're said to form a **Markov chain** if the system has a **transition probability** $P_{ij}$, denoting the probability of transition from $i$ to $j$. More generally, define $$P_{ij}^{(n)}=P(X_{m+n}=j|X_m=i)$$
- **The Chapman-Kolmogorov equations**$$P_{ij}^{(n)}=\sum_{k=0}^MP_{ik}^{(r)}P_{kj}^{(n-r)}\quad\text{for all}\quad0<r<n$$This can be proven with a slight expansion and rearrange of terms.
- **Ergodic Markov chain**: a Markov chain is said to be ergodic if for some $n>0$, $P_{ij}^{(n)}>0$ for all $i,j=0,\cdots,M$.
	- For a ergodic Markov chain, $$\pi_j=\lim_{n\to\infty}P_{ij}^{(n)}$$exists, and the $\pi_j,0\le j\le M$, are the unique nonnegative solutions of $$\pi_j=\sum_{k=0}^M\pi_kP_{kj},\qquad\sum_{j=0}^M\pi_j=1$$
	- Intuitively, $\pi_j$ is the limit that $P_{ij}^{(n)}$ converges to as $n\to\infty$. That is, for a large Markov chain, the probability of being in state $j$ after $n$ transitions is approximately equal to $pi_j$, no matter what the initial state was. It can be proven using the strong law of large numbers. [[Central Limit Theorem]]
	- Let $P_j$ be the long-run proportion of time that the Markov chain is in state $j$, then according to the uniqueness of $\pi_j$, we know that $P_j=\pi_j$.
- **Doubly stochastic**: a transition probability matrix is said to be doubly stochastic if $$\sum_{i=0}^MP_{ij}=1,\quad\forall j=0,1,\cdots,M$$
### Surprise, uncertainty, and entropy
- Let $S(p)$ denote the amount of surprise when hearing that an event of probability $p$ occurs. Then it should follow the following axioms:
	- $S(1)=0$.
	- $S(p)$ is a continuous, strictly decreasing function of $p$.
	- $S(pq)=S(p)S(q)$. This is intuitive when considering two events to occur in sequence/together.
- **Theorem**: if $S(\cdot)$ follows the above axioms, then $S(p)=-C\log_2p$ where $C$ is an arbitrary positive integer.
	- This follows from Cauchy's equation, and we usually take $C=1$.
	- For a discrete random variable $X$ taking values based on $P(X=x_i)=p_i$, it's natural to define the expected amount of surprise evoked by $X$ as $H(X)=-\sum p_i\log p_i$. This is how we define entropy in [[Entropy & Information]]. For more on this topic, refer to the same document.
### Coding theory
- Let $X$ take on the possible values $x_1,\cdots,x_N$. Then, in order to be able to encode the values of $X$ in binary sequences (none of which is an extension of another) of respective lengths $n_1,\cdots,n_N$, it is necessary and sufficient that $$\sum_{i=1}^N\Big(\frac{1}{2}\Big)^{n_i}\le1$$
- **The noiseless coding theorem**: let $X$ take on the values $P(X=x_i)=p(x_i),1\le i\le N$. Then, for any coding of $X$ that assigns $n_i$ bits to $x_i$, $$\sum_{i=1}^Nn_ip(x_i)\ge H(X)$$
## Simulation
### Simulating continuous random variables
- **The inverse transformation method**: let $U\sim Uniform(0,1)$. For any continuous distribution function $F$, if we define a random variable $Y$ by $$Y=F^{-1}(U)$$ then $Y$ has distribution function $F$.
- **The rejection method**: this methods can be decomposed into two steps:
	1. Simulate $Y\sim g$ and a random number $U$.
	2. If $U\le f(Y)/cg(Y)$, set $X=Y$. Otherwise return to step 1.
	The random variable $X$ generated by this method has density function $f$.
### Variance reduction techniques
Supposed that we're interested in computing $\theta=E[g(X_1,\cdots,X_n)]$. The easiest way of simulation is simulate random variables $X_i^{(k)}$ and take $Y_k=g(X_1^{(k)},\cdots,X_n^{(k)})$. Let the estimator be $\overline{Y}=\sum\frac{Y_s}{k}$, then $$E(\overline{Y})=\theta,\quad E((\overline{Y}-\theta)^2)=Var(\overline{Y})$$There are some techniques that could be use to reduce the variance of the estimator.
- **Use of antithetic variables**: consider $$Var\Big(\frac{Y_1+Y_2}{2}\Big)=\frac{1}{4}\big(Var(Y_1)+Var(Y_2)+2Cov(Y_1,Y_2)\big)=\frac{Var(Y_1)}{2}+\frac{Cov(Y_1,Y_2)}{2}$$It would be advantageous (in the sense that the variance would be reduced) if $Y_1,Y_2$ are negatively correlated than being independent.
  Assume that $X_i$ are simulated from $F_i^{-1}(U_i)$ using inverse transformation method, let $$\begin{align}&Y_1=g\big(F_1^{-1}(U_1),\cdots,F_n^{-1}(U_n)\big)\\&Y_2=g\big(F_1^{-1}(1-U_1),\cdots,F_n^{-1}(1-U_n)\big)\end{align}$$ then in most cases $Y_1,Y_2$ are negatively correlated (e.g., $g$ is monotonic).
- **Variance reduction by conditioning**