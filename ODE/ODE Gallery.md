## [[First Order ODE]] 
- **Separable equations**: For equations that could be rearranged into the form$$M(x)dx+N(y)dy=0$$we could easily solve them by integration. Don't forget the constant $C$ on the right-hand-side.
	- **Discussion on rigorousness**: strictly speaking the expression involving terms like $dx,dy$ are actually not rigorous - as discussed in [[Differential Calculus on Euclidean Space]], the operator $\frac{d}{dx}$ itself is a whole rather than a fraction, hence manipulation of denominator is not valid. Instead, what we're used to be doing is actually simplified version of the following more complex but rigorous process: starting with separation of variables: $$\frac{dy}{dx}=f(x)g(y)\quad\iff\quad\frac{1}{g(y)}\frac{dy}{dx}=f(x)$$this is an equality between [[Function on Euclidean Space]], hence integrating w.r.t $x$ yields $$\int\frac{1}{g(y(x))}\frac{dy}{dx}dx=\int f(x)dx$$at this point we invoke the change of variable formula, setting $u=y(x)$, to get $$\int\frac{1}{g(y(x))}\frac{dy}{dx}dx=\int\frac{1}{g(u)}du=\int\frac{1}{g(y)}dy$$hence we can work out the integral to obtain an equality of the form $y=y(x)$. To sum up, manipulation of $dx,dy$ works because we're not merely "doing whatever seems right", but instead **skipping the change of variable** step to make it more heuristic.
	- **Discussion under [[Differential Form]]**: it turns out that the theory of differential forms provide a rigorous theoretical foundation for the seemingly non-rigorous manipulation of $dx,dy$. Since by definition we have the equivalence $$\frac{dy}{dx}=-\frac{M(x)}{N(y)}\quad\iff\quad M(x)dx+N(y)dy=0$$if we set the 1-form $\omega=M(x)dx+N(y)dy$, then $$\omega=d\Phi\where\Phi=\int M(x)dx+\int N(y)dy$$we know that $\omega$ is exact, hence the condition $\omega=0$ is translated to $d\Phi=0$. This follows from the Stokes' theorem ([[Integral Calculus on Smooth Manifold]]). Note that rigorousness here actually follows from a purely algebraic construction of [[Integration & Differentiation]].
- **Homogeneous equations**: homogeneous equations are those in the form $$\frac{dy}{dx}=f(x,y)=g(\frac{y}{x})$$This kind of equation can be solved by:
	1. Introduce the new variable $v=y/x$ and express $dy/dx$ in terms of $v,x$.
	2. Substitute terms consisting $y$ with $v,x$. This will results in a separable equation.
	3. Solve the equation and substitute $y$ back into the solution.
- **Autonomous equations**: First order equations whose independent variable does not appear explicitly are called autonomous equations, which has the form $$dy/dt=f(y)$$Zeros of $f(y)$ are called **critical points**. Solutions that is converged to for large independent variable is called **asymptotically stable solution**, while the solutions that are diverged from are called **unstable equilibrium solution**, with the root being the critical point.
	- **Bifurcation points**: consider $dy/dt=f(a,y)$ where $a$ is a real parameter. As $a$ steadily increases/decreases, it often happens that at a certain value of $a$, called bifurcation points, critical points come together, or separate, and equilibrium solutions may be either lost or gained.
- **Exact equations**: If a differential equation $M(x,y)+N(x,y)y'=0$ can be written in the form $$\frac{d}{dx}\psi(x,\phi(x))=0$$then it is an exact differential equation. Its solution is given by $\psi(x,y)=C$.
	- **Calculation of $\psi$**: we can consider using $$\psi=\int_0^xM(s,y)ds+\int_0^yN(0,s)ds$$ or manipulate the equation to obtain a total derivative expression.
	- **Integrating factors**: it is possible to convert a differential equation that is not exact into an exact equation by multiplying the equation by a suitable integrating factor. See the next section below.
> [!info] Verification of exact equations
> Let the functions $M,N,M_y,N_x$, where subscripts denote partial derivatives, be continuous in the rectangular region $R:\alpha<x<\beta,\gamma<y<\delta$. Then equation $$M(x,y)+N(x,y)y'=0$$is an exact differential equation in $R$ iff $$M_y(x,y)=N_x(x,y)$$at each point of $R$. That is, there exists a function $\psi$ satisfying $$\psi_x(x,y)=M(x,y),\quad\psi_y(x,y)=N(x,y)$$iff $M,N$ satisfy the above equation.
- **Bernoulli equation**: For equations of the form $$y'+p(x)y=g(x)y^a,\quad a\neq0,1$$multiplying $y^{-1}$ on both sides and let $z=y^{1-a}$, then we get$$z'+(1-a)p(x)z=(1-a)g(x)$$which is a linear equation.
- **Implicit equations**: in some equations of the form$$F(x,y,y')=0$$the term $y'$ is not explicitly expressed. We discuss the following situations and some special concepts:
	- **$x$ or $y$ can be explicitly expressed**: if, say, we can write $$y=f(x,y')$$then introduce $p=y'$ and differentiate two sides, we get$$p=\frac{\partial f(x,p)}{\partial x}+\frac{\partial f(s,p)}{\partial p}\frac{dp}{dx}$$which is solvable. By integrating $p$ we can get $y$. Situations of $x=f(y,y')$ are essentially the same.
	- **The equation does not explicitly contains $x$ or $y$**: in this case we consider the **method of parameterization**. Say, for the equation $$F(y,y')=0$$we consider $$y=\psi(t),y'=\phi(t)$$After solving them, we calculate the implicit variable $x=\varphi(t)$ by$$\phi(t)=\frac{dy}{dx}=\frac{\psi'(t)}{\varphi'(t)}\Rightarrow\varphi(t)=\int\frac{\psi(t)}{\phi(t)}dt+C$$
## Clairaut equation
Clairaut equation is a differential equation of following form with $f$ continuously differentiable $$y(x)=x\frac{dy}{dx}+f(\frac{dy}{dx})$$To solve such equation, we differentiate with respect to $x$, yielding$$\left[x+f'\left(\frac{dy}{dx}\right)\right]\frac{d^2y}{dx^2}=0$$thus we must have $$\begin{align}&\frac{d^2y}{dx^2}=0\Rightarrow y(x)=Cx+f(C)\\&x+f'\left(\frac{dy}{dx}\right)=0\Rightarrow\begin{cases}x(t)=-f'(t)\\y(t)=f(t)-tf'(t)\end{cases}\quad\text{where }t=\frac{dy}{dx}\end{align}$$The former one is the **general solution** of the equation, while the latter one is the **singular solution**, whose graph is the envelope of the general solutions.
- **Singular solution as envelope of general solutions**: by assuming that two solutions meet at a specific point with the same slope would yield the value of $C$, and by putting it back to the condition we obtain an identity. 
## Chebyshev equation
Chebyshev equation is the second order linear DE of the form $$(1-x^2)\frac{d^2y}{dx^2}-x\frac{dy}{dx}+p^2y=0$$where $p$ is a real (or complex) constant. 
- **Solution**: this equation can be solved with the series method with $$y=\sum_{n=0}^\infty a_nx^n,\quad a_{n+2}=\frac{(n-p)(n+p)}{(n+1)(n+2)}a_n$$the series converges for $|x|<1$ by the ratio test.
	- The recurrence may be started with arbitrary values of $a_0,a_1$. The standard choice is $(a_0,a_1)=(1,0),(0,1)$. When $p\in N$ one of these two functions has its series terminate after a finite number of terms, depending on $p$ even or odd. The resulting polynomial is called the Chebyshev polynomial. 