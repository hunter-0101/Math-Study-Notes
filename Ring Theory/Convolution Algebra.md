**Remark**: we temporarily call an algebra ([[Ring]]) with the product operation called "convolution" a convolution algebra. Strictly speaking, the definition of convolution algebra relies on [[Category]] theory and more, and the "convolution" here is not the ordinary [[Convolution]]. 

---
Let $A$ be an algebra ([[Ring]]) of functions on some index set $X$ (or distributions on a space) equipped with a bilinear associative product $*$ (the [[Convolution]]) and and identity element $\delta$. for an element $\zeta\in A$ that is invertible, formally we'll have $$g=\zeta *f\quad\iff\quad f=\zeta^{-1}*g$$A common reason why $\zeta$ has an inverse, and why the inversion is expressible an integral/sum of simple terms is that the $A$ is diagonalizable by a family $\set{\phi_\alpha}$ (characters, eigenfunctions, irreducible representations, etc.). Concretely, if $\set{\phi_\alpha}$ is a complete orthogonal set ([[Basis in Hilbert Space]]) in $A$ s.t. convolution by $\zeta$ acts by multiplication by $\lambda(\alpha)$ on $\phi_\alpha$: $$\zeta*\phi_\alpha=\lambda(\alpha)\phi_\alpha$$then formally we expect the expression for identity $$\delta=\sum_\alpha\frac{1}{\lambda(\alpha)}\phi_\alpha\otimes\phi_\alpha^*$$and inversion is obtained by projecting $g$ into the $\phi_\alpha$ basis, dividing by $\lambda(\alpha)$, and re-expanding. 
- **[[Distribution]]-theoretic interpretation**: let $X$ be a space and $\set{\Phi_\alpha}$ a "complete" family of kernels s.t. in distribution sense, $$\delta_x(\cdot)=\int\Phi_\alpha(x)\Psi_\alpha(\cdot)d\mu(\alpha)$$then for any $f$ we have $$f(x)=\int\Phi_\alpha(x)\left(\int\Psi_\alpha(y)f(y)dy\right)d\mu(\alpha)$$this is the universal pattern that covers all examples below. The trick is simply 1). find a decomposition of $\delta$, and 2). swap summation/integration to reconstruct $f$. 
- **[[Incidence Algebra]]**: the incidence algebra is a specific type of convolution algebra defined on locally finite posets ([[Order]]). As discussed in the examples below, it can be generalized to other structures like $\mathbb R$ or $\mathbb C$, although analytical tools are required for well-defined-ness of the notion of convolution itself. 
## Motivating examples
Below we list some examples from different fields of math that motivates this note. From a purely algebraic perspective based on the convolution algebra, all results below are derived in the same pattern: we first find an invertible element $\zeta\in A^\times$, and then write $$f=f*1=f*(\zeta*\zeta^{-1})=(f*\zeta)*\zeta^{-1}$$here we utilized associativity of the convolution algebra. 
- **Fourier transform**: in [[FT on L2]] define the convolution as the inner product ([[Inner Product Space]]) over $\mathbb C$, then we have $$f(x)=\int_\mathbb R f(t)\delta_x^tdt=\int_\mathbb R f(t)\int_\mathbb Re^{2\pi i\xi x}\overline{e^{2\pi i\xi t}}d\xi dt=\int_\mathbb Re^{2\pi i\xi x}\left(\int_\mathbb R f(t)e^{-2\pi i\xi t}dt\right)dx$$and the expression in the large brace is exactly $\hat f(\xi)$. Here the expression of $\delta_x(t)$ in terms of an integral can be made rigorous via [[Distribution]] theory.
	- The above also applies to [[Fourier Series]], with the original convolution replaced by circular convolution, and $\delta_x$ being expressed via Fourier series.
- **Möbius transform**: in [[Arithmetic Function]] we prove the Möbius inversion formula via $$f(n)=\sum_{d\mid n}f(d)\delta_{n/d}^1=\sum_{d\mid n}f(d)\sum_{d'\mid n/d}\mu(d')=\sum_{d'\mid n}\mu(d')\left(\sum_{d\mid n/d'}f(d)\right)$$here the expression in the large brace is exactly the condition for $g$.
- **Finite-group FT**: in [[Group]] the finite Fourier transform is proved by $$f(x)=\sum_{y\in G}f(y)\delta_x(y)=\frac{1}{|G|}\sum_{y\in G}f(y)\sum_{\chi\in G^\vee}\chi(x)\overline{\chi(y)}=\frac{1}{|G|}\sum_{\chi\in G^\vee}\chi(x)\left(\sum_{y\in G}f(y)\overline{\chi(y)}\right)$$this is useful in signal processing and algebra (e.g., diagonalizing convolution operator on $G$). It's also why convolution becomes pointwise multiplication in the character basis. 
- **Laplace transform**: as discussed in [[Laplace Transform]], the Laplace inversion formula (which is derived as a [[Contour Integral]]) also arises by writing the identity distribution as an integral kernel built from $e^{st}$.
- **Orthogonal projection**: in [[Spectral Theorem]], if an operator has a spectral decomposition $I=\int dE(\lambda)$ (resolution of the identity), then one writes $$f=\int(dE(\lambda))f$$and in many inversion or reconstruction formula we substitute a representation of the identity as an integral of eigen-projectors. This is useful in [[Holomorphic Functional Calculus]] and other functional calculus.
- **Group representation**: in [[Group Representation]], for compact groups or finite groups, the matrix elements of irreducible representations satisfy orthogonality relations which yield a delta decomposition. That gives inversion formulas for class functions and diagonalization of convolution operators on the group algebra. 
## Invertibility test
For deriving new inversion formulas it's critical to verify whether an element is invertible in the convolution algebra, and further calculate the inverse of the invertible elements. Below we list some common techniques useful for this goal.
- **Triangular inversion**: when the convolution algebra has a natural partial order making the "zeta matrix" triangular, then invertibility reduces to nonzero diagonal entires, and the inverse can be obtained recursively ([[Solving Linear System]]). This is how we deal with the [[Incidence Algebra]] and Dirichlet convolution ([[Arithmetic Function]]).
	- **Matrix method**: when the convolution algebra is finite dimensional we know that it's isomorphic to a subspace of $M_n(F)$ ([[Matrix]]), hence tools from matrix theory can be immediately applied here.
- **Diagonalization**: if we have a transform $T$ that turns convolution into pointwise multiplication $$T(f*g)=T(f)\cdot T(g)$$then $f$ is invertible iff $T(f)$ vanishes nowhere on the spectral domain, and the inverse is $$f^{-1}=T^{-1}\left(\frac{1}{T(f)}\right)$$The application of [[FT on L2]], [[Fourier Series]], and [[Laplace Transform]] for solving [[Linear ODE]] and [[Linear PDE]] utilizes this idea.
	- **Caveat**: pointwise non-vanishing is necessary but may not be sufficient for the inverse to lie in the same function space - one needs analytic control, such as the Wiener's theorem ([[Fourier Series]]).
- **Banach algebra**: in a [[Banach Algebra]] $A$ we have $$f\in A^\times\quad\iff\quad 0\notin\sigma(f)$$The Gelfand transform maps an element to a continuous function on the maximal ideal space; invertibility reduces to non-vanishing of the Gelfand transform. 
	- **Neumann series**: if $f=e+h$ with $\|h\|<1$ the we can utilize geometric series to express its inverse explicitly. 