For a **commutative** ring $R$, the **formal power series ring** is defined as $$R[[X]]=\prod_{i\in\mathbb N} R$$with sum and product for $\alpha=(\alpha_i),\beta=(\beta_i)$ defined as $$\alpha+\beta=(\alpha_i+\beta_i),\quad\alpha\cdot\beta=\left(\sum_{k+l=i}(\alpha_k+\beta_l)\right)$$which is similar to [[Series]] sum and Cauchy product. The additive/multiplicative zero factors are $$0_{R[[X]]}=(0,0,\cdots),\quad1_{R[[X]]}=(1_R,0,0,\cdots)$$Note that the **indeterminate** $X$ is a **purely formal** variable that does not stand for any value.
- $R[[X]]^*=\set{\sum_{i=1}^\infty b_iX^i:b_0\in R^*}$. This can be proven by directly writing the system of equations for the identity to hold.
- **Necessity of commutativity**: without requiring commutativity of ring $R$ the same definition still works, but many desired properties will be lost, e.g., expression of product, polynomial evaluation as a ring homomorphism, etc..

---
The **polynomial ring** over $R$ is the subring of $R[[X]]$ given by $$R[X]=\Set{\sum_{i\in\mathbb N} a_iX^i\in R[[X]]:\text{all but finitely many }i\text{'s are }0}$$We further define in a recurrent manner $$R[X_1,\cdots,X_n]=R[X_1,\cdots,X_{n-1}][X_n]$$i.e., we consider polynomial in $n$ variables with coefficients in $R$ simply as polynomials in one variable but with coefficients that are themselves polynomials in $n-1$ variables.
- **PID structure over field**: for $K$ a [[Field]], $K[X]$ is a PID ([[Ideal]]).
  **Proof**: obviously $K[X]$ is an integral domain (contains no zero divisors), and we now pick a non-trivial ideal $I\subset K[X]$. By the well-ordering principle ([[Set Theory]]) we can pick $d\in I$ of the lowest degree, then by Euclidean division below we have $I\subset(d)$. Meanwhile since $(d)$ is principle we have $(d)\subset I$, hence $I=(d)$, completing the proof.
- **Hilbert's basis theorem**: if $R$ is Noetherian, then $R[X]$ is Noetherian. 
- **Polynomial**: any $f\in R[X]$ is to as a polynomial. Define its degree as $\deg f=\max_{a_i\neq0}i$, then $$\deg(f+g)\le\max(\deg f,\deg g),\quad\deg(fg)\le\deg f+\deg g$$The second equality holds when $R$ is an [[Integral Domain]], in which case  is also an integral domain with $(R[X])^*=R^*$.
- $f=\sum_{i=0}^n r_iX^i\in R[X]$ is nilpotent iff each $r_i$ is nilpotent. The $\Rightarrow$ part is shown in a repetitive manner: $r_0$ is nilpotent, subtract it away from $f$ and divides $x$ out, and repeat. Note that the **sum of nilpotent elements is still nilpotent**.
- **Euclidean division**: for any nonzero $f\in R[X]$ whose leading coefficient is a unit in $R$, and any $g\in R[X]$, there exist unique $q,h\in R[X]$ such that $$g=qf+h,\quad\text{with }\deg h<\deg f\text{ or }h=0$$This can be shown by directly constructing the desired $q,h$.
	- **Division**: when $g=qf$ we say that $f$ divides $g$, denoted $f\mid g$.
	- **Validity**: division algorithm exists iff $R$ is an field. Generally a ring with such a property as $R[X]$ is referred to as [[Euclidean Domain]].
	- **Factorization via root**: given $t\in R,p\in R[X]$ with leading coefficient being a unit, then $$p(t)=0\quad\iff\quad(X-t)\mid p$$This is a direct consequence of Euclidean division.
- If $F$ is a field then $F[X]$ is a unique factorization domain. Actually $R$ is a UFD iff $R[X]$ is a UFD. [[Integral Domain]]
	- **Proof**: for $R[X]$ to be a UFD its constant polynomials must be reducible, and by the monotonicity of degree $R$ is forced to be a UFD. To show the converse we consider the fraction field $F=\Frac(R)$. As $F[X]$ is UFD, any $p\in R[X]$ has a unique factorization in $F[X]$, and by Gauss's lemma $p$ has a unique factorization in $R[X]$.
- **Irreducibility criteria**: let $I$ be a proper ideal of an integral domain $R$, and let $p\in R[X]$ be non-constant and monic. If the image of $p$ in $(R/I)[X]$ is irreducible, then $p$ is irreducible in $R[X]$.
- **Polynomial evaluation (substitution)**: given commutative ring $S$ and a homomorphism $\chi:R\to S$, an evaluation in $s\in S$ is a homomorphism $\phi:R[X]\to S$ s.t. $$\phi(X)=s,\quad\phi\circ\iota=\chi$$where $\iota:r\mapsto(r,0,0,\cdots)$ is the canonical inclusion.
## Symmetric polynomial
A polynomial $f$ is **symmetric** if $f(X_{\sigma_1},\cdots,X_{\sigma_n})=f(X_1,\cdots,X_n),\forall\sigma\in S_n$. The **ring of symmetric polynomials** is denoted $R[X_1,\cdots,X_n]^{S_n}$.
- **Elementary symmetric polynomial**: in general we define $$e_k=\sum_{1\le j_1<\cdots< j_k\le n}X_{j_1}\cdots X_{j_k},\quad 1\le k\le n$$as the elementary symmetric polynomials. Note that we define $e_0=1$.
	- **Factorization of monic polynomial**: we have $$\prod_{j=1}^n(\lambda-X_j)=\lambda^n-e_1\lambda^{n-1}+\cdots+(-1)^ne_n$$which gives rise to the **Vieta's formula**.
- **Fundamental theorem of symmetric polynomial**: for commutative $R$, the set $R[X_1,\cdots,X_n]^{S_n}$ forms a polynomials ring in $e_k(1\le k\le n)$ as its generator.
  **Proof**: this can be shown by induction. The $n=1$ case is trivial; for $G\in R[X_1,\cdots,X_n]^{S_n}$, denote $e_k'$ the ESP for $n-1$ and $e_k$ for $n$, then $$G(X_1,\cdots,X_n)=G_0+G_1X_n+\cdots+G_\nu X_n^{\nu},\quad G_s=G_s(e'_1,\cdots,e_{n-1}'),\forall 1\le s\le\nu$$Since $e_k=e_k'+X_ne_{k-1}',(1\le k\le n$), we can solve this system to get $e_k'$ represented by $e_k$, and substitute them into $G$. Also notice $$X_n^n=X_n^{n-1}e_1-X_n^{n-2}e_2+\cdots+(-1)^{n-1}e_n$$we can reorganize $G$ and reduce it to the form $$G=f_0(e)+f_1(e)X_n+\cdots+f_{n-1}(e)X_n^{n-1}$$Now consider interchanging $r_k\leftrightarrow r_n(1\le k\le n-1)$, then by definition $G,f_k$ are all invariant, while $X_n$ on RHS will be replaced by $X_k$. Now we get $$\begin{bmatrix}1&X_1&\cdots&X_1^{n-1}\\1&X_2&\cdots&X_2^{n-1}\\\vdots&\vdots&\ddots&\vdots\\X_n&X_n^2&\cdots&X_n^{n-1}\end{bmatrix}\begin{bmatrix}f_0(e)\\f_1(e)\\\vdots\\ f_{n-1}(e)\end{bmatrix}=\begin{bmatrix}G\\G\\\vdots\\G\end{bmatrix}$$The matrix is non-singular ([[Matrix]]), hence invertible, and since the pre-image of $[G,\cdots,G]^T$ is exactly $[G,0,\cdots,0]$, we have $[f_0,\cdots,f_{n-1}]=[G,0,\cdots,0]$, hence $G=f_0(e)$ is the desired expression.
	- **Alternative proof**: after deriving $0=f_0(e)+\cdots+f_{n-1}(e)X_n^{n-1}-G:=F$ we can also notice that this polynomial has roots $X_k(1\le k\le n)$, hence $\deg F\ge n$, which means $f_0=G$ and other coefficients vanishes.
	- **Constructive proof**: organize $G$ in dictionary order, then its first term $aX_1^{l_1}\cdots X_n^{l_n}$ must satisfies $l_1\ge\cdots\ge l_n\ge0$. Now consider $\phi_1=ae_1^{l_1-l_2}\cdots e_n^{l_n}$, then its first terms agrees with $G$, hence $G-\phi_1$ is of lower (dictionary) order. Repeating the above process, which is guaranteed to terminates since there are finitely many $(l_1,\cdots,l_n)$, yields a desired decomposition. 
	- **Uniqueness**: the expression is unique, which can also be proven inductively.
- **Newton's identity**: define the $k$-th **power sum** $p_k=\sum_{j=1}^nx_j^k$, then we have $$ke_k=\sum_{i=1}^k(-1)^{i-1}e_{k-i}p_i,\quad\forall 1\le k\le n$$These identities hold for any $R[X_1,\cdots,X_n]^{S_n}$, and yields the recurrence $$p_k=(-1)^{k-1}ke_k+\sum_{i=1}^{k-1}(-1)^{k-1+i}e_{k-i}p_i,\quad\forall 1\le k\le n$$Both can be shown by simple induction and straightforward algebraic manipulation. 
	- **Matrix form**: to write in [[Matrix]], the original Newton's identity can be written as $$\begin{bmatrix}e_1\\2e_2\\3e_3\\4e_4\\\vdots\end{bmatrix}=\begin{bmatrix}
	p_1\\-p_2&p_1\\p_3&-p_2&p_1\\-p_4&p_3&-p_2&p_1\\\vdots&\vdots&\vdots&\vdots&\ddots\end{bmatrix}\begin{bmatrix}e_0\\e_1\\e_2\\e_3\\\vdots\end{bmatrix}$$note that this is an infinite dimensional matrix, although summation over any line is finite.
- **Antisymmetric polynomial**:  we say that $p\in R[X_1,\cdots,X_n]$ is antisymmetric if $$p(X_{\sigma_1},\cdots,X_{\sigma_n})=\sgn(\sigma)p(X_1,\cdots,X_n),\quad\forall\sigma\in S_n$$
	- **Division by Vandermonde [[Determinant]]**: consider $\sigma=(ij),i\neq j$, then the sign of $p$ is inverted, hence by the factorization via root above we have that $$(X_i-X_j)\mid p,\quad\forall i\neq j\quad\Longrightarrow\quad\det V_n(X_1,\cdots,X_n)\mid p$$as a immediate result.
## Reducibility
In this section we discuss a useful method for determining reducibility of a polynomial.
Given $f\in R[X_1,\cdots,X_n]$ it's obvious that we have the isomorphism $$\left(R[X_1,\cdots,X_n]/I\right)[X_1,\cdots,X_k]\cong R[X_1,\cdots,X_k]\where I=(X_{k+1},\cdots,X_n)$$Intuitively we're treating $X_{k+1},\cdots,X_n$ as parameters rather than variables. Hence by assigning specific values we can realize reduction of order: $$X_i=r_i\in R,\quad k+1\le i\le n\quad\Longrightarrow\quad f(X_1,\cdots,X_n)=f_k(X_1,\cdots,X_k)$$and at this point we could analyze reducibility of $f_k$, which is usually much easier. Once one factorization is obtained, we could start from there to construct factorization of $f$. 